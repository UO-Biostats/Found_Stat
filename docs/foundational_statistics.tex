\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Foundational Statistics - Bi 610 - Spring 2020},
            pdfauthor={Clayton M. Small and William A. Cresko},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Foundational Statistics - Bi 610 - Spring 2020}
\author{Clayton M. Small and William A. Cresko}
\date{2020-06-08}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{course-overview}{%
\chapter{Course Overview}\label{course-overview}}

This is the complete set of \emph{course materials} for the \emph{Foundational Statistics Course} at the University of Oregon for the Spring of 2020. It is written in \textbf{Markdown} so that it can be easily updated.

In this book you will find nearly all the information you will need to complete the course.

\hypertarget{introduction-to-the-course}{%
\chapter{Introduction to the course}\label{introduction-to-the-course}}

This is the complete set of \emph{course materials} for the \emph{Foundational Statistics Course} at the University of Oregon for the Spring of 2020. It is written in \textbf{Markdown} so that it can be easily updated.

In this book you will find nearly all the information you will need to complete the course.

\hypertarget{instructors}{%
\section{Instructors}\label{instructors}}

Dr.~Clay Small, \href{mailto:csmall@uoregon.edu}{\nolinkurl{csmall@uoregon.edu}}

Dr.~Bill Cresko, \href{mailto:wcresko@uoregon.edu}{\nolinkurl{wcresko@uoregon.edu}}

\hypertarget{course-information}{%
\section{Course Information}\label{course-information}}

Virtual Office Hours: T-R 12 to 1:30 (Zoom)

\hypertarget{software}{%
\section{Software}\label{software}}

\begin{itemize}
\item
  Latest version of R
\item
  Latest version of RStudio
\end{itemize}

\hypertarget{inclusion-and-accessibility}{%
\section{Inclusion and Accessibility}\label{inclusion-and-accessibility}}

Please tell us your preferred pronouns and/or name, especially if it differs from the class roster. We take seriously our responsibility to create inclusive learning environments. Please notify us if there are aspects of the instruction or design of this course that result in barriers to your participation! You are also encouraged to contact the Accessible Education Center in 164 Oregon Hall at 541-346-1155 or \href{mailto:uoaec@uoregon.edu}{\nolinkurl{uoaec@uoregon.edu}}.

We are committed to making this course an inclusive and respectful learning space. Being respectful includes using preferred pronouns for your classmates. Your classmates come from a diverse set of backgrounds and experiences; please avoid assumptions or stereotypes, and aim for inclusivity. Let us know if there are classroom dynamics that impede your (or someone else's) full engagement.

Because of the COVID-19 pandemic, this course is being delivered entirely remotely. We realize that this situation makes it difficult for some students to interact with the material, for a variety of reasons. We are committed to flexibility during this stressful time and emphasize that we will work with students to overcome difficult barriers as they arise.

Please see this page for more information on campus resources, academic integrity, discrimination, and harassment (and reporting of it).

\hypertarget{course-schedule}{%
\chapter{Course Schedule}\label{course-schedule}}

\hypertarget{weeks-1-2}{%
\section{Weeks 1-2}\label{weeks-1-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data organization and management

  \begin{itemize}
  \tightlist
  \item
    best practices, reproducibility, etc.
  \end{itemize}
\item
  Basic programming fundamentals for data curation

  \begin{itemize}
  \tightlist
  \item
    The Unix environment and fundamental commands
  \item
    Formatting and manipulating tabular text files from the terminal
  \end{itemize}
\item
  Introduction to R and Rstudio

  \begin{itemize}
  \tightlist
  \item
    Installation/Updates
  \item
    R object types and assignment
  \end{itemize}
\item
  Practice with R objects

  \begin{itemize}
  \tightlist
  \item
    vectors, matrices, data frames, etc.
  \end{itemize}
\item
  Applying core programming fundamentals in R

  \begin{itemize}
  \tightlist
  \item
    vectorized operations
  \item
    replicate, apply family, ifelse, for loops, etc.
  \end{itemize}
\end{enumerate}

\hypertarget{week-3}{%
\section{Week 3}\label{week-3}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plotting/visualizing data as a means of exploration

  \begin{itemize}
  \tightlist
  \item
    Different plot types
  \item
    Scale, transformations, etc.
  \end{itemize}
\item
  Fundamentals of plotting in base R

  \begin{itemize}
  \tightlist
  \item
    par
  \item
    using palettes, points, sizes, etc. to convey information
  \item
    axes and labels
  \end{itemize}
\item
  R markdown
\end{enumerate}

\hypertarget{week-4}{%
\section{Week 4}\label{week-4}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Population parameters, samples, and sampling distributions

  \begin{itemize}
  \tightlist
  \item
    Central Limit Theorem and the normal dist.
  \item
    Mean and st. dev.
  \end{itemize}
\item
  Probability and probability distributions
\item
  Calculating summary statistics

  \begin{itemize}
  \tightlist
  \item
    Other common summary statistics (quantiles, etc.)
  \end{itemize}
\end{enumerate}

\hypertarget{week-5}{%
\section{Week 5}\label{week-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Parameter estimation

  \begin{itemize}
  \tightlist
  \item
    Simulating data sets with known parameters
  \item
    Revisit probability distributions
  \end{itemize}
\item
  Uncertainty in estimation

  \begin{itemize}
  \tightlist
  \item
    Parametric and nonparametric approaches to uncertainty
  \end{itemize}
\end{enumerate}

\hypertarget{week-6}{%
\section{Week 6}\label{week-6}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Experimental design

  \begin{itemize}
  \tightlist
  \item
    lexicon
  \item
    considering sources of variance
  \item
    types of variables (categorical, ordinal, rational)
  \item
    confounding variables
  \end{itemize}
\item
  Frequentist hypothesis testing

  \begin{itemize}
  \tightlist
  \item
    error types
  \item
    p-values
  \item
    degrees of freedom
  \item
    statistical power
  \item
    multiple testing problem
  \end{itemize}
\end{enumerate}

\hypertarget{week-7}{%
\section{Week 7}\label{week-7}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Comparing means between groups

  \begin{itemize}
  \tightlist
  \item
    Student's t-test
  \end{itemize}
\item
  Bootstrapping and randomization to compare means
\end{enumerate}

\hypertarget{week-8}{%
\section{Week 8}\label{week-8}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Relationships between quantitative variables

  \begin{itemize}
  \tightlist
  \item
    correlation and covariance
  \end{itemize}
\item
  Simple linear regression

  \begin{itemize}
  \tightlist
  \item
    residuals and least squares
  \item
    fitting linear regression models
  \end{itemize}
\end{enumerate}

\hypertarget{week-9}{%
\section{Week 9}\label{week-9}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Analysis of variance

  \begin{itemize}
  \tightlist
  \item
    Table components and test statistics
  \end{itemize}
\item
  General linear models in R

  \begin{itemize}
  \tightlist
  \item
    Model formulae
  \item
    Interpretation of summary output
  \end{itemize}
\item
  More complex ANOVA frameworks

  \begin{itemize}
  \tightlist
  \item
    Nested models
  \item
    Factorial models
  \end{itemize}
\end{enumerate}

\hypertarget{week-10}{%
\section{Week 10}\label{week-10}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Frequency-based statistical tests

  \begin{itemize}
  \tightlist
  \item
    Chi-squared tests
  \item
    Contingency tables and tests of independence
  \end{itemize}
\item
  Brief introduction to generalized linear models (time permitting)

  \begin{itemize}
  \tightlist
  \item
    logistic regression
  \end{itemize}
\end{enumerate}

\hypertarget{background-material-for-the-course}{%
\chapter{Background material for the course}\label{background-material-for-the-course}}

\hypertarget{description-of-the-course}{%
\section{Description of the course}\label{description-of-the-course}}

This course in an introduction to data management, data visualization, and statistical
inference. It is intended for early-stage graduate students with no background in
statistics. No prior coursework (undergraduate or graduate) in statistics or
programming is assumed. The primary objective of the course is to get students up to
speed with respect to organization, manipulation, visualization, and analysis of data,
using the R statistical language. The emphasis on application is strong, with the goal
of enabling students (after the course) to analyze their own data sets with confidence
using reasonable approaches, and, when faced with more difficult analyses,
to be able to communicate their inference objectives clearly to expert analysts.
Students will learn to organize and analyze data sets in the form of RStudio projects,
using R Markdown files to reproducibly capture and render code, visualizations, and
analyses. In-class exercises will be delivered in the form of pre-formatted R
Notebooks, which can be interactively executed by students without having to write
all code from scratch.

The course is designed to acquaint students primarily with univariate (single
response variable) analysis. Multivariate analysis will be covered in the Advanced
Biostatics 2-course series offered during the Fall and Winter terms. Examples and
assignments in class will include data sets primarily from the biological sciences,
including studies of morphological and molecular traits, behaviors, ecological
questions, and clinical studies. For specific statistical topics covered in class, please
see the course goals and tentative schedule below.

\hypertarget{course-goals}{%
\section{Course goals:}\label{course-goals}}

\begin{itemize}
\tightlist
\item
  Properly organize and format primary data and metadata files for analysis
\item
  Learn programming fundamentals of the R statistical language, including
  objects, functions, iteration, and simulation.
\item
  Make publication-quality data visualizations, including scatterplots, boxplots,
  frequency distributions, mosaic plots, etc.
\item
  Understand Type I and Type II statistical error, including p-values and power analysis.
\item
  Understand ordinary least-squares regression and linear models in general
\item
  Learn the fundamentals of strong experimental design
\item
  Learn to apply general linear models to basic univariate analysis problems,
  including Analysis of Variance (ANOVA)
\item
  Learn nonparametric approaches to parameter estimate and statistical
  inference, including resampling (bootstrapping), permutation, and rank-
  based analysis.
\item
  Understand how to analyze binary response variables and frequency-based
  (e.g.~contingency table) data sets.
\end{itemize}

\hypertarget{introduction-to-r-and-rstudio}{%
\section{Introduction to R and RStudio}\label{introduction-to-r-and-rstudio}}

R is a core computational platform for statistical analysis. It was developed a number of years ago to create an open source environment for advanced computing in statistics and has since become the standard for statistical analysis in the field, replacing commercial packages like SAS and SPSS for the most part. Learning R is an essential part of becoming a scientist who is able to work at the cutting edge of statistical analysis -- or even to perform conventional statistical tests (e.g.~a t-test) in a standard way. An important part of R is that it is script-based, which makes it easy to create reproducible analysis pipelines, which is an emerging feature of the open data/open analysis movement in science. This is becoming an important component of publication and sharing of research results, so being able to engage fully with this effort is something that all young scientists should do.

RMarkdown is an extra layer placed on top of R that makes it easy to integrate text explanations of what is going on, native R code/scripts, and R output all in one document. The final result can be put into a variety of forms, including webpages, pdf documents, Word documents, etc. Entire books are now written in RMarkdown and its relatives. It is a great way to make quick webpages, like this document, for instance. It is very easy to use and will be the format that I use to distribute your assignments to you and that you will use to turn in your assignments.

R Projects are a simple way of designating a working directory in which to house files related to a given, well, project. Those files might include primary data and metadata files ready for reading into R, \texttt{.R} scripts, Rmarkdown files, and output such as Rmarkdown-rendered .html files or individual plots, for example. The nice thing about organizing your work with R Projects is that you can keep everything needed to reproduce an analysis in a single directory on your computer. You can open an R Project in RStudio by opening the project's index (\texttt{.RProj}) file, which will automatically set your working directory to that of the project and facilitate loading any saved environments, etc.

In Chapter 6 we will begin working in R and RStudio, but you can get them installed now (in that order) on your computer, if you haven't already. Get the most recent \emph{released} R version by following this link:
\url{https://www.r-project.org/}

We will do our work using Rstudio, which is a powerful and convenient user interface for R, and can be downloaded from here for installation:
\url{https://rstudio.com/products/rstudio/}

\hypertarget{learning-resources}{%
\subsection{Learning resources}\label{learning-resources}}

There are tons of resources for learning R and RMarkdown on the internet. Here are just a few, but you will no doubt find your own favorites as you become routine R users.

There is an organized group that is dedicated to training in R called DataCamp (\url{https://www.datacamp.com/}). They provide all of the basics for free. They actually have training for most data science platforms. RStudio provides links for training directly related to R and RMarkdown here:
\url{https://education.rstudio.com/}

There are also many, many R training videos on YouTube. Most of them are very well meaning but may not be as in-depth as you want.

You can also go the old ``paper'' manual route by reading the materials provided by R itself:
\url{https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf}

In reality, if you want to do almost anything in R, simply type in what you are interested in doing into Google and include ``in R'' and a whole bunch of links telling you exactly what to do will magically appear. Most of them appear as discussions on websites like StackOverflow and Stats.StackExchange. In that case, the first thing that you see is the question--usually someone doing it just a bit wrong--so you should scroll down to see the right way to do it in the answers. It is really an amazing resource that will speed you along in nearly every form of analysis that you are interested in.

Please do not hesitate to contact us if you have questions or run into obstacles. The point of this class is to learn by doing, but our aim is that the doing should involve reasonable first efforts supplemented with help if needed. Also, many of your classmates have some experience with R, writing code, or statistics in general, so they are an excellent resource as well!

\hypertarget{organizing-and-manipulating-data-files}{%
\chapter{Organizing and manipulating data files}\label{organizing-and-manipulating-data-files}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Many of you will already be familiar with data file organizaiton, editing, and formatting for analysis. If so, much of the following material may be review. If not, some of the following guidelines and tools should prove to be quite useful. In biology, and many other fields, primary data are routinely stored as ``flat'' text files. The exact formatting depends on the type of data, of course, but often we are working with text files organized into rows and columns. Rows can naturally be defined by lines in a file, and columns can be defined by separators (also called delimiters) such as spaces, tabs, or commas, to name a few commonly used ones. Fortunately there are some very powerful and simple-to-use (with a little practice) tools that can be invoked directly from a computer's command line, or included in written ``scripts'' that your computer's operating system can interpret upon you running them. These command line tools are now nearly ubiquitous on all personal computer platforms. Computers running a LINUX operating system allow direct access to these tools via the command line, as does the macOS operating system of Apple computers via the Terminal. Computers running Microsoft Windows 10 now also facilitate use of these conventional ``UNIX tools'' through a Windows Subsystem for Linux.

In the following sections, we provide a \emph{very brief} introduction to using some of these tools in order to organize your data files, parse them for information, and perform some basic text manipulations. Mastering these activities is not necessary for this course (in fact, many of the text manipulation tasks can be done in R!), but if you learn to adopt at least some of these skills you will become a better, more organized analyst, and it will help you become comfortable with the command line and programming in general.

\hypertarget{navigating-file-systems-from-the-command-line}{%
\section{Navigating file systems from the command line}\label{navigating-file-systems-from-the-command-line}}

\hypertarget{access-to-the-command-line}{%
\subsection{Access to the command line}\label{access-to-the-command-line}}

The first step to using command line tools is to get access to the command line! On Mac and Linux systems you can simply do this by finding and opening the \texttt{Terminal} application. On Windows 10 systems, you'll have to install a Linux Bash Shell if you haven't already. To do this you will need to follow the instructions here: \url{https://itsfoss.com/install-bash-on-windows/}
When you get to the point of choosing the Linux distribution to install, I recommend Ubuntu.

At this point you should have command line access through a terminal prompt, which should look something like my Mac Terminal below:
\includegraphics{/Users/csmall/github_repos/Found_Stat/images/MacTerminal.png}

You are now ready to navigate and explore files simply by typing!

\hypertarget{navigating-directories-and-files}{%
\subsection{Navigating directories and files}\label{navigating-directories-and-files}}

When you are at the command line, just think of your computer as you would if you were navigating using a graphical application (e.g.~Mac Finder or Windows Explorer). You are always in a directory in your file system, and you can move to any other directory by typing the appropriate command and destination, then hitting Enter.

The first crucial UNIX command to learn is \texttt{pwd}. This command stands for ``print working directory,'' and it will literally print the path of the directory you are currently in.

Another important command is \texttt{ls}. This lists the files and directories (by default) in your working directory. If you specify a different directory, it will list the files and/or directories there. Most UNIX commands (and indeed command-line programs in general), can be run with options. One way to invoke an option is to type a ``flag'' along with the command. In the case of \texttt{ls}, we can type \texttt{ls\ -l}, for example, which will print the output line-by-line. We can also add another flag: \texttt{ls\ -lh} (equivalent to \texttt{ls\ -l\ -h}), which will print items line-by-line but also make sure the item sizes are ``human readable.'' If you ever have questions about how to use a UNIX program, including the flags and other options, you can type \texttt{man\ program\_name} and a wonderful help manual will appear. To exit and return to the command prompt, just hit ``q''. These \texttt{man} pages are extremely useful and should be your first go-to if you need information for a particular command. Please use these regularly!

The command \texttt{cd} will change your location from the current directory to another directory. Like many other programs (UNIX and otherwise) requiring you to input directory and file locations, with \texttt{cd} you can specify your desired location using either the \emph{absolute} or \emph{relative} path. An absolute path is the full ``address'' of a directory or file, starting from the root of your file system. An example of an absolute path to a directory in my file system is \texttt{/Users/csmall/Dropbox/sculpin\_project/images/}. Regardless of where my current working directory is in my file system, I can change to this \texttt{images/} directory using \texttt{cd} and the full path. I can also use a relative path, which is a sort of ``shortcut,'' to specify the location of a directory or file. Let's say I am in \texttt{/Users/csmall/Dropbox/BiostatsFound\_S2020/} and I want to get to the \texttt{images/} directory above. I could type \texttt{cd\ ../sculpin\_project/images}, which uses a relative path to take me ``up'' one directory (as denoted by \texttt{../}) into \texttt{Dropbox/} and back ``down'' into \texttt{sculpin\_project/images}. In fact, \texttt{..} is a special file in every directory that just means ``the directory above.'' The special file \texttt{.} is the current directory. And to mention one final useful designation for navigation shortcuts, you can use the \texttt{\textasciitilde{}} to denote your home directory.

The schematic below should help you visualize how to think about file system navigation from the commmand line:
\includegraphics{/Users/csmall/github_repos/Found_Stat/images/Directory_example.jpeg}

And for another example, take a look at this series of navigation commands from my terminal and see if you can follow along:
\includegraphics{/Users/csmall/github_repos/Found_Stat/images/MacTerminal_2.png}

If you want to create a new directory, you can use the \texttt{mkdir} command, including the desired name of the new directory. By default this will create the directory in your current working directory, but you can use absolute or relative paths to instead write the directory somewhere else. If you want to delete an empty directory, \texttt{rmdir} is the appropriate command.

Now let's briefly cover some UNIX commands that are useful for managing files. Some of these apply to directories as well, which I will point out as we go. The command \texttt{touch} can be used to create a new, empty file, which you can add to using a plain text editor. Examples of popular plain text editors with advanced user interfaces are BBEdit and Atom. You can also use command line text editors, such as \texttt{nano}, \texttt{emacs}, and \texttt{vim}. Most UNIX/LINUX systems have \texttt{nano} installed by default. To copy or change the name and/or location of a file (or directory), use \texttt{cp} and \texttt{mv} commands, respectively. Note that by using absolute or relative paths, you can specify where you want the file or directory to end up. Be especially careful with these, however, because you will overwrite any existing file or directory if you specify the same name and location. Another command you should be extremely cautious with is \texttt{rm}, which removes (permanently deletes) a file. \texttt{rm\ -r} can be used to delete a non-empty directory AND all of its contents.

In many cases you will want to look at files, or parts of them at least, from the command line. \texttt{cat} will print the entire contents of a file, but can also be used to combine (``concatenate'') multiple files in a line-wise manner. \texttt{less} and \texttt{more} will display specific lines of a file (starting with the first ones), with single- or multi-line ``scrolling,'' respectively, activated using the return or down-arrow keys. To leave the display, you need to hit the ``q'' key. \texttt{head} and \texttt{tail} will display the first or last, respectively, \emph{n} lines of the file, where \emph{n} is provided as a flag (e.g. \texttt{head\ -200\ file.tsv}). The ``word count'' command \texttt{wc} can quantify elements of a text file in various ways, but one common application is \texttt{wc\ -l}, which counts the number of lines in a file.

An aside: If you are working from the command line and want to terminate a process (say you accidentally start a task that will take way too long), press Ctrl-C.

\hypertarget{a-quick-review-of-important-unix-commands-for-navigation-and-viewing}{%
\subsubsection{A quick review of important UNIX commands for navigation and viewing}\label{a-quick-review-of-important-unix-commands-for-navigation-and-viewing}}

\texttt{pwd} - prints working directory

\texttt{ls} - lists contents of a directory

\texttt{cd} - changes the working directory

\texttt{mkdir} - creates a new directory

\texttt{rmdir} - deletes an empty directory

\texttt{touch} - creates an empty file

\texttt{cp} - copies a file or directory

\texttt{mv} - changes the name of a file or directory

\texttt{rm} - deletes a file, or a directory and everything inside with \texttt{-r}

\texttt{cat} - prints the entire file to the terminal, or concatenates and prints multiple files

\texttt{less} - displays the first lines of a file, with scrolling line-by-line

\texttt{head} - prints the first 10 lines (default) of a file

\texttt{tail} - prints the last 10 lines (default) of a file

\texttt{wc\ -l} - prints the number of lines in a file

\hypertarget{useful-unix-commands-for-file-manipulation}{%
\subsection{Useful UNIX commands for file manipulation}\label{useful-unix-commands-for-file-manipulation}}

In many cases you will want to search for specific characters or combinations of characters, and do various things with that information. Maybe you want to isolate the lines of a file that contain the query, or perhaps you want to count how many lines contain the query. The tool \texttt{grep} is extremely useful in this regard. We don't have time for a comprehensive dive into the utilities of \texttt{grep}, but a few common applications are worth mentioning. Character patterns we search for using \texttt{grep} may or may not involve special characters that are not interpreted literally. Here we will discuss just a few common cases of \texttt{grep} searches and the special characters involved. Some examples of these special characters include \texttt{\^{}} (beginning of a line), \texttt{\$} (end of a line), \texttt{.} (any single character except a newline), \texttt{*} (zero or more instances of the preceding character), and \texttt{\textbackslash{}s} (any white space). The standard syntax for \texttt{grep} from the command line is \texttt{grep\ "expression"\ filename}. So, if you wanted to return all of the lines in the data file \texttt{zfish\_data.tsv} (assuming it is in the current directory) that begin with ``embryo\_10'', you could try \texttt{grep\ "\^{}embryo\_10"\ zfish\_data.tsv}. This search would also (unintentionally) find lines beginning with ``embryo\_100'' or ``embryo\_101'', etc., if they exist. So, you have to be careful, and learning the rules just takes practice. In this case \texttt{grep\ "\^{}embryo\_10\textbackslash{}s"\ zfish\_data.tsv} would achieve the desired result, assuming that there is a whitespace delimiter between fields (``columns'') in the data file. Useful flags for \texttt{grep} include \texttt{-c} (which counts the number of lines containing the query), \texttt{-v} (which returns the lines that \emph{do not} contain the query), and \texttt{-n} (which prints the line number for each line containing the query). I encourage you to look at many different \texttt{grep} use cases online as your demand for complex searches grows.

The program \texttt{sed} has reasonably complex applications, but is commonly used as a sort of ``search and replace'' tool. The syntax for \texttt{sed} use is similar to \texttt{grep}, except that the query and replacement expressions are organized (with other information) using slashes. For ``search and replace'' functionality, that syntax looks like this: \texttt{sed\ \textquotesingle{}s/query/replacement/flag\textquotesingle{}\ filename}. One common option for the ``flag'' component is ``g'', meaning ``global'', which replaces all instances. If no flag designation is made only the first instance in the file is replaced. Building on our toy example from above, \texttt{sed\ \textquotesingle{}s/\^{}embryo\_/\^{}larva\_/g\textquotesingle{}\ zfish\_data.tsv} would perform a global replacement and print the output to the terminal. To change the contents in the original file on the fly, including \texttt{sed\ -i} would do the trick, but is riskier than redirecting the output to a new file.

\texttt{cut} is quite straightforward, and can be used to isolate individual fields (think of them like ``columns'') from a text file, provided the fields are consistently separated by a delimiter on each line. So, if I had a comma-separated file and I just wanted the first two columns I could type \texttt{cut\ -f1,2\ -d"\textbackslash{}t"\ filename}. Note that if you don't specify a delimiter using the \texttt{-d} flag, then it is assumed to be tab-delimited. If you want to bring together fields in separate files, \texttt{join} can be used to accomplish this. The two files should have equivalent rows, however, for this action to work properly.

If you want to sort text files alphanumerically, in a field-wise fashion, \texttt{sort} is quite useful. If a file contains a single field, minimal specification is required, aside from tuning numerical sorting. For example, if you want to sort numerically, use the \texttt{-n} flag, and if you want to sort from largest to smallest, add the \texttt{-r} flag. If you want to sort a multi-field file based on just one field, you can use the ``key'' flag. For instance, if you have a tab-delimited file and want to sort by the second field in reverse numerical order, \texttt{sort\ -k2,2\ -nr\ filename.tsv} would give you the desired result. Finally, if you want to eliminate lines with the same value for a given field, you can use the \texttt{-u} ``unique'' flag.

The UNIX program \texttt{awk} is an extremely powerful tool, and can itself be used essentially as a mini programming language. We will not get into the myriad uses of \texttt{awk} here, but the reference at the bottom of the chapter is a great resource if you want to learn more. \texttt{awk} is extremely efficient at parsing and capturing text files in a column-wise manner, with the ability to also evaluate logical statements applied to rows. The structure of \texttt{awk} commands is more complex than that of other UNIX programs we have discussed, but it is still very intuitive. One unique feature is that \texttt{awk} contains its own internal functions, which are typed inside curly braces. The ``print'' function can be used to extract fields, much like \texttt{cut}. For instance, \texttt{awk\ -F:\ \textquotesingle{}\{print\ \$1,\$6\}\textquotesingle{}\ filename.tsv} would print the first and sixth field from \texttt{filename.tsv}, assuming a ``:'' delimiter. With \texttt{awk}, fields are specified using the \texttt{\$} character. If you want also to select only specific rows from a set of columns (like those with a certain value), you can incorporate logical operators. In the above example if we had wanted fields 1 and 6, but only those rows with a value of at least 610 in field 4, we could type the following \texttt{awk\ -F:\ \textquotesingle{}\$4\ \textgreater{}=\ 610\ \{print\ \$1,\$6\}\textquotesingle{}\ filename.tsv}. Again, this is just scratching the surface with \texttt{awk}, which boasts a great deal of potential for your text file manipulation needs.

\hypertarget{a-quick-review-of-key-unix-commands-for-text-file-searching-and-manipulation}{%
\subsubsection{A quick review of key UNIX commands for text file searching and manipulation}\label{a-quick-review-of-key-unix-commands-for-text-file-searching-and-manipulation}}

\texttt{grep} - searches a file for characters and character combinations

\texttt{sed} - stream edits characters and character combinations

\texttt{cut} - isolates specific fields (``columns'') from a file using a delimiter

\texttt{join} - combines fields (``columns'') from multiple files with equivalent rows

\texttt{sort} - orders the rows in a file based on one or more fields

\texttt{awk} - flexibly parses, evaluates, and selectively prints row- and column-wise

\hypertarget{a-quick-word-on-pipes-and-carrots}{%
\subsection{A quick word on pipes and carrots}\label{a-quick-word-on-pipes-and-carrots}}

One very convenient feature of UNIX commands is that you can control the flow of input and output from one command to another using the \texttt{\textbar{}} (``pipe'') character. For instance, I may want to search an entire file for rows that begin with ``fish-1'', and then replace the ``-'' with ``\_''. To do this I could do something like \texttt{cat\ file.tsv\ \textbar{}\ grep\ "\^{}fish-1"\ \textbar{}\ sed\ \textquotesingle{}s/fish-1/fish\_1/g\textquotesingle{}} This, of course, would print the output to the terminal, but I could actually capture that output into a file using the \texttt{\textgreater{}} character. \texttt{cat\ filename\ \textbar{}\ grep\ "\^{}fish-1"\ \textbar{}\ sed\ \textquotesingle{}s/fish-1/fish\_1/g\textquotesingle{}\ \textgreater{}\ ./newfile.tsv} would write this new file to my current working directory. Furthermore, if you want to append lines of text to an existing file, the ``double sideways right-pointing carrot'' character \texttt{\textgreater{}\textgreater{}} can be used.

The above lessons on UNIX commands for file manipulation truly just scratch the surface of what can be accomplished at the command line and in ``shell scripts.'' You certainly will have further questions and be hungry for more, but we simply don't have time during this course. But to work on your UNIX skills for now, check out \texttt{Ex1\_Unix\_Intro.html} (on Canvas). We need to move on to R now, but at the bottom of this chapter are some UNIX command resources I have found to be especially useful.

\hypertarget{data-file-and-data-file-entry-dos-and-donts}{%
\section{Data file and data file entry dos and don'ts}\label{data-file-and-data-file-entry-dos-and-donts}}

Do store a copy of your data in a nonproprietary format, such as plain ASCII text (aka a flat file). This is especially important if you are using tools (like UNIX commands) to parse and manipulate the files. Formats like Microsoft Excel are not acceptable as input for many analysis tools, and not everyone has access to proprietary software.

Do leave an un-edited copy of an original data file, even when main analyses require an edited version.

Do use descriptive names for your data files and variables, and use them consistently!

Do maintain effective metadata about the data.

Do add new observations to a data file as rows.

Do add new variables to a data file as columns.

Don't include multiple data types in the same column.

Don't use non-alphanumeric characters (other than the underscore) in file or directory names.

Don't use spaces, tabs, commas, colons, semicolons, or other characters commonly used as field (column) delimiters in names of individual data entries. For example, don't use something like \texttt{March\ 8} as a value for date in a data set.

Don't copy and paste data directly from rich-text-formatted files (like Microsoft Word) into primary data files.

\hypertarget{exercises-associated-with-this-chapter}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter}}

\begin{itemize}
\tightlist
\item
  Exercise 1 (file: \texttt{Ex1\_Unix\_Intro.html})
\end{itemize}

\hypertarget{additional-learning-resources}{%
\section{Additional learning resources}\label{additional-learning-resources}}

\begin{itemize}
\item
  \url{http://mally.stanford.edu/~sr/computing/basic-unix.html} - A nice ``cheat sheet''
\item
  \url{http://korflab.ucdavis.edu/Unix_and_Perl/} - Outstanding tutorial by Keith Bradnam and Ian Korf
\item
  \url{https://www.datacamp.com/courses/introduction-to-shell-for-data-science} - DataCamp tutorial
\item
  \url{https://www.gnu.org/software/gawk/manual/gawk.html} - A comprehensive guide to \texttt{awk}
\end{itemize}

\hypertarget{an-introduction-to-the-r-language}{%
\chapter{An Introduction to the R language}\label{an-introduction-to-the-r-language}}

\hypertarget{background}{%
\section{Background}\label{background}}

\texttt{R} is a computer programming language and environment especially useful for graphic visualization and statistical analysis of data. It is an offshoot of a language developed in 1976 at Bell Laboratories called \texttt{S}. \texttt{R} is an interpreted language, meaning that every time code is run it must be translated to machine language by the \texttt{R} interpreter, as opposed to being compiled prior to running. \texttt{R} is the premier computational platform for statistical analysis thanks to its GNU open-source status and countless packages contributed by diverse members of the scientific community.

\hypertarget{why-use-r}{%
\section{\texorpdfstring{Why use \texttt{R}?}{Why use R?}}\label{why-use-r}}

\begin{itemize}
\tightlist
\item
  Good general scripting tool for statistics and mathematics
\item
  Powerful and flexible and free
\item
  Runs on all computer platforms
\item
  New packages released all the time
\item
  Superb data management \& graphics capabilities
\item
  Reproducibility - can keep your scripts to see exactly what was done
\item
  Can embed your \texttt{R} analyses in dynamic, polished files using R markdown
\item
  You can write your own functions
\item
  Lots of online help available
\item
  Can use a nice IDE such as \texttt{RStudio}
\end{itemize}

\hypertarget{important-r-terms-and-definitions}{%
\section{\texorpdfstring{Important \texttt{R} terms and definitions}{Important R terms and definitions}}\label{important-r-terms-and-definitions}}

\begin{figure}
\centering
\includegraphics{/Users/csmall/github_repos/Found_Stat/images/R_definitions_Logan.001.jpeg}
\caption{Alt text}
\end{figure}

From Logan, M. 2010. \emph{Biostatistical Design and Analysis Using R}

Operators are symbols in programming that have a specific meaning

\begin{figure}
\centering
\includegraphics{/Users/csmall/github_repos/Found_Stat/images/R_definitions_Logan.002.jpeg}
\caption{Alt text}
\end{figure}

From Logan, M. 2010. \emph{Biostatistical Design and Analysis Using R}

\hypertarget{getting-started-with-r-via-the-rstudio-environment}{%
\section{\texorpdfstring{Getting started with \texttt{R} via the RStudio Environment}{Getting started with R via the RStudio Environment}}\label{getting-started-with-r-via-the-rstudio-environment}}

To begin working with \texttt{R}, open RStudio. You should first see something that looks like this:
\includegraphics{/Users/csmall/github_repos/Found_Stat/images/MacTerminal_3.png}

To open a new script editor (where you will keep track of your code and notes), go to File \textgreater{} New File \textgreater{} R Script. Note that there are other options for file types, which we will be using in the future. For now, though, we want a plain script, which when saved will have the extention \texttt{.R}.

It is easy to run code directly from the script editor. For single lines of code, simply make sure your cursor is on that line, and hit Ctrl-Enter. For multiple lines, highlight the block of code you want to run and hit Ctrl-Enter.

Now your display should look somehting like below (but without the red pane labels, of course):
\includegraphics{/Users/csmall/github_repos/Found_Stat/images/R_definitions_Logan.003.jpeg}

Note that you can also type commands directly from the command line using the \texttt{R} Console (lower left pane), and the \texttt{R} interpreter will run them when you press Enter.

Any objects you define, and a summary of their values, will appear in the upper right pane, and the lower right pane differs in appearance depending on instructions you provide to \texttt{R\ Studio}. For instance, if you produce a plot, it will appear there by default. Another extremely important feature of R functions (we'll get to them in a bit) is the help file. Recall from Chapter 5 our discussion of \texttt{man} pages for UNIX programs. Help files the equivalent for \texttt{R} functions. They contain almost everything you need to know about a given function, and most of them even include and example at the bottom. These help files will appear in the lower right RStudio pane when you call them, for example when you run \texttt{help(function\_name)} at the \texttt{R} Console.

\hypertarget{r-programming-basics}{%
\subsection{R Programming Basics}\label{r-programming-basics}}

For the code examples below, it might be useful for you to start your own RStudio session, open a new \texttt{.R} file and type/run code while reading.

\begin{itemize}
\tightlist
\item
  Commands can be submitted through the terminal, console or scripts
\item
  In your scripts, anything that follows \texttt{\#} symbol (aka hash) is just for humans
\item
  Notice on these slides I'm evaluating the code chunks and showing output
\item
  The output is shown here after the two \texttt{\#} symbols and the number of output items is in \texttt{{[}{]}}
\item
  Also notice that \texttt{R} follows the normal priority of mathematical evaluation
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{4}\OperatorTok{*}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{4}\OperatorTok{+}\DecValTok{3}\OperatorTok{*}\DecValTok{2}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 16
\end{verbatim}

\hypertarget{a-note-on-r-markdown}{%
\subsubsection{\texorpdfstring{A note on \texttt{R} Markdown}{A note on R Markdown}}\label{a-note-on-r-markdown}}

This format provides a much better way to embed code and output, in an easily readable, reproducible manner. We will dive into \texttt{R} Markdown next week, so for now just be aware that it exists.

\begin{itemize}
\item
  \url{http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html}
\item
  You can insert \texttt{R} chunks into \texttt{Rmarkdown} documents
\end{itemize}

\hypertarget{assigning-variables}{%
\subsubsection{Assigning Variables}\label{assigning-variables}}

\begin{itemize}
\item
  To ``store'' information for later use, like the arithmetic operation above, we can assign variables in \texttt{R}.
\item
  Variables are assigned values using the \texttt{\textless{}-} operator.
\item
  Variable names must begin with a letter, and should not contain spaces or \texttt{R} operators (see above) but other than that, just about anything goes.
\item
  Do keep in mind that \texttt{R} is case sensitive.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{2}
\NormalTok{x }\OperatorTok{*}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\NormalTok{x }\OperatorTok{*}\StringTok{ }\DecValTok{3}
\NormalTok{y }\OperatorTok{-}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

These do not work

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{3y <-}\StringTok{ }\DecValTok{3}
\DecValTok{3}\OperatorTok{*}\NormalTok{y <-}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\hypertarget{arithmetic-operations-with-functions}{%
\subsubsection{Arithmetic operations with functions}\label{arithmetic-operations-with-functions}}

\begin{itemize}
\item
  Arithmetic operations can be used with functions as well as numbers.
\item
  Try the following, and then your own.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{+}\DecValTok{2}
\NormalTok{x}\OperatorTok{^}\DecValTok{2}
\KeywordTok{log}\NormalTok{(x) }\OperatorTok{+}\StringTok{ }\KeywordTok{log}\NormalTok{(x}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Note that the last of these - \texttt{log()} - is a built in function of \texttt{R}, and therefore the argument for the function (in this case ``x'' or ``x+1'') needs to be put in parentheses.
\item
  These parentheses will be important, and we'll come back to them later when we add other arguments after the object in the parentheses.
\item
  The outcome of calculations can be assigned to new variables as well, and the results can be checked using the \texttt{print()} function.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\DecValTok{67}
\KeywordTok{print}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 67
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{124}
\NormalTok{z <-}\StringTok{ }\NormalTok{(x}\OperatorTok{*}\NormalTok{y)}\OperatorTok{^}\DecValTok{2}
\KeywordTok{print}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 69022864
\end{verbatim}

\hypertarget{strings}{%
\subsubsection{Strings}\label{strings}}

\begin{itemize}
\item
  Assignments and operations can be performed on characters as well.
\item
  Note that characters need to be set off by quotation marks to differentiate them from numeric objects.
\item
  The c(function) stands for `concatenate'.
\item
  Note that we are using the same variable names as we did previously, which means that we're overwriting our previous assignment.
\item
  A good general rule is to use new names for each variable, and make them short but still descriptive
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ "I Love"}
\KeywordTok{print}\NormalTok{ (x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "I Love"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ "Biostatistics"}
\KeywordTok{print}\NormalTok{ (y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Biostatistics"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(x,y)}
\KeywordTok{print}\NormalTok{ (z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "I Love"        "Biostatistics"
\end{verbatim}

The variable z is now a vector of character objects.

\hypertarget{factors}{%
\subsubsection{Factors}\label{factors}}

\begin{itemize}
\item
  Sometimes we would like to treat character objects as if they were units for subsequent calculations.
\item
  These are called factors, and we can redefine our character object as one of class factor.
\item
  This might seem a bit strange, but it's important for statistical analyses where we might want to calculate the mean or variance for two different treatments. In that case the two different treatments would be coded as two different ``levels'' of a factor we designate in our metadata. This will become clear when we get into hypothesis testing in \texttt{R}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z_factor <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(z)}
\KeywordTok{print}\NormalTok{(z_factor)}
\KeywordTok{class}\NormalTok{(z_factor)}
\end{Highlighting}
\end{Shaded}

Note that factor levels are reported alphabetically. I used the \texttt{class()} function to ask \texttt{R} what type of object ``z\_factor'' is. \texttt{class()} is one of the most important tools at your disposal. Often times you can debug your code simply by changing the class of an object. Because functions are written to work with specific classes, changing the class of a given object is crucial in many cases.

\hypertarget{vectors}{%
\subsubsection{Vectors}\label{vectors}}

\begin{itemize}
\item
  In general R thinks in terms of vectors (a list of characters factors or numerical values) and it will benefit any R user to try to write programs with that in mind.
\item
  R operations, and therefore functions, are vectorized.
\item
  This means an operation or function will be performed for each element in a vector.
\item
  Vectors can be assigned directly using the `c()' function and then entering the exact values.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{9}\NormalTok{)}
\KeywordTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  2  3  4  2  1  2  4  5 10  8  9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x_plus <-}\StringTok{ }\NormalTok{x}\OperatorTok{+}\DecValTok{1}
\KeywordTok{print}\NormalTok{(x_plus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  3  4  5  3  2  3  5  6 11  9 10
\end{verbatim}

\begin{itemize}
\item
  Creating vectors of new data by entering it by hand can be a drag.
\item
  However, it is also very easy to use functions such as \texttt{seq()} and \texttt{sample()}.
\item
  Try the examples below. Can you figure out what the three arguments in the parentheses mean?
\item
  Within reason, try varying the arguments to see what happens
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{10.0}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.1}\NormalTok{)}
\KeywordTok{print}\NormalTok{(seq_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1]  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0  1.1  1.2  1.3  1.4
##  [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9
##  [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4
##  [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9
##  [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4
##  [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9
##  [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{10.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{, }\DataTypeTok{by =} \FloatTok{-0.1}\NormalTok{)}
\KeywordTok{print}\NormalTok{(seq_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 10.0  9.9  9.8  9.7  9.6  9.5  9.4  9.3  9.2  9.1  9.0  8.9  8.8  8.7  8.6
##  [16]  8.5  8.4  8.3  8.2  8.1  8.0  7.9  7.8  7.7  7.6  7.5  7.4  7.3  7.2  7.1
##  [31]  7.0  6.9  6.8  6.7  6.6  6.5  6.4  6.3  6.2  6.1  6.0  5.9  5.8  5.7  5.6
##  [46]  5.5  5.4  5.3  5.2  5.1  5.0  4.9  4.8  4.7  4.6  4.5  4.4  4.3  4.2  4.1
##  [61]  4.0  3.9  3.8  3.7  3.6  3.5  3.4  3.3  3.2  3.1  3.0  2.9  2.8  2.7  2.6
##  [76]  2.5  2.4  2.3  2.2  2.1  2.0  1.9  1.8  1.7  1.6  1.5  1.4  1.3  1.2  1.1
##  [91]  1.0  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq_square <-}\StringTok{ }\NormalTok{(seq_}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\NormalTok{(seq_}\DecValTok{2}\NormalTok{)}
\KeywordTok{print}\NormalTok{(seq_square)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 100.00  98.01  96.04  94.09  92.16  90.25  88.36  86.49  84.64  82.81
##  [11]  81.00  79.21  77.44  75.69  73.96  72.25  70.56  68.89  67.24  65.61
##  [21]  64.00  62.41  60.84  59.29  57.76  56.25  54.76  53.29  51.84  50.41
##  [31]  49.00  47.61  46.24  44.89  43.56  42.25  40.96  39.69  38.44  37.21
##  [41]  36.00  34.81  33.64  32.49  31.36  30.25  29.16  28.09  27.04  26.01
##  [51]  25.00  24.01  23.04  22.09  21.16  20.25  19.36  18.49  17.64  16.81
##  [61]  16.00  15.21  14.44  13.69  12.96  12.25  11.56  10.89  10.24   9.61
##  [71]   9.00   8.41   7.84   7.29   6.76   6.25   5.76   5.29   4.84   4.41
##  [81]   4.00   3.61   3.24   2.89   2.56   2.25   1.96   1.69   1.44   1.21
##  [91]   1.00   0.81   0.64   0.49   0.36   0.25   0.16   0.09   0.04   0.01
## [101]   0.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq_square_new <-}\StringTok{ }\NormalTok{(seq_}\DecValTok{2}\NormalTok{)}\OperatorTok{^}\DecValTok{2}
\KeywordTok{print}\NormalTok{(seq_square_new)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 100.00  98.01  96.04  94.09  92.16  90.25  88.36  86.49  84.64  82.81
##  [11]  81.00  79.21  77.44  75.69  73.96  72.25  70.56  68.89  67.24  65.61
##  [21]  64.00  62.41  60.84  59.29  57.76  56.25  54.76  53.29  51.84  50.41
##  [31]  49.00  47.61  46.24  44.89  43.56  42.25  40.96  39.69  38.44  37.21
##  [41]  36.00  34.81  33.64  32.49  31.36  30.25  29.16  28.09  27.04  26.01
##  [51]  25.00  24.01  23.04  22.09  21.16  20.25  19.36  18.49  17.64  16.81
##  [61]  16.00  15.21  14.44  13.69  12.96  12.25  11.56  10.89  10.24   9.61
##  [71]   9.00   8.41   7.84   7.29   6.76   6.25   5.76   5.29   4.84   4.41
##  [81]   4.00   3.61   3.24   2.89   2.56   2.25   1.96   1.69   1.44   1.21
##  [91]   1.00   0.81   0.64   0.49   0.36   0.25   0.16   0.09   0.04   0.01
## [101]   0.00
\end{verbatim}

\begin{itemize}
\item
  Here is a way to create your own data sets that are random samples.
\item
  Again, on your own, play around with the arguments in the parentheses to see what happens.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{ (}\DecValTok{10000}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{sample}\NormalTok{ (}\DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{, }\DecValTok{10000}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T)}
\NormalTok{xy <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(x,y)}
\KeywordTok{plot}\NormalTok{(x,y) }
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{foundational_statistics_files/figure-latex/Samples from distributions 1-1}

\begin{itemize}
\item
  You've probably figured out that ``y'' from the last example is a draw of numbers with equal probability (what we call a flat, or uniform distribution).
\item
  What if you want to draw from a defined probability distribution, like the normal distribution?
\item
  Again, play around with the arguments in the parentheses to see what happens.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\KeywordTok{print}\NormalTok{ (x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] -115.736062   46.162100  -20.570776  -76.484887 -123.604512  -26.888954
##   [7]   76.317225  -57.788088  110.387306 -112.070642  -24.530856   68.746708
##  [13] -120.029738   93.466653   39.184877    9.435532  -81.923434 -192.788644
##  [19]  -43.199988   12.227361 -107.457187  -50.481329  -39.041030   74.775718
##  [25]   16.151459    7.464346  151.623071  -95.181466  -79.264048  -97.977955
##  [31]  -66.382312  -60.579807 -131.643555  -91.266695  116.313564   26.891971
##  [37]  119.407931   39.898933   -7.547961    3.272318   14.036778   49.989889
##  [43]  182.852914  102.644496   40.666657  -75.383949   68.251600 -119.179269
##  [49]  141.137111   41.354446   19.627214   18.594767 -101.945437  176.305210
##  [55]  182.163007  -13.627601  -79.786733   12.449008   53.162520 -142.908410
##  [61]   38.408529  -33.026918   64.944359  -39.583385 -142.549870  172.957354
##  [67]   30.252369   87.199171   72.097828   -7.062924   -5.167127  -93.072087
##  [73]  -75.259291 -222.993224  215.267991  111.004214   -9.791820 -262.514250
##  [79]   26.070458  127.347038    9.721671  -50.540868    2.243590  -22.680583
##  [85]  155.454690  -51.485703  -28.734447 -124.666379  139.953807  124.313111
##  [91]   18.581008  165.526695  -25.762179  110.525407  -83.971741  -13.418931
##  [97]  -75.917847  -44.366603 -194.754218  116.146678
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{50}\NormalTok{,}\DecValTok{50}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{foundational_statistics_files/figure-latex/Samples from distributions 2-1}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(x, }\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{500}\NormalTok{,}\DecValTok{500}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=1\linewidth]{foundational_statistics_files/figure-latex/Samples from distributions 2-2}

Can you figure out what the three rnorm() arguments represent?

\hypertarget{basic-summary-statistics}{%
\subsubsection{Basic Summary Statistics}\label{basic-summary-statistics}}

We will get into the details regarding summary statistics later, but for now, check out several of the \texttt{R} functions that calculate them.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(x)}
\KeywordTok{median}\NormalTok{(x)}
\KeywordTok{var}\NormalTok{(x)}
\KeywordTok{log}\NormalTok{(x)}
\KeywordTok{ln}\NormalTok{(x)}
\KeywordTok{sqrt}\NormalTok{(x)}
\KeywordTok{sum}\NormalTok{(x)}
\KeywordTok{length}\NormalTok{(x)}
\KeywordTok{sample}\NormalTok{(x, }\DataTypeTok{replace =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Notice that the last function (\texttt{sample}) has an argument (\texttt{replace=T})
\item
  Arguments simply modify or direct the function in some way
\item
  There are many arguments for each function, some of which are defaults
\end{itemize}

\hypertarget{getting-help-to-understand-functions}{%
\subsubsection{Getting help to understand functions}\label{getting-help-to-understand-functions}}

\begin{itemize}
\item
  Getting help on any function is very easy - just type a question mark and the name of the function.
\item
  There are functions for just about anything within \texttt{R} and it is easy enough to write your own functions if none already exist to do what you want to do.
\item
  In general, function calls have a simple structure: a function name, a set of parentheses and an optional set of arguments you assign parameters to and send to the function.
\item
  Help pages exist for all functions that, at a minimum, explain what parameters exist for the function.
\item
  Help can be accessed a few ways - try them :
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{-}\StringTok{ }\KeywordTok{help}\NormalTok{(mean)}
\OperatorTok{-}\StringTok{ }\NormalTok{?mean}
\OperatorTok{-}\StringTok{ }\KeywordTok{example}\NormalTok{(mean)}
\OperatorTok{-}\StringTok{ }\KeywordTok{help.search}\NormalTok{(}\StringTok{"mean"}\NormalTok{)}
\OperatorTok{-}\StringTok{ }\KeywordTok{apropos}\NormalTok{(}\StringTok{"mean"}\NormalTok{)}
\OperatorTok{-}\StringTok{ }\KeywordTok{args}\NormalTok{(mean)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-associated-with-this-chapter-1}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-1}}

\begin{itemize}
\tightlist
\item
  Exercise 2 (\texttt{rtutorial\_1} in \texttt{foundstats} R package)
\end{itemize}

\hypertarget{additional-learning-resources-1}{%
\section{Additional learning resources:}\label{additional-learning-resources-1}}

\begin{itemize}
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\item
  \url{http://library.open.oregonstate.edu/computationalbiology/} - O'Neil, S.T. 2017. A Primer for Computational Biology
\end{itemize}

\hypertarget{more-r-functions-complex-objects-basic-plotting-and-rmarkdown}{%
\chapter{More R Functions, Complex Objects, Basic Plotting, and RMarkdown}\label{more-r-functions-complex-objects-basic-plotting-and-rmarkdown}}

\hypertarget{background-1}{%
\section{Background}\label{background-1}}

In this chapter we will cover a variety of topics, all of which will help you build your \texttt{R} programming skills and make you capable of dealing with data sets using \texttt{R}. We will explore additional base \texttt{R} functions that are extremely useful for generating and manipulating vectors, combining vectors into multidimensional \texttt{R} objects, and working with those objects. We will also cover base \texttt{R} plotting functions to get you started with making your own publication-quality plots. Finally, we will touch on the RMarkdown file format, how to write those files in \texttt{RStudio}, and how to render the \texttt{.Rmd} file into polished, readable \texttt{.html} documents.

\hypertarget{more-on-functions}{%
\section{More on functions}\label{more-on-functions}}

In the last chapter we touched on functions in \texttt{R}, gave a few examples of commonly used functions, and covered how to learn more about a function using the \texttt{help()} function. As mentioned, functions and their use follow a basic structure. To call functions we type their name and include a set of parameters expressed as arguments, which specify what we want them to do, inside parentheses \texttt{()}. For example, to successfully call the function \texttt{mean()}, we need, at minimum, to supply a vector of numeric values. That vector can be an obect we have already assigned in our environment, or it can be the outcome of another function called within the \texttt{mean()} function. Below are these two alternatives.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20
\end{verbatim}

The second alternative illustrates the power of ``nesting'' functions within \texttt{R}. You don't need to perform tasks by defining a bunch of intermediate objects and calling functions in piecemeal manner. In many cases it is much more efficient to nest functions within one another, as long as it doesn't jeopardize the functionality or readability of your code.

Base \texttt{R} includes dozens of useful functions that will become part of your regular arsenal. We have already mentioned several of these and discussed how to discover and learn more about them. As you become a more advanced \texttt{R} user, and in particular as you begin performing tasks and analyses more specific to your field of study, you will need to use functions that are not included in the base \texttt{R} library. Fortunately, there are thousands of functions distributed in the form of \texttt{R} ``packages,'' which you can easily install on your system. Packages especially easy to find and use are those distributed via the Comprehensive R Archive Network (CRAN): \url{https://cran.r-project.org/web/packages/index.html}. If you find a specific function or set of functions you are interested in trying out, for instance after a Google search of your problem, you can download and install the package those functions belong to by running the following command from your \texttt{R} Console:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"name_of_package"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that the name of the package has to be spelled correctly (and \texttt{R} is case sensitive), and that the name of the package should be in quotation marks. You will get a series of messages printed to the Console, and finally either a confirmation of installation or error message. Once you have installed a package successfully, you do not need to re-run the \texttt{install.packages()} function. If you want to check whether a package has already been installed, and look at the details of that installation, you can always run the following from the Console:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{installed.packages}\NormalTok{(}\StringTok{"name_of_package"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To actually use the functions from an installed package, you have to ``load'' that package into your current working environment. To do that we use the \texttt{library()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(name_of_package)}
\end{Highlighting}
\end{Shaded}

Note that you do not include quotation marks around the package name for the \texttt{library()} function. Unlike package installation, you will need to invoke \texttt{library()} every time you start a new \texttt{R} session to load the package and its functions.

It is also possible, and quite straightforward, to write your own \texttt{R} functions, which you can define within your \texttt{.R} or \texttt{.Rmd} scripts for convenient usage. If you get the the point at which you want to distribute your own functions in the form of a package, that is possible too. Later during this course we will get a little experience in writing simple \texttt{R} functions. Writing more involved functions and publishing packages, however, are topics for a more advanced \texttt{R} course.

\hypertarget{more-base-r-functions-useful-for-working-with-vectors}{%
\subsection{\texorpdfstring{More base \texttt{R} functions useful for working with vectors}{More base R functions useful for working with vectors}}\label{more-base-r-functions-useful-for-working-with-vectors}}

Below are annotated lists of base \texttt{R} functions commonly used to work with vectors. We will not take the time here to give specific examples for each function, because their usage is quite straightforward and you will get plenty of practice with them in associated exercies. You can also practice using the \texttt{help()} function if you have specific questions.

\textbf{The following functions provide information about vectors:}

\begin{itemize}
\item
  \texttt{head()}: returns the first elements of an object (like a vector or data frame)
\item
  \texttt{tail()}: returns the last elements of an object (like a vector or data frame)
\item
  \texttt{length()}: returns the number of elements in a vector
\item
  \texttt{class()}: returns the class of elements in a vector (e.g. ``character'', ``numeric'', ``factor'', etc.)
\end{itemize}

\textbf{The following functions can modify or generate vectors in structured ways:}

\begin{itemize}
\item
  \texttt{sort()}: returns a sorted vector from an orignal vector of numeric values
\item
  \texttt{seq()}: returns a ``series'' of numeric values beginning at one value and ending at another, while also specifying the size of increments/decrements between values
\item
  \texttt{rep()}: returns a vector of identical elements, repeated a specified number of times
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 1 1 1 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\StringTok{"one"}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "one" "one" "one" "one" "one"
\end{verbatim}

Note that \texttt{seq()} and \texttt{rep()} can be repeated and/or combined in various ways, in some cases using \texttt{c()}, to generate vectors in a multitude of patterned ways.

\textbf{The following functions can generate vectors of random values, randomly shuffle vectors, or generate vectors of values drawn from defined probability distributions:}

\begin{itemize}
\item
  \texttt{sample()}: randomly selects and returns elements from a vector (``shuffles'' a vector when size argument is set to original vector size and replace argument is set to ``FALSE'')
\item
  \texttt{rnorm()}: randomly draws values from a theoretical normal distribution
\item
  \texttt{rbinom()}: randomly draws values from a theoretical binomial distribution
\item
  \texttt{set.seed()}: sets \texttt{R}'s random number generator seed so that operations with stochastic properties can be reproduced
\end{itemize}

\textbf{The following functions can change the class of elements in a particular vector:}

\begin{itemize}
\item
  \texttt{as.numeric()}: changes the class of objects in a vector to ``numeric''.
\item
  \texttt{as.factor()}: changes the class of objects in a vector to ``factor''.
\item
  \texttt{as.character()}: changes the class of objects in a vector to ``character''.
\end{itemize}

The \texttt{as.xxx} family of \texttt{R} functions is especially useful if you need to convert the class of a particular object for a given function to use the object properly.

\hypertarget{indexing-vectors}{%
\section{Indexing vectors}\label{indexing-vectors}}

Now that we are quite familiar with different ways for generating vectors, let's discuss how we isolate specific elements from those vectors. This process is called ``indexing,'' and in \texttt{R} simple numeric (or ``positional'') indexing is intuitively based on integers, starting from ``1''. We use the square braces for numeric indexing in \texttt{R}: \texttt{{[}{]}}. For example if we want to index the first element in a vector, we simply type \texttt{{[}1{]}} after the vector. Indexing can be performed on a defined vector, or on the fly using the immediate output of a function call.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Using our vector z from above}
\NormalTok{z[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## On the fly using output from the c() function}
\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{)[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

To isolate a series of consecutive elements from a vector, we simply use the \texttt{:} character. For example, if we want to index the first (or last) 4 elements from the vector below we could do this, respectively:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{)[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10 20 30 40
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{)[}\DecValTok{4}\OperatorTok{:}\DecValTok{7}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  40  50 100 200
\end{verbatim}

For indexing discontinuous elements, we can use our old friend, the \texttt{c()} function inside of the square braces. So, if we want to index the first 3 and the 5th elements:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{)[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10 20 30 50
\end{verbatim}

Finally, we can use the \texttt{-} character to index all elements of a vector, ``minus'' other elements. When excluding even consecutive elements, however, we have to include \texttt{c()}. For instance, if we want all \textbf{except} the first 2 elements, we could do:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{)[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  30  40  50 100 200
\end{verbatim}

\hypertarget{more-complex-data-objects-in-r}{%
\section{\texorpdfstring{More complex data objects in \texttt{R}}{More complex data objects in R}}\label{more-complex-data-objects-in-r}}

Vectors are extremely important object types in \texttt{R}, for the reasons and examples we have already discussed. Other types of objects in \texttt{R} are also important, and necessary to learn about to do meaningful and efficient work. These other types of objects are more complex than vectors, but they can, in many cases, be composed of vectors.

\hypertarget{lists}{%
\subsection{lists}\label{lists}}

Lists in \texttt{R} are aggregates of different objects, and those objects can be a mixed variety of types. For example, a list could be an aggregate of 3 different vectors, even if those vectors are different lengths and contain elements of a different class. We can generate lists using the \texttt{list()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{vec2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"happy"}\NormalTok{, }\StringTok{"sad"}\NormalTok{, }\StringTok{"grumpy"}\NormalTok{)}
\NormalTok{vec3 <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"high"}\NormalTok{, }\StringTok{"low"}\NormalTok{))}

\NormalTok{mylist <-}\StringTok{ }\KeywordTok{list}\NormalTok{(vec1, vec2, vec3)}

\KeywordTok{print}\NormalTok{(mylist)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1]  10  20  30  40  50 100 200
## 
## [[2]]
## [1] "happy"  "sad"    "grumpy"
## 
## [[3]]
## [1] high low 
## Levels: high low
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(mylist)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(mylist)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 3
##  $ : num [1:7] 10 20 30 40 50 100 200
##  $ : chr [1:3] "happy" "sad" "grumpy"
##  $ : Factor w/ 2 levels "high","low": 1 2
\end{verbatim}

Let's take note of a few things from the output above. First, notice that each of the three vectors in \texttt{mylist} has a numeric (positional) index. Unlike individual vectors, however, primary elements of lists are indexed by double square braces \texttt{{[}{[}{]}{]}}. So, if we want to index the \texttt{vec2} element of \texttt{mylist}, we type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mylist[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "happy"  "sad"    "grumpy"
\end{verbatim}

Taking it one step further, if we want to index the 2nd element of the \texttt{vec2} element of \texttt{mylist}, we type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mylist[[}\DecValTok{2}\NormalTok{]][}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "sad"
\end{verbatim}

The other things we should note from our exploration of \texttt{mylist} above is that 1. It has a class when we call the \texttt{class()} function, and 2. We see a nice breakdown of the 3 components that make up \texttt{mylist} when we call the \texttt{str()} function. \texttt{str()}, which is short for ``structure,'' is an especially useful function for trying to understand the organization of complex objects in \texttt{R}.

\hypertarget{data-frames}{%
\subsection{data frames}\label{data-frames}}

There is a special class of list we very often work with in \texttt{R} called a ``data frame.'' You can think of data frames as an especially useful organizing structure for data sets. Data frames are lists of vectors, but the vectors have to be the same length. Also, the vectors (officially known as ``columns'') in data frames have names we refer to as ``column names,'' and the rows also have names. For the types of analysis we will be dealing with in this course, it helps to organize our data so that variables in our study correspond to columns and observations correspond to rows. Let's explore some practical details regarding the generation and use of data frames.

\hypertarget{creating-data-frames-in-r}{%
\subsubsection{\texorpdfstring{creating data frames in \texttt{R}}{creating data frames in R}}\label{creating-data-frames-in-r}}

We can generate data frames manually, like we did with the list \texttt{mylist} above. Here, for example, we can set up three variables (habitat, temp and elevation) as vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{habitat <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"mixed"}\NormalTok{, }\StringTok{"wet"}\NormalTok{, }\StringTok{"wet"}\NormalTok{, }\StringTok{"wet"}\NormalTok{, }\StringTok{"dry"}\NormalTok{, }\StringTok{"dry"}\NormalTok{, }\StringTok{"dry"}\NormalTok{,}\StringTok{"mixed"}\NormalTok{))}
\NormalTok{temp <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{3.4}\NormalTok{, }\FloatTok{3.4}\NormalTok{, }\FloatTok{8.4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{5.6}\NormalTok{, }\FloatTok{8.1}\NormalTok{, }\FloatTok{8.3}\NormalTok{, }\FloatTok{4.5}\NormalTok{)}
\NormalTok{elevation <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{9.2}\NormalTok{, }\FloatTok{3.8}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{5.6}\NormalTok{, }\FloatTok{4.1}\NormalTok{, }\FloatTok{7.1}\NormalTok{, }\FloatTok{5.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we can use the \texttt{data.frame()} function to incorporate the vectors into columns of the data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(habitat, temp, elevation)}
\KeywordTok{row.names}\NormalTok{(mydata) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Reedy Lake"}\NormalTok{, }\StringTok{"Pearcadale"}\NormalTok{, }\StringTok{"Warneet"}\NormalTok{, }\StringTok{"Cranbourne"}\NormalTok{, }
                       \StringTok{"Lysterfield"}\NormalTok{, }\StringTok{"Red Hill"}\NormalTok{, }\StringTok{"Devilbend"}\NormalTok{, }\StringTok{"Olinda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note above that we used a function called \texttt{row.names} to assign row names to \texttt{mydata}. The function \texttt{colnames()} does the same, but for column names.

\hypertarget{working-with-pre-loaded-base-r-data-frames.}{%
\subsubsection{\texorpdfstring{working with pre-loaded base \texttt{R} data frames.}{working with pre-loaded base R data frames.}}\label{working-with-pre-loaded-base-r-data-frames.}}

There are a few data frames that are available to work with whenever you begin an \texttt{R} session. These can be a great way to practice plotting and analysis, and in fact many examples written to accompany \texttt{R} functions include these data frames to promote reproducibility and convenience. Two of these pre-loaded data frames that are especially popular are \texttt{mtcars} and \texttt{iris}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(mtcars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\hypertarget{reading-in-data-frames-in-r}{%
\subsubsection{\texorpdfstring{reading in data frames in \texttt{R}}{reading in data frames in R}}\label{reading-in-data-frames-in-r}}

A strength of \texttt{R} is being able to import data from an external source. For example, if you have a comma- or tab- separated text file (like the UNIX-friendly formats we discussed previously), it can be easily read into \texttt{R}, by default as a data frame. One function for accomplishing this is \texttt{read.table()}, although functions like \texttt{read.delim()} can be similarly applied. Two important arguments for \texttt{read.table()} are ``header'' and ``row.names'', which indicate that there is a header row (with column names) and row label column (with row names), respectively. You also need to supply the file path and name in quotation marks (no path necessary if the file is in the current working directory), and what character is used as the field (column) delimiter. Here is an example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{YourFile <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{'yourfile.csv'}\NormalTok{, }\DataTypeTok{header=}\NormalTok{T, }\DataTypeTok{row.names=}\DecValTok{1}\NormalTok{, }\DataTypeTok{sep=}\StringTok{','}\NormalTok{)}
\NormalTok{YourFile <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{'yourfile.txt'}\NormalTok{, }\DataTypeTok{header=}\NormalTok{T, }\DataTypeTok{row.names=}\DecValTok{1}\NormalTok{, }\DataTypeTok{sep=}\StringTok{'}\CharTok{\textbackslash{}t}\StringTok{'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exporting-data-frames-in-r}{%
\subsubsection{\texorpdfstring{exporting data frames in \texttt{R}}{exporting data frames in R}}\label{exporting-data-frames-in-r}}

If you ever want to save a data frame in a format that you can work with outside of \texttt{R}, the \texttt{write.table()} function does pretty much the opposite of its ``read'' counterpart.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.table}\NormalTok{(YourFile, }\StringTok{"yourfile.csv"}\NormalTok{, }\DataTypeTok{quote=}\NormalTok{F, }\DataTypeTok{row.names=}\NormalTok{T, }\DataTypeTok{sep=}\StringTok{","}\NormalTok{)}
\KeywordTok{write.table}\NormalTok{(YourFile, }\StringTok{"yourfile.txt"}\NormalTok{, }\DataTypeTok{quote=}\NormalTok{F, }\DataTypeTok{row.names=}\NormalTok{T, }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{indexing-data-frames}{%
\subsubsection{indexing data frames}\label{indexing-data-frames}}

Indexing data frames can be acheived in two different ways. We can use numeric (positional) indexing as in the case of vectors and lists (see above). With a data frame, we can index any subset of it using two pieces of information: row coordinates and column coordinates. To accomplish this we use single square braces \texttt{{[},{]}}, in which the row coordinate(s) are typed first, followed by a comma, followed by the column cooridate(s). If we want to index all rows or all columns, we just leave the space to the left or right of the comma blank, respectively. Here are some examples for indexing subsets of the \texttt{iris} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## The first row, with all columns}
\NormalTok{iris[}\DecValTok{1}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## The first 5 rows and the first 2 columns}
\NormalTok{iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width
## 1          5.1         3.5
## 2          4.9         3.0
## 3          4.7         3.2
## 4          4.6         3.1
## 5          5.0         3.6
\end{verbatim}

With data frames, we can also use the column names to index subsets. To do this we use the \texttt{\$} character after the name of the data frame, followed by the name of the column we want to index. Again, below is a demonstration using \texttt{iris}. Indexing using column names is perhaps the most useful when defining statistical models, a topic we will reach later in the course.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## The first 5 rows of the first column}
\NormalTok{iris}\OperatorTok{$}\NormalTok{Sepal.Length[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.1 4.9 4.7 4.6 5.0
\end{verbatim}

\hypertarget{matrices}{%
\subsection{matrices}\label{matrices}}

Matrices in \texttt{R} are somewhat similar to data frames, but mixed classes among columns are not permitted, and rows and columns are only positionally indexed as opposed to having names. Positional indexing for matrices, not surprisingly, follows the \texttt{{[}rownumber,\ columnnumber{]}} convention, similar to data frames. A matrix can be generated using the \texttt{matrix()} function, as demonstrated below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Populate a 3x3 matrix with values 1 to 9}
\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{3}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    8
## [3,]    3    6    9
\end{verbatim}

\hypertarget{a-few-additional-base-r-functions-for-working-with-complex-r-objects}{%
\subsection{\texorpdfstring{A few additional base \texttt{R} functions for working with complex \texttt{R} objects}{A few additional base R functions for working with complex R objects}}\label{a-few-additional-base-r-functions-for-working-with-complex-r-objects}}

To add to your foundational knowledge of \texttt{R} functions, below are a few more functions especially useful for working with objects like data frames and matrices.

\begin{itemize}
\item
  \texttt{dim()}: returns the number of rows and columns of a data frame or matrix
\item
  \texttt{View()}: opens up a GUI ``viewer'' for visual inspection of data frames (not recommended for large data frames)
\item
  \texttt{cbind()}: combines columns into a single object, which can be used to define or build data frames or matrices
\item
  \texttt{rbind()}: combines rows into a single object, which can be used to define or build data frames or matrices
\item
  \texttt{t()}: transposes a data frame or matrix, such that rows become columns, and columns become rows
\end{itemize}

\hypertarget{some-brief-notes-on-basic-programming-in-r}{%
\section{\texorpdfstring{Some brief notes on basic programming in \texttt{R}}{Some brief notes on basic programming in R}}\label{some-brief-notes-on-basic-programming-in-r}}

At some point during your development as an \texttt{R} user you will want to programmatically manipulate \texttt{R} objects in an iterative, repeatable manner to automate tasks like plotting, simulations, and analysis. This use of the \texttt{R} language is especially relevant if you want to write your own functions. Here we touch on a few tools and approaches that will open the door to more powerful programming in \texttt{R}. These are skills that are great to practice and learn, but at a fairly foundational level for now. More advanced programming training in \texttt{R} is beyond the scope of this course.

\hypertarget{conditional-statements-with-ifelse}{%
\subsection{\texorpdfstring{conditional statements with \texttt{ifelse()}}{conditional statements with ifelse()}}\label{conditional-statements-with-ifelse}}

One fundamental structural component of computer programming languages is the idea of conditional statements, which often take the form of ``if/else'' evaluation and execution. The idea is that we can write an algorithm to evaluate a particular statement using a logical operator, and if that statement is true have the program do one thing, but if the statement is false, have it do somehting ``else.'' In \texttt{R} we can write these statements with a structure similar to other languages, but we can also use the single \texttt{R} function \texttt{ifelse()} to accomplish the same thing. The \texttt{ifelse()} function is very easy to use. The first argument is the logical evaluation, the second argument is the action to take if that statement is true, and the third argument is the action to take if false. It is also possible to nest multiple \texttt{ifelse()} function calls wihtin one another, if mulitiple evaluations need to be performed with different outcomes. Below is a simple example for using \texttt{ifelse()} to generate a vector of values (``colors''), based on another vector.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## First define a character vector}
\NormalTok{char_vec <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"treatment"}\NormalTok{,}\DecValTok{5}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"control"}\NormalTok{,}\DecValTok{3}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"treatment"}\NormalTok{, }\DecValTok{4}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"control"}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\KeywordTok{print}\NormalTok{(char_vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "treatment" "treatment" "treatment" "treatment" "treatment" "control"  
##  [7] "control"   "control"   "treatment" "treatment" "treatment" "treatment"
## [13] "control"   "control"   "control"   "control"   "control"   "control"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Generate a vector that stores the color "red" for "treatment" and "blue" for "control"}
\NormalTok{col_vec <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(char_vec}\OperatorTok{==}\StringTok{"treatment"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(col_vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "red"  "red"  "red"  "red"  "red"  "blue" "blue" "blue" "red"  "red" 
## [11] "red"  "red"  "blue" "blue" "blue" "blue" "blue" "blue"
\end{verbatim}

\hypertarget{replicate-tapply-and-apply}{%
\subsection{\texorpdfstring{\texttt{replicate()}, \texttt{tapply()}, and \texttt{apply()}}{replicate(), tapply(), and apply()}}\label{replicate-tapply-and-apply}}

In some cases we want to repeat a given process over and over again. For example, maybe we want to simulate the sampling process and generate 100 random samples of 100 values from a normal distribution. Fortunately, the \texttt{R} function \texttt{replicate()} makes this very easy.

In the example below, we ``shuffle'' the order of the integers 1 through 10 five times using \texttt{replicate()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{replicate}\NormalTok{(}\DecValTok{5}\NormalTok{, }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{replace=}\OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1] [,2] [,3] [,4] [,5]
##  [1,]    6    8    8    1    2
##  [2,]   10    1    9    3    7
##  [3,]    3    9    5    5    1
##  [4,]    8    5    7    6    8
##  [5,]    9    4   10    4    9
##  [6,]    5    2    1    2    3
##  [7,]    2   10    6    7   10
##  [8,]    4    7    4   10    6
##  [9,]    7    6    3    8    4
## [10,]    1    3    2    9    5
\end{verbatim}

Note that the first argument is the number of total iterations we want to reproduce, and that the function returns a matrix as output.

The \texttt{replicate()} function belongs to a group of functions referred to informally as the ``apply'' family. Another commonly used function from this group is \texttt{tapply()}, which allows you to apply a function to one vector (for example a numeric vector in a data frame), in a group-wise manner based on one or more factor vectors that correspond to the numeric vector. In other words, if we want to find the maximum value of variable x for each level of factor y in a data frame, we could use \texttt{tapply()} to do so. Below is an example, again using the \texttt{iris} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Find the maximum petal length for each species in the iris data frame}
\KeywordTok{tapply}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Petal.Length, iris}\OperatorTok{$}\NormalTok{Species, max)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     setosa versicolor  virginica 
##        1.9        5.1        6.9
\end{verbatim}

Note that the first argument is the numerical column, and the second is a factor column. The third is the function we wish to apply, in this case to each species separately.

Another, similar function from this family is simply called \texttt{apply()}, and it can be used to apply a function to either all rows (with the MARGIN argument set to 1) or all columns (with the MARGIN argument set to 2) in a data frame or matrix. This is especially useful for calculating summary statistics for what we call the ``margins'' of data in tables.

\hypertarget{for-loops-in-r}{%
\subsection{\texorpdfstring{for loops in \texttt{R}}{for loops in R}}\label{for-loops-in-r}}

Another fundamental concept in computer programming is the ``for loop,'' which is an algorithmic strategy for iteratively performing a task according to a pre-defined counter or loop variable, then terminating when the ``loop'' is evaluated as complete. For example, we may want to perform a specific calculation again and again for sucessive elements of an \texttt{R} object (like a data frame), and build a vector that successively stores the calculation for each iteration of the ``loop.'' We will not devote much time to for loops in \texttt{R} here, because the ``apply'' group of functions can accomplish many of the tasks you would write a for loop to perform, with much greater speed. If you do find a need for including a for loop in your future \texttt{R} programming, however, the commented example below illustrates an efficient framework in which ``pre-allocation'' of an output vector maximizes for loop speed despite some of \texttt{R}'s.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Calculate mpg/cyl and mpg/wt, for every row in mtcars and if the second is at least twice the size of the first include that ratio and another character value "Yes" in a growing 2-column dataframe. If the ratio is less than 2, then include "No" in the second column. }

\CommentTok{## first pre-allocate our new data frame, which contains NAs initially}
\NormalTok{newdf <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(mtcars}\OperatorTok{$}\NormalTok{mpg)), }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{length}\NormalTok{(mtcars}\OperatorTok{$}\NormalTok{mpg)))}

\CommentTok{## then write the for loop to do the above task for every row in mtcars}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(mtcars}\OperatorTok{$}\NormalTok{mpg)) \{}
\NormalTok{  newdf[i,}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{(mtcars}\OperatorTok{$}\NormalTok{mpg[i]}\OperatorTok{/}\NormalTok{mtcars}\OperatorTok{$}\NormalTok{wt[i])}\OperatorTok{/}\NormalTok{(mtcars}\OperatorTok{$}\NormalTok{mpg[i]}\OperatorTok{/}\NormalTok{mtcars}\OperatorTok{$}\NormalTok{cyl[i])}
\NormalTok{  newdf[i,}\DecValTok{2}\NormalTok{] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(newdf[i,}\DecValTok{1}\NormalTok{]}\OperatorTok{>=}\DecValTok{2}\NormalTok{, }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)}
\NormalTok{\}}
  \KeywordTok{print}\NormalTok{(newdf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    rep.NA..length.mtcars.mpg.. rep.NA..length.mtcars.mpg...1
## 1                     2.290076                           Yes
## 2                     2.086957                           Yes
## 3                     1.724138                            No
## 4                     1.866252                            No
## 5                     2.325581                           Yes
## 6                     1.734104                            No
## 7                     2.240896                           Yes
## 8                     1.253918                            No
## 9                     1.269841                            No
## 10                    1.744186                            No
## 11                    1.744186                            No
## 12                    1.965602                            No
## 13                    2.144772                           Yes
## 14                    2.116402                           Yes
## 15                    1.523810                            No
## 16                    1.474926                            No
## 17                    1.496726                            No
## 18                    1.818182                            No
## 19                    2.476780                           Yes
## 20                    2.179837                           Yes
## 21                    1.622718                            No
## 22                    2.272727                           Yes
## 23                    2.328967                           Yes
## 24                    2.083333                           Yes
## 25                    2.080624                           Yes
## 26                    2.067183                           Yes
## 27                    1.869159                            No
## 28                    2.643754                           Yes
## 29                    2.523659                           Yes
## 30                    2.166065                           Yes
## 31                    2.240896                           Yes
## 32                    1.438849                            No
\end{verbatim}

In the case above, we used the length of the \texttt{mtcars} data frame (number of rows) to build a pre-allocated (filled with NAs) data frame of the correct size. Then, we also used the values 1 through that length to set up our ``counter'' in the for loop. The loop stops after tasks have been completed for \texttt{i=32}, which corresponds to the final row in \texttt{mtcars}. As mentioned, it's probably better to rely on the other convenient \texttt{R} functions above for iterative processes, but pre-allocation of output objects is the way to go if you do need to rely on a for loop.

\hypertarget{fundamentals-of-plotting-in-r}{%
\section{\texorpdfstring{Fundamentals of plotting in \texttt{R}}{Fundamentals of plotting in R}}\label{fundamentals-of-plotting-in-r}}

The world of plotting in \texttt{R} is incredibly diverse, and there are entire courses dedicated to data visualization using \texttt{R}. Here we will very briefly cover a few of the most useful plotting functions and strategies using base \texttt{R}. This should be enough of an introduction to get you jump started, but you will no doubt discover more appealing and finely tuned strategies to apply in your future as an \texttt{R} user. For example, some people will find that the highly flexible, customizable package \texttt{ggplot2} and its plotting functions are preferable over base \texttt{R}. I encourage you to explore tools like this on your own, once you feel comfortable with \texttt{R} in general. We will also introduce plot- and visualization-related lessons throughout the remainder of the course, as they pertain to the analysis topic at hand.

\hypertarget{basic-plotting-with-plot}{%
\subsection{\texorpdfstring{Basic plotting with \texttt{plot()}}{Basic plotting with plot()}}\label{basic-plotting-with-plot}}

One ``high level'' plotting function in base \texttt{R} is simply called \texttt{plot()}. This function can accomplish many, many plotting goals, so we will start with it. Below, we start by calling \texttt{plot()} on a single vector that we have generated. Spend a little time examining the code, and the arguments passed to \texttt{plot()} in this example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{10.0}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.1}\NormalTok{) }
\KeywordTok{plot}\NormalTok{(seq_}\DecValTok{1}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"space"}\NormalTok{, }\DataTypeTok{ylab =}\StringTok{"function of space"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"p"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-36-1.pdf}

We only supplied the one vector (\texttt{seq\_1}) to \texttt{plot()} in this case, which resulted in the function just defining the x-axis values as the numeric positions (1 to 101) of \texttt{seq\_1}. Also, this style of plot is known as a ``scatterplot.'' There is usually more ``scatter,'' for example when plotting two variables that not perfectly related. With \texttt{plot()}, we usually want to examine the relationship between two different variables, like below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{10.0}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.1}\NormalTok{)}
\NormalTok{seq_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{10.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{, }\DataTypeTok{by =} \FloatTok{-0.1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(seq_}\DecValTok{1}\NormalTok{, seq_}\DecValTok{2}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"sequence 1"}\NormalTok{, }\DataTypeTok{ylab =}\StringTok{"sequence 2"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"p"}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-37-1.pdf}

In this example, \texttt{plot()} takes the first argument as the x-axis variable, and the second argument as the y-axis variable. You can also use the \texttt{\textasciitilde{}} to specify variables, but in this case the y-axis variable comes first (\texttt{y\ \textasciitilde{}\ x}). Also note the other arguments, which are usually named pretty intuitively. Note the axis label arguments, the type of object plotted (``p'' stands for ``points''), and the color of the plotted objects. There are many possible arguments, and many are actually set by another function called \texttt{par()}, that \texttt{plot()} calls on internally. One great resource for understanding plotting function arguments is the help menu for \texttt{par()}. I promise, if you become familiar with the \texttt{par()} documentation, you will quickly ascend the ranks of plotting prowess, and it will save you many frustrating moments in the future! I encourage you to study the \texttt{plot()} and \texttt{par()} documentation and practice using some of the other arguments that are especially useful, including ``main'', ``xlim'', ``ylim'', and ``cex'', for example.

The nice thing about graphical parameters is that, like many things in \texttt{R}, they are vectorized. So, if we want to use different symbols (look into the ``pch'' argument), colors (``col''), or sizes (look at ``cex'') of points for different observations in something like a data frame, we can supply those in the form of a vector! Taking the example above, if we want to plot the first 10 observations as blue, and the remaining observations as red, we can supply a vector of ``blues'' and ``reds'' in the appropriate order to \texttt{plot()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{10.0}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.1}\NormalTok{)}
\NormalTok{seq_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\FloatTok{10.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{, }\DataTypeTok{by =} \FloatTok{-0.1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(seq_}\DecValTok{1}\NormalTok{, seq_}\DecValTok{2}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"sequence 1"}\NormalTok{, }\DataTypeTok{ylab =}\StringTok{"sequence 2"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"p"}\NormalTok{, }
     \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\DecValTok{10}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\DecValTok{91}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-38-1.pdf}

You can see how this would be a nice way to differentiate among observation types in your data set, and produce an information-rich, single plot, as opposed to producing many plots that highlight single variables.

Sometimes we want to include multiple plots, as different panels, in the same figure. Fortunately this is made easy by the \texttt{mfrow} argument within \texttt{par()}. You simply set the dimensions, denoted by number of rows and number of columns in parentheses, before calling \texttt{plot()} repeatedly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq_square <-}\StringTok{ }\NormalTok{(seq_}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\NormalTok{(seq_}\DecValTok{2}\NormalTok{)}
\NormalTok{seq_square_new <-}\StringTok{ }\NormalTok{(seq_}\DecValTok{2}\NormalTok{)}\OperatorTok{^}\DecValTok{2}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{ (seq_}\DecValTok{1}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"time"}\NormalTok{, }\DataTypeTok{ylab =}\StringTok{"p in population 1"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"p"}\NormalTok{, }\DataTypeTok{col =} \StringTok{'red'}\NormalTok{)}
\KeywordTok{plot}\NormalTok{ (seq_}\DecValTok{2}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"time"}\NormalTok{, }\DataTypeTok{ylab =}\StringTok{"p in population 2"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"p"}\NormalTok{, }\DataTypeTok{col =} \StringTok{'green'}\NormalTok{)}
\KeywordTok{plot}\NormalTok{ (seq_square, }\DataTypeTok{xlab=}\StringTok{"time"}\NormalTok{, }\DataTypeTok{ylab =}\StringTok{"p2 in population 2"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"p"}\NormalTok{, }\DataTypeTok{col =} \StringTok{'blue'}\NormalTok{)}
\KeywordTok{plot}\NormalTok{ (seq_square_new, }\DataTypeTok{xlab=}\StringTok{"time"}\NormalTok{, }\DataTypeTok{ylab =}\StringTok{"p in population 1"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{col =} \StringTok{'yellow'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-39-1.pdf}

\hypertarget{histograms-using-hist}{%
\subsection{\texorpdfstring{Histograms using \texttt{hist()}}{Histograms using hist()}}\label{histograms-using-hist}}

We will talk more about frequency distributions and histograms later in the course, but for now it is a good idea to become familiar with one way to plot them. If we have a quantitative variable, like height, and we want to know what the distribution among individuals looks like, we can use a histogram. The function \texttt{hist()} will help us with this task. To illustrate, below we will sample values from a binomial distribution. Don't worry about what this means now, as we will return to it later, but the scenario is intuitive. Let's say we flip a coin 20 times and record the number of ``heads'' as ``successes,'' and let's further say that we perform this ``20 coin flips'' activity 1000 times. And let's assume that our coin is ``fair,'' such that the probability of getting heads on any given flip is 0.5. We can simulate this process using the \texttt{rbinom()} function and plot the results using \texttt{hist()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(}\KeywordTok{rbinom}\NormalTok{(}\DataTypeTok{n=}\DecValTok{1000}\NormalTok{, }\DataTypeTok{size=}\DecValTok{20}\NormalTok{, }\DataTypeTok{prob=}\FloatTok{0.5}\NormalTok{), }\DataTypeTok{xlab=}\StringTok{"number of heads"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"number of activities"}\NormalTok{,}
     \DataTypeTok{main=}\StringTok{"Freq. Dist. of Coin Flip Successes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/binomial function-1.pdf}

Note that, as expected, our most frequent observation is that we get 10 heads out of 20 flips.

\hypertarget{boxplots-using-boxplot}{%
\subsection{\texorpdfstring{Boxplots using \texttt{boxplot()}}{Boxplots using boxplot()}}\label{boxplots-using-boxplot}}

In many cases we want to summarize the distribution of a qunatitiative variable using ``quartiles'' (we'll cover these in depth later), and perhaps we want to do this separately for different observation types in our data set. A boxplot (or ``box and whisker plot,'' depending on how it is drawn), depicts the 1st, 2nd (median), and 3rd quartile for a vector of numeric values using a box. ``Whiskers'' are often added to define ``fences'' beyond which are putative ``outliers.'' The \texttt{boxplot()} function of base \texttt{R} is convenient to use, particularly when your data set is organized in a data frame. Below is a series of simple examples to illustrate the utility of \texttt{boxplot()}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## make a modified version of the iris data frame, which includes a "Region" factor}
\NormalTok{new_iris <-}\StringTok{ }\NormalTok{iris}
\NormalTok{new_iris}\OperatorTok{$}\NormalTok{Region <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"West"}\NormalTok{, }\DecValTok{5}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"East"}\NormalTok{, }\DecValTok{5}\NormalTok{)), }\DecValTok{15}\NormalTok{))}

\CommentTok{## make a boxplot of Sepal.Length that plots individual boxes for the separate Species}
\KeywordTok{boxplot}\NormalTok{(Sepal.Length }\OperatorTok{~}\StringTok{ }\NormalTok{Species, }\DataTypeTok{data=}\NormalTok{new_iris)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-40-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## make a boxplot of Sepal.Length that shows all 6 combinations of factor levels from Species and Region, including a different color for each species}
\KeywordTok{boxplot}\NormalTok{(Sepal.Length }\OperatorTok{~}\StringTok{ }\NormalTok{Species}\OperatorTok{*}\NormalTok{Region, }\DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{),}
        \DataTypeTok{data=}\NormalTok{new_iris, }\DataTypeTok{names=}\KeywordTok{c}\NormalTok{(}\StringTok{"set_E"}\NormalTok{,}\StringTok{"ver_E"}\NormalTok{,}\StringTok{"vir_E"}\NormalTok{,}\StringTok{"set_W"}\NormalTok{,}\StringTok{"ver_W"}\NormalTok{,}\StringTok{"vir_W"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-40-2.pdf}

Above you can see that by using the \texttt{*} character between the factors ``Species'' and ``Region in our plotting''formula" \texttt{boxplot()} produces a box for each factor level combination. Also, for \texttt{boxplot()} note that the ``col'' argument refers to the boxes themselves, so if we supply a vector of 6 colors, those will be applied to the boxes in order from left to right. Speaking of colors, an almost limitless array of colors can be specified in \texttt{R} plotting functions. Furthermore, colors can be coded using their names, or hexadecimal RGB specification. For a thorough treatment and great resources regarding colors in \texttt{R}, I recommend visiting the links at the bottom of the chapter.

\hypertarget{a-brief-introduction-to-rmarkdown}{%
\section{\texorpdfstring{A brief introduction to \texttt{RMarkdown}}{A brief introduction to RMarkdown}}\label{a-brief-introduction-to-rmarkdown}}

\texttt{RMarkdown} is a language that is distinct from \texttt{R}, but that incorporates \texttt{R} code ``chunks,'' which can be displayed and run if desired in the final, knitted output. The output can be knitted to a variety of file formats, such as \texttt{.html}, \texttt{.pdf}, or even Microsoft Word. For this course we will get into the habit of knitting to \texttt{.html}, which is the least buggy and error-prone in my experience. In the short section below, we will go over the simple steps required to write and knit your first \texttt{.Rmd} file, including the basic style elements of the language and some essential \texttt{R} chunk settings.

To get started using \texttt{RMarkdown}, you first need to make sure that you install the package \texttt{rmarkdown} from your Console, using \texttt{install.packages()}. Then, assuming you have an \texttt{RStudio} session running, click on File -\textgreater{} New File -\textgreater{} R Markdown. This will open a window in which you will type the name of your new file and the author's (your) name. A new file in your \texttt{RStudio} script editor pane (the upper left one) should appear. There will be a templated header, along with some other templated code, which you can modify based on your preferences. You may want to get rid of the pdf output line at the top for now, as we will knit to \texttt{.html} for this course. Knitting to \texttt{.pdf} requires some addtional software installation, which we don't have time to troubleshoot during this course. In any case, let's now cover some basic formatting code and code ``chunk'' types.

Below I will provide the code you would type in your own \texttt{RMarkdown} file, followed by what it looks like rendered in this \texttt{Bookdown} document, which is built using a collection of \texttt{RMarkdown} files itself!

\hypertarget{rmarkdown-formatting-basics}{%
\subsection{\texorpdfstring{\texttt{RMarkdown} formatting basics}{RMarkdown formatting basics}}\label{rmarkdown-formatting-basics}}

You can include ``nested'' headers (like the one directly above) by using \texttt{\#} symbols. For example this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Experiment with headers}

\CommentTok{### Try a third-level header}

\CommentTok{#### Or a fourth-level header}
\end{Highlighting}
\end{Shaded}

Renders as this:

\hypertarget{experiment-with-headers}{%
\section{Experiment with headers}\label{experiment-with-headers}}

\hypertarget{try-a-third-level-header}{%
\subsection{Try a third-level header}\label{try-a-third-level-header}}

\hypertarget{or-a-fourth-level-header}{%
\subsubsection{Or a fourth-level header}\label{or-a-fourth-level-header}}

Text can be rendered in bold, italics, or both like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Text}\NormalTok{ can easilly be *italicized* or **bolded** or ***both***}
\end{Highlighting}
\end{Shaded}

Which renders as this:

Text can easilly be \emph{italicized} or \textbf{bolded} or \textbf{\emph{both}}

Links can be included like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Here}\NormalTok{ is a useful link: [Rmd intro by RStudio](https://rmarkdown.rstudio.com/articles_intro.html)}

\ExtensionTok{Here}\NormalTok{ is another: [R Markdown cheat sheet](https://rmarkdown.rstudio.com/lesson-15.html)}
\end{Highlighting}
\end{Shaded}

Which render like this:

Here is a useful link: \href{https://rmarkdown.rstudio.com/articles_intro.html}{Rmd intro by RStudio}

Here is another: \href{https://rmarkdown.rstudio.com/lesson-15.html}{R Markdown cheat sheet}

For many more details on \texttt{RMarkdown} format and coding, I highly recommend the above links.

\hypertarget{rmarkdown-code-chunk-options}{%
\subsection{\texorpdfstring{\texttt{RMarkdown} code chunk options}{RMarkdown code chunk options}}\label{rmarkdown-code-chunk-options}}

Code chunks in \texttt{RMarkdown} exist to show \texttt{R} code, run the code, or both. In every \texttt{RMarkdown} file you write, you will demarcate code chunks with three ``ticks'' at the top of the chuck followed immediately by the chunk options in curly braces, on the same line, and another three ticks (on their own line) below the chunk of code. This is what a coded chunk looks like:

\begin{Shaded}
\begin{Highlighting}[]
\BaseNTok{```\{r, eval = TRUE, echo = TRUE\}}
\BaseNTok{seq(1, 10, 1)}
\BaseNTok{```}
\end{Highlighting}
\end{Shaded}

Which renders like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10
\end{verbatim}

Note that in the above example the \texttt{R} code will be both run (``evaluated'') and displayed (``echoed'') in the knitted \texttt{.html} file. If we want to suppress either or both of those from being rendered, we just set the chunk options to ``FALSE''.

When your \texttt{RMarkdown} file is completed, save any final changes, and click on the ``Knit'' icon in the toolbar, or click File -\textgreater{} Knit Document. Assuming there are no errors in your code, the rendered \texttt{.html} file should load in a new window for inspection, and the file should be saved in the same location as your \texttt{.Rmd} file. This has been a minimal treatment of \texttt{RMarkdown}, but it should be enough guidance to get you started writing your own \texttt{RMarkdown} scripts. Please consult the aforementioned \texttt{RMarkdown} resources for additional instruction, examples, and help.

\hypertarget{exercises-associated-with-this-chapter-2}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-2}}

\begin{itemize}
\tightlist
\item
  Exercise 2 (\texttt{rtutorial\_1} in \texttt{foundstats} R package)
\item
  Exercise 3 (\texttt{rtutorial\_2} in \texttt{foundstats} R package)
\end{itemize}

\hypertarget{additional-learning-resources-2}{%
\section{Additional learning resources:}\label{additional-learning-resources-2}}

\begin{itemize}
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\item
  \url{http://library.open.oregonstate.edu/computationalbiology/} - O'Neil, S.T. 2017. A Primer for Computational Biology
\item
  \url{http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf} - A nice \texttt{.pdf} menu for many \texttt{R} colors
\item
  \url{https://www.stat.ubc.ca/~jenny/STAT545A/block14_colors.html} - A good introduction to colors in \texttt{R}
\item
  \url{https://medialab.github.io/iwanthue/} - A cool automated color palette selection tool
\item
  \url{https://rmarkdown.rstudio.com/articles_intro.html} - \texttt{RStudio} guide to \texttt{RMarkdown}
\item
  \url{https://rmarkdown.rstudio.com/lesson-15.html} - \texttt{RMarkdown} ``cheat sheet''
\end{itemize}

\hypertarget{introduction-to-probability-and-probability-distributions}{%
\chapter{Introduction to Probability and Probability Distributions}\label{introduction-to-probability-and-probability-distributions}}

\hypertarget{background-2}{%
\section{Background}\label{background-2}}

A practical knowledge of statistical inference requires a basic understanding of probability. For example we often want to understand how likely a particular observation or set of observations is (e.g.~from a sample of a population), given some expectation. That expectation may be based on a theoretical probability distribution we can use to model variation in nature. In this chapter we will introduce some core concepts of probability and how those pertain to understanding observed \textbf{parameters}, or features, and variation within systems.

\hypertarget{what-is-probability}{%
\section{What is probability?}\label{what-is-probability}}

\begin{itemize}
\tightlist
\item
  \textbf{Frequency interpretation}
  ``Probabilities are understood as mathematically convenient approximations to long run relative frequencies.''
\item
  \textbf{Subjective interpretation}
  ``A probability statement expresses the opinion of some individual regarding how certain an event is to occur.''
\end{itemize}

\hypertarget{random-variables-probability}{%
\section{Random variables \& probability}\label{random-variables-probability}}

\textbf{Probability} is the expression of belief in some future outcome based on information about a system. In statistics, we often think about variables we want to understand or estimate in the real world. In terms of probability, a \textbf{random variable} can take on different values with different probabilities. The \textbf{sample space} of a random variable is the universe of all possible values for that variable. It may be helpful to think of the sample space in the form of a plotted function, where possible values of the random variable make up the x-axis, and the probability of ``drawing'' a particular value at random makes up the y-axis.

The \textbf{sample space} can be represented by a \textbf{probability distribution} when our random variable is discrete. By discrete we mean that the variable can take on a limited (finite) number of values. Meristic traits like the number of bristles on the abdomen of an insect or the number of action potentials a neuron experiences in a single window of time can only have positive integer values. Continuous random variables like human height, on the other hand, can in theory take on an infinite number of values, but are in practice limited by our measurement precision. For continuous variables, the sample space is represented by what we call a \textbf{probability density function} (PDF). Probabilities over a sample space \textbf{always sum to 1.0}, and we use tools from algebra (for probability distributions) and calculus (for probability density functions) to make use of their properties in statistical modeling and inference.

Distributions of random variables can be expressed as functions that have \textbf{moments}. These moments are metrics of a function's shape, and these can be estimated. For example the 1st, 2nd, 3rd and 4th moments of a distribution correspond to the mean, variance, skewness, and kurtosis, respectrively. For now let's just consider the first two.

\begin{itemize}
\tightlist
\item
  The expectation or mean of a random variable X is:
\end{itemize}

\[E[X] = \sum_{\text{all x}}^{}xP(X=x) = \mu\]

\begin{itemize}
\tightlist
\item
  Often we want to know how dispersed the random variable is around its mean
\item
  One measure of dispersion is the variance:
\end{itemize}

\[Var(X) = E[X^2] = \sigma^2\]

There are many \textbf{families} or \textbf{forms} of probability distributions and PDFs, and which ones we apply in statistics depend on the dynamical system we are trying to represent. We will return to the most commonly used ones below. Probability distributions and PDFs are mathematically defined by features we call \emph{parameters}, which are defined by the moments pointed out above. The parameters of the functions themselves are used to understand properties of the systems we use the functions to model. For example the normal distribution (also called the Gaussian distribution), which is probably the most famous PDF in statistics, is characterized by 2 parameters: \(mu\) (the mean) and \(sigma^{2}\) (the variance). In practical terms, those parameters dictate the central peak or ``mode'' and the spread (width), respectively. These parameters are clearly important for us in thinking about the systems we study. For example in biology we often think about random variables as values expressed by individual living things. We may consider, in theory, all possible indviduals under a given set of circumstances, and one or more random variables associated with those individuals. In statistics we call this theoretical notion of all individuals a \textbf{\emph{population}}. If it makes sense to model a random variable in that population with a particular probability distribution or PDF, it opens the door to estimating the aforementioned parameters, but in the population. Mean height definitely tells us something about the most common values in a population of humans, as does how variable height is among individuals. So you can see how probability distributions and PDFs, when applied under the appropriate assumptions, help us understand, quantify, and compare random variables in populations.

\hypertarget{probability-and-the-bernoulli-distribution}{%
\section{Probability and the Bernoulli distribution}\label{probability-and-the-bernoulli-distribution}}

To think about probability and probability distributions, let's start with an example (the Bernoulli distribution. It describes the expected outcome of a single event with probability \texttt{p}. A good example of this scenario is the flipping of a \textbf{fair} coin once.

\[Pr(X=\text{Head}) = \frac{1}{2} = 0.5 = p \]

\[Pr(X=\text{Tails}) = \frac{1}{2} = 0.5 = 1 - p \]

\begin{itemize}
\item
  If the coin isn't fair then \(p \neq 0.5\)
\item
  However, the probabilities still sum to 1
  \[ p + (1-p) = 1 \]
  The same extensions can be made for other binary possibilities, like success or failure, ``yes'' or ``no'' answers, choosing an allele at a biallelic locus from a population, etc.
\end{itemize}

\hypertarget{probability-rules}{%
\section{Probability rules}\label{probability-rules}}

Let's take a moment to cover some basic rules of probability that have to do with observance of multiple ``events.''

\begin{itemize}
\tightlist
\item
  Let's say we flip a coin twice
\item
  Represent the first flip as `X' and the second flip as `Y'
\end{itemize}

\[ Pr(\text{X=H and Y=H}) = p*p = p^2 \]
\[ Pr(\text{X=H and Y=T}) = p*p = p^2 \]
\[ Pr(\text{X=T and Y=H}) = p*p = p^2 \]
\[ Pr(\text{X=T and Y=T}) = p*p = p^2 \]

\begin{itemize}
\tightlist
\item
  What about the probability that the \texttt{H} and \texttt{T} can occur in any order?
\end{itemize}

\[ \text{Pr(X=H and Y=T) or Pr(X=T and Y=H)} = \]
\[ (p*p) + (p*p) = 2p^{2} \]

\begin{itemize}
\tightlist
\item
  These are the \textbf{`and'} and \textbf{`or'} rules of probability

  \begin{itemize}
  \tightlist
  \item
    `and' means multiply the probabilities
  \item
    `or' means sum the probabilities
  \item
    most probability distributions can be built up from these simple rules
  \end{itemize}
\end{itemize}

\hypertarget{joint-probability}{%
\section{Joint probability}\label{joint-probability}}

\[Pr(X,Y) = Pr(X) * Pr(Y)\]\\

\begin{itemize}
\tightlist
\item
  Again (as above) this multiplication is true for two \textbf{independent} events
\item
  However, for two non-independent events we also have to take into account their \textbf{covariance}
\item
  To do this we need \textbf{conditional probabilities}
\end{itemize}

\hypertarget{conditional-probability}{%
\section{Conditional probability}\label{conditional-probability}}

\begin{itemize}
\tightlist
\item
  For two \textbf{independent} variables
\end{itemize}

\[Pr(Y|X) = Pr(Y)\text{ and }Pr(X|Y) = Pr(X)\]

\begin{itemize}
\tightlist
\item
  For two \textbf{non-independent} variables
\end{itemize}

\[Pr(Y|X) \neq Pr(Y)\text{ and }Pr(X|Y) \neq Pr(X)\]

Variables that are non-independent have a shared variance, which is also known as the \textbf{covariance}. You can think of this as two variables that consistently deviate from their respective meansCovariance standardized to a mean of zero and a unit standard deviation is \textbf{correlation} We will explore

\hypertarget{a-brief-note-on-likelihood-vs.probability}{%
\section{A brief note on likelihood vs.~probability}\label{a-brief-note-on-likelihood-vs.probability}}

\begin{itemize}
\item
  The \textbf{probability} of an event is the proportion of times that the event would occur if we repeated a random trial over and over again under the same conditions.
\item
  The \textbf{likelihood} is the probability of observing a particular set of data or outcome, given a particular parameter value.
\end{itemize}

\texttt{L{[}parameter\textbar{}data{]}\ =\ Pr{[}data\textbar{}parameter{]}}

Extending from this, the parameter value at which the likelihood is maximized is called the maximum likelihood estimate (MLE). You don't need to worry too much about likelihood in this course, but realize that many of our formualae for estimating parameters from data actually produce maximum likelihood estimates. The formula we use to calculate a mean from a sample of observations, for example, produces the maximum likelihood estimate for the population mean from which that sample was taken. The \textbf{likelihood function} (for a single parameter) or \textbf{likelihood surface} (for multiple parameters) describes the relationship between different parameter values and their likelihood. We can't always derive convenient equations to obtain maximum likelihood estimates, however, and in those cases we may have to rely on algorithmic searches of ``parameter space'' to find the MLE.

\hypertarget{probability-distributions-commonly-used-in-biological-statistics}{%
\section{Probability distributions commonly used in biological statistics}\label{probability-distributions-commonly-used-in-biological-statistics}}

(Many of these are thanks to Sally Otto at UBC)

\hypertarget{discrete-probability-distributions}{%
\subsection{Discrete Probability Distributions}\label{discrete-probability-distributions}}

\hypertarget{geometric-distribution}{%
\subsubsection{\texorpdfstring{\textbf{Geometric Distribution}}{Geometric Distribution}}\label{geometric-distribution}}

\begin{itemize}
\tightlist
\item
  If a single event has two possible outcomes the probability of having to observe \texttt{k} trials before the first ``one'' appears is given by the \textbf{geometric distribution}
\item
  The probability that the first ``one'' would appear on the first trial is \texttt{p}, but the probability that the first ``one'' appears on the second trial is \texttt{(1-p)*p}
\item
  By generalizing this procedure, the probability that there will be \texttt{k-1} failures before the first success is:
\end{itemize}

\[P(X=k)=(1-p)^{k-1}p\]

\begin{itemize}
\tightlist
\item
  mean = \(\frac{1}{p}\)
\item
  variance = \(\frac{(1-p)}{p^2}\)
\end{itemize}

\hypertarget{geometric-distribution-1}{%
\subsubsection{\texorpdfstring{\textbf{Geometric Distribution}}{Geometric Distribution}}\label{geometric-distribution-1}}

\begin{itemize}
\tightlist
\item
  Example: If the probability of extinction of an endangered population is estimated to be 0.1 every year, what is the expected time until extinction?
\end{itemize}

\begin{center}\includegraphics[width=0.8\linewidth]{images/prob.017} \end{center}

\hypertarget{binomial-distribution}{%
\subsubsection{\texorpdfstring{\textbf{Binomial Distribution}}{Binomial Distribution}}\label{binomial-distribution}}

\begin{itemize}
\item
  A \textbf{binomial distribution} results from the \textbf{combination} of several independent Bernoulli events
\item
  \textbf{Example}

  \begin{itemize}
  \tightlist
  \item
    Pretend that you flip 20 fair coins

    \begin{itemize}
    \tightlist
    \item
      or collect alleles from a heterozygote
    \end{itemize}
  \item
    Now repeat that process and record the number of heads
  \item
    We expect that most of the time we will get approximately 10 heads
  \item
    Sometimes we get many fewer heads or many more heads
  \end{itemize}
\item
  The distribution of probabilities for each combination of outcomes is
\end{itemize}

\[\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}\]

\begin{itemize}
\tightlist
\item
  \texttt{n} is the total number of trials
\item
  \texttt{k} is the number of successes
\item
  \texttt{p} is the probability of success
\item
  \texttt{q} is the probability of not success
\item
  For binomial as with the Bernoulli \texttt{p\ =\ 1-q}
\end{itemize}

\begin{center}\includegraphics[width=1\linewidth]{images/week_2.003} \end{center}

\hypertarget{negative-binomial-distribution}{%
\subsubsection{\texorpdfstring{\textbf{Negative Binomial Distribution}}{Negative Binomial Distribution}}\label{negative-binomial-distribution}}

\begin{itemize}
\tightlist
\item
  Extension of the geometric distribution describing the waiting time until \texttt{r} ``ones'' have appeared.
\item
  Generalizes the geometric distribution
\item
  Probability of the \(r^{th}\) ``one'' appearing on the \(k^{th}\) trial:
\end{itemize}

\[P(X=k)=(\frac{k-1}{r-1})p^{r-1}(1-p)^{k-r}p\]

which simplifies to

\[P(X=k)=(\frac{k-1}{r-1})p^{r}(1-p)^{k-r}\]

\begin{itemize}
\item
  mean = \(\frac{r}{p}\)
\item
  variance = \(r(1-p)/p^2\)
\item
  Example: If a predator must capture 10 prey before it can grow large enough to reproduce
\item
  What would the mean age of onset of reproduction be if the probability of capturing a prey on any given day is 0.1?
\item
  Notice that the variance is quite high (\textasciitilde{}1000) and that the distribution looks quite skewed
\end{itemize}

\begin{center}\includegraphics[width=0.5\linewidth]{images/prob.018} \end{center}

\hypertarget{poisson-probability-distribution}{%
\subsubsection{\texorpdfstring{\textbf{Poisson Probability Distribution}}{Poisson Probability Distribution}}\label{poisson-probability-distribution}}

\begin{itemize}
\item
  Another common situation in biology is when each trial is discrete, but the number of observations of each outcome is observed/counted
\item
  Some examples are

  \begin{itemize}
  \tightlist
  \item
    counts of snails in several plots of land
  \item
    observations of the firing of a neuron in a unit of time
  \item
    count of genes in a genome binned to units of 500 AA
  \end{itemize}
\item
  Just like before you have `successes', but

  \begin{itemize}
  \tightlist
  \item
    now you count them for each replicate
  \item
    the replicates now are units of area or time
  \item
    the values can now range from 0 to a large number
  \end{itemize}
\end{itemize}

\begin{itemize}
\tightlist
\item
  For example, you can examine 1000 genes

  \begin{itemize}
  \tightlist
  \item
    count the number of base pairs in the coding region of each gene
  \item
    what is the probability of observing a gene with `r' bp?
  \end{itemize}
\item
  \texttt{Pr(Y=r)} is the probability that the number of occurrences of an event \texttt{y} equals a count \texttt{r} in the total number of trials
\end{itemize}

\[Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}\]

\begin{itemize}
\tightlist
\item
  Note that this is a single parameter function because \(\mu = \sigma^2\)
\item
  The two together are often just represented by \(\lambda\)
\end{itemize}

\[Pr(y=r) = \frac{e^{-\lambda}\lambda^r}{r!}\]

\begin{itemize}
\tightlist
\item
  This means that for a variable that is truly Poisson distributed:

  \begin{itemize}
  \tightlist
  \item
    the mean and variance should be equal to one another
  \item
    variables that are approximately Poisson distributed but have a larger variance than mean are often called `overdispersed'
  \item
    quite common in RNA-seq and microbiome data
  \end{itemize}
\end{itemize}

\hypertarget{poisson-probability-distribution-gene-length-by-bins-of-500-nucleotides}{%
\paragraph{Poisson Probability Distribution \textbar{} gene length by bins of 500 nucleotides}\label{poisson-probability-distribution-gene-length-by-bins-of-500-nucleotides}}

\begin{center}\includegraphics[width=0.8\linewidth]{images/week_2.004} \end{center}

\hypertarget{poisson-probability-distribution-increasing-parameter-values-of-lambda}{%
\paragraph{\texorpdfstring{Poisson Probability Distribution \textbar{} increasing parameter values of \(\lambda\)}{Poisson Probability Distribution \textbar{} increasing parameter values of \textbackslash{}lambda}}\label{poisson-probability-distribution-increasing-parameter-values-of-lambda}}

\begin{center}\includegraphics[width=0.7\linewidth]{images/week_2.005} \end{center}

\hypertarget{continuous-probability-distributions}{%
\subsection{\texorpdfstring{\textbf{Continuous probability distributions}}{Continuous probability distributions}}\label{continuous-probability-distributions}}

P(observation lies within dx of x) = f(x)dx

\[P(a\leq X \leq b) = \int_{a}^{b} f(x) dx\]

Remember that the indefinite integral sums to one

\[\int_{-\infty}^{\infty} f(x) dx = 1\]

\texttt{E{[}X{]}} may be found by integrating the product of \texttt{x} and the probability density function over all possible values of \texttt{x}:

\[E[X] = \int_{-\infty}^{\infty} xf(x) dx \]

\(Var(X) = E[X^2] - (E[X])^2\), where the expectation of \(X^2\) is

\[E[X^2] = \int_{-\infty}^{\infty} x^2f(x) dx \]

\hypertarget{uniform-distribution}{%
\subsubsection{\texorpdfstring{\textbf{Uniform Distribution}}{Uniform Distribution}}\label{uniform-distribution}}

\[E[X] = \int_{a}^{b} x\frac{1}{b-a} dx = \frac{(a+b)}{2} \]

\begin{center}\includegraphics[width=1\linewidth]{images/prob.019} \end{center}

\hypertarget{exponential-distribution}{%
\subsubsection{\texorpdfstring{\textbf{Exponential Distribution}}{Exponential Distribution}}\label{exponential-distribution}}

\[f(x)=\lambda e^{-\lambda x}\]

\begin{itemize}
\tightlist
\item
  \texttt{E{[}X{]}} can be found be integrating \(xf(x)\) from 0 to infinity
\end{itemize}

\begin{itemize}
\tightlist
\item
  leading to the result that
\end{itemize}

\begin{itemize}
\item
  \(E[X] = \frac{1}{\lambda}\)
\item
  \(E[X^2] = \frac{1}{\lambda^2}\)
\item
  For example, let equal the instantaneous death rate of an individual.
\item
  The lifespan of the individual would be described by an exponential distribution (assuming that does not change over time).
\end{itemize}

\begin{center}\includegraphics[width=0.7\linewidth]{images/prob.020} \end{center}

\hypertarget{gamma-distribution}{%
\subsubsection{\texorpdfstring{\textbf{Gamma Distribution}}{Gamma Distribution}}\label{gamma-distribution}}

\begin{itemize}
\tightlist
\item
  The gamma distribution generalizes the exponential distribution.
\item
  It describes the waiting time until the \(r^{th}\) event for a process that occurs randomly over time at a rate \(\lambda\) :
\end{itemize}

\[f(x) = \frac{e^{-\lambda x}\lambda x^{r-1}}{(r-1)!}\lambda\]

\[ Mean =  \frac{r}{\lambda} \]
\[ Variance = \frac{r}{\lambda^2} \]

\begin{itemize}
\tightlist
\item
  \textbf{Example}: If, in a PCR reaction, DNA polymerase synthesizes new DNA strands at a rate of 1 per millisecond, how long until 1000 new DNA strands are produced?
\end{itemize}

\begin{itemize}
\tightlist
\item
  Assume that DNA synthesis does not deplete the pool of primers or nucleotides in the chamber, so that each event is independent of other events in the PCR chamber.
\end{itemize}

\hypertarget{the-gaussian-or-normal-distribution}{%
\subsubsection{The Gaussian or Normal Distribution}\label{the-gaussian-or-normal-distribution}}

As mentioned, the normal distribution has two parameters.

\hypertarget{mu-and-sigma}{%
\paragraph{\texorpdfstring{(\(\mu\) and \(\sigma\))}{(\textbackslash{}mu and \textbackslash{}sigma)}}\label{mu-and-sigma}}

\begin{center}\includegraphics[width=0.4\linewidth]{images/week_2.032} \end{center}

where
\[\large \pi \approx 3.14159\]

\[\large \epsilon \approx 2.71828\]

To write that a variable (v) is distributed as a normal distribution with mean \(\mu\) and variance \(\sigma^2\), we write the following:

\[\large v \sim \mathcal{N} (\mu,\sigma^2)\]

\hypertarget{normal-pdf-estimates-of-mean-and-variance}{%
\paragraph{Normal PDF \textbar{} estimates of mean and variance}\label{normal-pdf-estimates-of-mean-and-variance}}

Estimate of the mean from a single sample

\[\Large \bar{x} = \frac{1}{n}\sum_{i=1}^{n}{x_i} \]

Estimate of the variance from a single sample

\[\Large s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i - \bar{x})^2} \]

\begin{center}\includegraphics[width=0.9\linewidth]{images/week_2.010} \end{center}

\hypertarget{why-is-the-normal-special-in-biology}{%
\paragraph{Why is the Normal special in biology?}\label{why-is-the-normal-special-in-biology}}

\begin{center}\includegraphics[width=1\linewidth]{images/week_2.013} \end{center}

\begin{center}\includegraphics[width=1\linewidth]{images/week_2.015} \end{center}

\begin{center}\includegraphics[width=0.6\linewidth]{images/week_2.014} \end{center}

\hypertarget{parent-offspring-resemblance}{%
\paragraph{Parent-offspring resemblance}\label{parent-offspring-resemblance}}

\begin{center}\includegraphics[width=0.45\linewidth]{images/week_2.016} \end{center}

\hypertarget{genetic-model-of-complex-traits}{%
\paragraph{Genetic model of complex traits}\label{genetic-model-of-complex-traits}}

\begin{center}\includegraphics[width=0.9\linewidth]{images/week_2.017} \end{center}

\hypertarget{distribution-of-f_2-genotypes-really-just-binomial-sampling}{%
\paragraph{\texorpdfstring{Distribution of \(F_2\) genotypes \textbar{} really just binomial sampling}{Distribution of F\_2 genotypes \textbar{} really just binomial sampling}}\label{distribution-of-f_2-genotypes-really-just-binomial-sampling}}

\begin{center}\includegraphics[width=0.7\linewidth]{images/week_2.018} \end{center}

\hypertarget{why-else-is-the-normal-special}{%
\paragraph{Why else is the normal special?}\label{why-else-is-the-normal-special}}

\begin{itemize}
\tightlist
\item
  The normal distribution is immensely useful because of the central limit theorem, which says that the he mean of many random variables independently drawn from the same distribution is distributed approximately normally
\item
  One can think of numerous situations, such as

  \begin{itemize}
  \tightlist
  \item
    when multiple genes contribute to a phenotype
  \item
    or that many factors contribute to a biological process
  \end{itemize}
\item
  In addition, whenever there is variance introduced by stochastic factors the central limit theorem holds
\item
  Thus, normal distributions occur throughout genomics
\item
  It's also the basis of the majority of classical statistics
\end{itemize}

\hypertarget{a-note-on-z-scores-of-normal-variables}{%
\paragraph{A note on z-scores of normal variables}\label{a-note-on-z-scores-of-normal-variables}}

\begin{itemize}
\tightlist
\item
  Often we want to make variables more comparable to one another
\item
  For example, consider measuring the leg length of mice and of elephants

  \begin{itemize}
  \tightlist
  \item
    Which animal has longer legs in absolute terms?
  \item
    Proportional to their body size?
  \item
    Proportional to their body size?
  \end{itemize}
\item
  A good way to answer these last questions is to use `z-scores'
\end{itemize}

\begin{itemize}
\tightlist
\item
  z-scores are standardized to a mean of 0 and a standard deviation of 1
\item
  We can modify any normal distribution to have a mean of 0 and a standard deviation of 1
\item
  Another term for this is the standard normal distribution
\end{itemize}

\[\huge z_i = \frac{(x_i - \bar{x})}{s}\]

\hypertarget{exercises-associated-with-this-chapter-3}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-3}}

\begin{itemize}
\tightlist
\item
  Problem Set 2
\end{itemize}

\hypertarget{additional-learning-resources-3}{%
\section{Additional learning resources:}\label{additional-learning-resources-3}}

\begin{itemize}
\item
  Irizarry, R. A. Introduction to Data Science. \url{https://rafalab.github.io/dsbook/} - A gitbook written by a statistician, with great introductions to key topics in statistical inference.
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\end{itemize}

\hypertarget{parameter-estimation-basics-and-the-sampling-process}{%
\chapter{Parameter Estimation Basics and the Sampling Process}\label{parameter-estimation-basics-and-the-sampling-process}}

\hypertarget{background-3}{%
\section{Background}\label{background-3}}

A major goal of statistics is to estimate \textbf{parameters}, or features, of a population so that we can compare them to values that are of practical importance to our understanding of the system, or to compare parameter estimates between and among different populations. We may want to know whether the slope of a relationship between two variables is really different from zero (e.g.~no relationship) or whether the average value of a trait in a population is especially extreme relative to other populations. Some of these tasks are getting into the territory of hypothesis testing (which we will get to later), but estimating the parameters we ultimately may want to compare is an important first step. In this chapter we will discuss how parameters are estimated from samples we, as scientists, collect.

\hypertarget{understanding-populations-and-their-parameters}{%
\section{Understanding populations and their parameters}\label{understanding-populations-and-their-parameters}}

From a probabilistic standpoint we often think about the systems we study as theoretical ``populations'' of entities, wherein each population is defined by a particular set of shared conditions. Almost always (unless we are simulating a population), it is simply not possible to measure or observe all of the entities in a population, so if we want to understand the population we need to estimate its parameters. As empiricists, how do we estimate parameters? As you probably know, we conduct studies and/or experiments in which we take random samples and measure variables in the sample that correspond to the population parameters of interest. It is important to consider whether these samples are random if we apply the probability distribution and random variable framework (from last chapter) to our statistical inference.

When we take a random sample from a population we can estimate a parameter by doing a calculation based on the sample itself. So, maybe we want to estimate the mean mass of all adult rainbow trout in a lake. We can't measure all fish, so we take a random sample and calculate a sample mean mass. In typed notation, we use letters from the greek alphabet to represent population parameters, and letters from the latin alphabet to represent sample attributes. For example, a population mean is represented by \(\sigma\), and a sample mean by \(\bar{x}\).

A sample mean gives us an estimate of the true population mean, but because it is a random sample, we don't know how close our estimate is to the true parameter value. We do know from first principles of probability that as the size of our sample increases, so does the closeness of our estimate to the true value. That is, as our sample size approaches our actual population size (which may be infinitely large depending on how defined), our estimate approaches the true parameter value. We also know that the closeness of our estimate to the real parameter value depends on how much ``spread'' there is in the distribution of values that defines the population. These two values - the sample size and the spread of the distribution - contribute to what is known as the \textbf{standard error} of a random variable. The standard error for any given sample attribute (such as a sample mean), can be calculated either based on distributional assumptions, or by a process called ``resampling.'' We will return to these procedures below. The important thing to understand for now, is that the standard error can be used to indicate how close a sample-based estimate is to the actual population parameter value.

One way to get a handle on how the sampling process relates to parameter estimation is to actually simulate a population, based on a known probability distribution for example, and take multiple samples of varying sizes from that population. You can calculate estimates (such as the mean and standard deviation) from each sample, then visualize how they vary and how close they are to the true parameter value. Here is a quick example, in which we simulate a random, normally distributed variable in a population of 1000 individuals, take 50 random samples of 10 individuals each time, and look at the distribution of sample means across those 50 samples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{32}\NormalTok{)}
\NormalTok{pop_var <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{30}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{8}\NormalTok{) }\CommentTok{#Define our population random variable}
\NormalTok{samps_var <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DecValTok{50}\NormalTok{, }\KeywordTok{sample}\NormalTok{(pop_var, }\DecValTok{10}\NormalTok{)) }\CommentTok{#Take 50 samples of size 10}
\NormalTok{samps_var_means <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(samps_var, }\DecValTok{2}\NormalTok{, mean) }\CommentTok{#Calculate the mean from each sample}
\KeywordTok{hist}\NormalTok{(samps_var_means) }\CommentTok{#Plot the distribution of sample means}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-60-1.pdf}
In the above example we see that we do most commonly get a sample mean near the expected population value of 30, but that there is quite a bit of variation! This \textbf{sampling} variation is what we have to deal with, and account for, as empircial scientists. If this had been a real-world scenario, we likely would be basing our estimate for \(\sigma\) on just a single \(\bar{x}\). In this simulation nine of our samples gave us an estimated mean \textless{} 28. Whether that estimate is ``close enough'' to the true value of 30 depends on a variety of questions we may have about the system, but this idea of uncertainty in our parameter estimation is important. Fortunately we can rely on a number of tools to evaluate how close we think our sample-based estimates are to population parameter values in the real world, which we visit below.

\hypertarget{more-on-parameter-estimation-and-sampling-distributions}{%
\section{More on parameter estimation and sampling distributions}\label{more-on-parameter-estimation-and-sampling-distributions}}

The exercise above illustrates the concept of a sampling distribution. We sampled over and over again (50 times) and calculated the mean for each sample to demonstrate the sampling distribution for the mean, our original parameter of interest. One important point is that the sampling distribution for a given parameter is often very different from the variable's distribution in the population. In many cases, the sampling distribution is normal or approximately so.

\begin{center}\includegraphics[width=1\linewidth]{images/week_2.025} \end{center}

For most real world data sets we can't empirically determine a sampling distribution by taking many actual samples, because we often have just the one sample. Fortunately we can rely on the Central Limit Theorem (discussed in the last chapter) to make some assumptions about sampling distributions, particularly when estimating a mean from a single sample, or when estimating most any parameter using a ``pseudo'' or re-sampling process we refer to as ``bootstrapping.''

As noted, the \textbf{expected value} of a very large number of sample estimates is the value of the parameter being estimated. The sampling distribution of an estimate models all values we might have obtained from our sample and their probabilities of occurrence. The standard error of an estimate can be conceptualized as the standard deviation of a sampling distribution. So, whenever we obtain a parameter estimate, we need to include the standard error in some form or another, which is a measure of the precision of about our estimate.

\hypertarget{calculating-the-standard-error-of-the-mean}{%
\section{Calculating the standard error of the mean}\label{calculating-the-standard-error-of-the-mean}}

Because distribution functions for sums of many independent events are approximately normal (the Central Limit Theorem), and because dividing any normally distributed random variable by a single value (constant) is also normally distributed, this leads to a special case for sample means. These assumptions work out very conveniently for means, because a mean is just that: a sum of observations divided by the total number of observations. Because we can assume that sampling distributions for means are effectively normal, we can use attributes of the normal probability density function to conveniently calculate the standard error of any mean estimated from a random sample.

\begin{center}\includegraphics[width=1\linewidth]{images/week_2.028} \end{center}

\[\huge \sigma_{\bar{x}} \approx s_{\bar{x}} = \frac{s}{\sqrt{n}} \]

\begin{itemize}
\tightlist
\item
  Where \(s_{\bar{x}}\) is the estimated standard error of the distribution of the mean estimates
\item
  This is usually just referred to as the `standard error of the mean' (SEM)
\item
  Note that this \textbf{is not} the standard deviation of the original distribution
\item
  Importantly, the SEM will go down as the sample size increases
\end{itemize}

\hypertarget{the-bootstrap-to-estimate-parameters-and-the-standard-error}{%
\section{The bootstrap to estimate parameters and the standard error}\label{the-bootstrap-to-estimate-parameters-and-the-standard-error}}

Unfortunately, most other kinds of estimates do not have this amazing property, but we can rely on another approach to calculate the standard error. This involves generating your own sampling distribution for the estimate using the ``bootstrap,'' a method invented by Efron (1979). We call the bootstrap, and other methods that do not rely on distributional assumptions of the variable itself, ``nonparametric'' approaches.

We can actually quite easily use \texttt{R} to take a random sample of individuals (with replacement) from the original data to implement the bootstrap via the following steps:

\begin{itemize}
\tightlist
\item
  Calculate the estimate using the measurements in the bootstrap sample (step 1)

  \begin{itemize}
  \tightlist
  \item
    This is the first bootstrap replicate estimate
  \end{itemize}
\item
  Repeat steps 1 and 2 a large number of times (1000 times is reasonable)
\item
  Calculate the sample standard deviation of all the bootstrap replicate estimates obtained in step 3
\item
  The resulting quantity is called the ``bootstrap standard error''
\end{itemize}

The bootstrap is effective for a number of reasons. It can be applied to almost any sample statistic, including means, proportions, correlations, and regression parameters. It works when there is no ready formula for a standard error, for example when estimating the median, trimmed mean, correlation, eigenvalue, etc. Is nonparametric, so doesn't require normally-distributed data, as mentioned. It works well for parameter estimates that are based on complicated sampling procedures or calculations. For example, it is used to assess confidence in local relationships within phylogenetic trees.

\hypertarget{confidence-intervals}{%
\section{Confidence intervals}\label{confidence-intervals}}

A concept related to parameter estimates and their standard errors is the idea of the ``confidence interval.'' A confidence interval is a range of values about a parameter \textbf{estimate}, such that we are X\% certain that the true population parameter value lies within that interval. We will return to the topic again in the hypothesis testing section of this book, when we discuss the \emph{t} distribution in the context of comparing two population means. For now, know that for a normally distributed sample, a confidence interval about the population mean can be calculated using the \texttt{t.test()} function in base \texttt{R}. The 95\% confidence interval is commonly reported in statistical analysis results, by convention, but other values are occasionally reported as well.

\hypertarget{the-relationship-between-mean-and-variance}{%
\section{The relationship between mean and variance}\label{the-relationship-between-mean-and-variance}}

To add one last, short note on the comparison of population standard deviations, it is important to understand that population means and variances (and hence standard deviations) tend to have a strong, positive relationship. This means that an otherwise similarly shaped distribution, but with a much larger mean, will by default have a much larger standard deviation as well. The positive mean-variance relationship tends to make direct comparisons of variation between populations with very different means difficult. For instance, comparing the standard deviation for a body measurement in a population of mice, with the same body measurement in a population of elephants is not meaningful. To make standard deviations comparable across populations with very different means, we can instead compare a standardized metric called the ``coefficient of variation'' (CV), which is simply the sample standard deviation divided by the sample mean (and usually expressed as a \% by multiplying by 100).

\hypertarget{exercises-associated-with-this-chapter-4}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-4}}

\begin{itemize}
\tightlist
\item
  Problem Set 2
\end{itemize}

\hypertarget{additional-learning-resources-4}{%
\section{Additional learning resources:}\label{additional-learning-resources-4}}

\begin{itemize}
\item
  Irizarry, R. A. Introduction to Data Science. \url{https://rafalab.github.io/dsbook/} - A gitbook written by a statistician, with great introductions to key topics in statistical inference.
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\end{itemize}

\hypertarget{principles-of-experiment-and-study-design}{%
\chapter{Principles of Experiment and Study Design}\label{principles-of-experiment-and-study-design}}

\hypertarget{background-4}{%
\section{Background}\label{background-4}}

In order to obtain proper estimates for the parameters we care about as scientists, we have to keep in mind the probability and sampling principles discussed in the preceeding chapters. Studies and experiments enable us to take samples and effectively make statistical inferences, but only if we design the studies in structured ways that adhere to (and therefore take advantage of) sampling theory assumptions. Two of the most important conepts along these lines are \textbf{replication} and \textbf{randomization}.

As you learned in the last chapter, our uncertainty (as measured by the standard error) about estimation of a particular population parameter is high when our sample size is low, and this uncertainty decreases as we increase sample size. For this reason, the number of ``replicates'' in a study is intrinsically tied to uncertainty. We will return to this in the context of ``statistical power'' in the next chapter. If we require little uncertainty when estimating or comparing parameters, we need adequate replication. Furthermore, sampling has to be performed at the level of each population in question. For example, if we are comparing two treatments in an experiment, we need adequate replication within both of those treatments, because they represent two different populations. We also learned that sampling theory is based on the notion of the random variable, which is modeled by random draws of observations from a theoretical population. Individuals in a study or experiment need to be selected or assigned to groups randomly and independent of one another to avoid bias in our parameter estimates and comparisons. In this chapter we will discuss how these considerations, and related ones, have been formalized into conventions of experimental design that should be followed across the scientific community.

\hypertarget{what-is-an-experimental-study}{%
\section{What is an experimental study?}\label{what-is-an-experimental-study}}

We should start by describing a few different categories of study. All categories usually involve, either directly or inderectly, the estimation of population parameters. In some studies, which we refer to as ``experimental,'' the researcher assigns treatments to units is such a way to ``isolate'' the effects of the treatments on the variable(s) of interest.

\emph{A quick aside: In study design parlance, we refer to these variables of interest as ``dependent'' or ``response'' variables. Response variables are usually the basis for the population parameters we are trying to estimate and compare. The variables we are manipulating (as in an experiment) or otherwise measuring for their explanatory potential, are called ``independent'' or ``explanatory'' variables.}

In an observational study, on the other hand, we let nature ``do the assigning'' of treatments to units, and we simply observe and measure the relationships among variables, whether they are causal or not. The crucial advantage of experiments over observational studies, then, derives from the random assignment of treatments to units. Random assignment, or randomization, minimizes the influence of confounding variables. To illustrate the advantage of randmonization, consider the following example.

\hypertarget{a-hypothetical-study-example}{%
\subsection{A hypothetical study example}\label{a-hypothetical-study-example}}

Let's say that we know survival of climbers of Mount Everest is higher for individuals taking supplemental oxygen than those who don't. As physiologists, or anthropologists, or exercise physiologists, or sociolgists, or tourism economists, we may want to know \textbf{\emph{why}} this observation is true.

One possibility is that supplemental oxygen (our explanatory variable) really does cause higher survival (our response variable). The other is that the two variables are associated because other variables affect both supplemental oxygen and survival. For instance, use of supplemental oxygen might be a benign indicator of a greater overall preparedness of the climbers that use it. Variables (like preparedness) that distort the causal relationship between the measured variables of interest (oxygen use and survival) are called \textbf{confounding variables}. They are correlated with the variable of interest, and therefore they prevent a decision about cause and effect. With randomized assignment, no confounding variables will be associated with treatment except by chance, so if sample sizes are large enough no spurious conclusions about cause and effect should be reached.

\hypertarget{basic-study-design-terminology}{%
\section{Basic study design terminology}\label{basic-study-design-terminology}}

Many experimentalists, and indeed most statisticians working with experimental data, use a common set of terms to describe elements of a study. Those terms are used and defined in this section. For example, we defined response (or dependent) and explanatory (or independent) variables above, and more will be introduced below. It is good to become familiar with these terms, as they will continually pop up as you read the literature within your own study discipline, read technical publications and online posts regarding statistical analysis, and engage in discussions with collaborators and analysts. To start out, below is a table of some general terms we have covered already, with some more formal definitions.

\includegraphics[width=12.4in]{images/Logan_ExpTerms}

From Logan, M. 2010

\hypertarget{clinical-trials}{%
\section{Clinical trials}\label{clinical-trials}}

The gold standard of experimental designs is the \textbf{clinical trial}. In fact, experimental design in all areas of biology have been informed by procedures used in clinical trials. A clinical trial is an experimental study in which two or more treatments are assigned to human subjects, usually with some design specifics we visit below. The design of clinical trials has been refined and approached with great care because the cost of making a mistake with human subjects is so high. Experiments on nonhuman subjects are simply called ``laboratory experiments,'' or ``field experiments'' in cases where the experiment is not confined to a small, indoor space.

\hypertarget{a-clinical-trial-example}{%
\subsection{A clinical trial example}\label{a-clinical-trial-example}}

Transmission of the HIV-1 virus via sex workers contributes to the rapid spread of AIDS in Africa. The spermicide nonoxynol-9 had shown \emph{in vitro} activity against HIV-1, which motivated a clinical trial by van Damme et al. (2002). In this study the authors tested whether a vaginal gel containing the chemical would reduce the risk of acquiring the disease by female sex workers. Data were gathered on a volunteer sample of 765 HIV-free sex-workers in six clinics in Asia and Africa. Two gel treatments were assigned randomly to women at each clinic, one gel containing nonoxynol-9 and the other a placebo. Neither the subjects nor the researchers making observations at the clinics knew who received the treatment and who got the placebo. The table below shows the raw data.

\includegraphics[width=12.9in]{images/images_6a.005}

A major goal of experimental design is to eliminate bias and to reduce sampling error when estimating and testing effects of one variable on another.

To reduce bias, the experiment described above included the following:

\begin{itemize}
\tightlist
\item
  A \textbf{Simultaneous control group}: Inclusion of both the treatment of interest and a control group (the women receiving the placebo).
\item
  \textbf{Randomization}: Treatments were randomly assigned to women at each clinic.
\item
  \textbf{Blinding}: Neither the subjects nor the clinicians knew which women were assigned which treatment.
\end{itemize}

To reduce the effects of sampling error, the experiment included the following:

\begin{itemize}
\tightlist
\item
  \textbf{Replication}: A study is carried out on multiple, independent subjects.
\item
  \textbf{Balance}: Equal sample sizes across treatment groups. In this case, the number of women was nearly equal in the two groups at every clinic.
\item
  \textbf{Blocking}: Treatments are applied systematically within larger groups that likely explain large amounts of variation. Here subjects were grouped according to the clinic they attended, yielding multiple repetitions of the same experiment in different settings (``blocks'').
\end{itemize}

Let's consider each of these design elements in turn, in a bit more depth.

\hypertarget{simultaneous-control-groups}{%
\subsection{Simultaneous control groups}\label{simultaneous-control-groups}}

In clinical trials either a placebo or the currently accepted treatment should be provided. In experiments requiring intrusive methods to administer treatment, such as injections, surgery, restraint, confinement, etc., the control subjects should be perturbed in the same way as the other subjects, except for the treatment itself, as far as ethical considerations permit. The ``sham operation'', in which surgery is carried out without the experimental treatment itself, is an example.vIn field experiments, applying a treatment of interest may physically disturb the plots receiving it and the surrounding areas, perhaps by trampling the ground by the researchers.Ideally, the same disturbance should be applied to the control plots.

\hypertarget{randomization}{%
\subsection{Randomization}\label{randomization}}

The researcher should randomize assignment of treatments to units or subjects. Chance rather than conscious or unconscious decision determines which units end up receiving the treatment and which the control. A completely randomized design is one in which treatments are assigned to all units by randomization. Randomization breaks the association between possible confounding variables and the explanatory variable. It doesn't eliminate the variation contributed by confounding variables, only their correlation with the treatment variable. Randomization ensures that variation from confounding variables is similar between the different treatment groups.

Randomization should be carried out using a random process such as a random number generator. A general strategy might include the following steps:

\begin{itemize}
\tightlist
\item
  List all n subjects, one per row, in a spreadsheet or computer programming object.
\item
  Use the computer to give each individual a random number.
\item
  Assign treatment A to those subjects receiving the lowest numbers and treatment B to those with the highest numbers.
\end{itemize}

Other procedures for assigning treatments to subjects are almost always inferior because they do not eliminate the effects of confounding variables. ``Haphazard'' assignment, in which the researcher chooses a treatment while trying to make it random, has repeatedly been shown to be non-random and prone to bias.

\hypertarget{blinding}{%
\subsection{Blinding}\label{blinding}}

Blinding is the process of concealing information from participants (sometimes including researchers) about which subjects receive which treatment. Blinding prevents subjects and researchers from changing their behavior, consciously or unconsciously, as a result of knowing which treatment they were receiving or administering. For example, studies showing that acupuncture has a significant effect on back pain are limited to those without blinding (Ernst and White 1998). In a single-blind experiment, the subjects are unaware of the treatment that they have been assigned. Treatments must be indistinguishable to subjects, which prevents them from responding differently according to knowledge of treatment. Blinding can also be a concern in non-human studies where animals respond to stimuli

In a double-blind experiment the researchers administering the treatments and measuring the response are also unaware of which subjects are receiving which treatments. Researchers sometimes have pet hypotheses, and they might treat experimental subjects in different ways depending on their hopes for the outcome. Many response variables are difficult to measure and require some subjective interpretation, which makes the results prone to a bias. Researchers are naturally more interested in the treated subjects than the control subjects, and this increased attention can itself result in improved response. Reviews of medical studies have revealed that studies carried out without double- blinding exaggerated treatment effects by 16\% on average compared with studies carried out with double-blinding (Jni et al.~2001). Experiments on non--human subjects are also prone to bias from lack of blinding. Bebarta et al.(2003) reviewed 290 two-treatment experiments carried out on animals or on cell lines. The odds of detecting a positive effect of treatment were more than threefold higher in studies without blinding than in studies with blinding. Blinding can be incorporated into experiments on nonhuman subjects using coded tags that identify the subject to a ``blind'' observer without revealing the treatment (and who measures units from different treatments in random order).

\hypertarget{replication}{%
\subsection{Replication}\label{replication}}

The goal of experiments is to estimate and test treatment effects against the background of variation between individuals (``noise'') caused by other variables. One way to reduce noise is to make the experimental conditions constant. In field experiments, however, highly constant experimental conditions might not be feasible nor desirable. By limiting the conditions of an experiment, for example, we also limit the generality of the results. There is always a tradeoff between the range of explanatory variables explored and the extent of replication required. For many different treatment types in an experiment, or a broad range of explanatory variable values in an observational study, we need adequate replication across the range of that explanatory variable, so more replication will be required.

Replication in an experiment is the assignment of each treatment to multiple, independent experimental units. Without replication, we would not know whether response differences were due to the treatments or just chance differences between the treatments caused by other factors. As discussed, studies that use more units (i.e.~that have larger sample sizes) will have smaller standard errors and a higher probability of getting the correct answer from a hypothesis test. Larger samples mean more information, and more information means better estimates and more powerful tests.

\hypertarget{a-note-on-pseudoreplication}{%
\subsection{A note on pseudoreplication}\label{a-note-on-pseudoreplication}}

Replication is not about the total number of plants or animals used, but the number of independent units in the experiment. An ``experimental unit'' is the independent unit to which treatments are assigned. The figure below shows three experimental designs used to compare plant growth under two temperature treatments (indicated by the shading of the pots). The first two designs are actually un-replicated with respect to temperature and regarding growth as a random, independent variable. When individual observations in a study are erroneously treated as independent in subsequent statistical analysis, we call it ``psuedoreplication.'' Pseudoreplication results in violation of the assumptions we rely on to conduct statistical hypothesis tests, and it will cause misleading conclusions to be drawn, usually in the form of rejection of the null hypothesis when the null hypothesis is actually true.

\includegraphics[width=13.03in]{images/images_6a.006}

\hypertarget{balance}{%
\subsection{Balance}\label{balance}}

A study design is balanced if all treatments have the same sample size. Conversely, a design is unbalanced if there are unequal sample sizes between treatments. Balance is a second way to reduce the influence of sampling error on estimation and hypothesis testing. To appreciate this, look again at the equation for the standard error of the difference between two treatment means

\includegraphics[width=11.75in]{images/images_6a.007}

For a fixed total number of experimental units, n1 + n2, the standard error is smallest when n1 and n2 are equal. Balance has other benefits as well. For example, Analysis of Variance (discussed later in this book) is more robust to departures from the assumption of equal variances when designs are balanced or nearly so.

\hypertarget{blocking}{%
\subsection{Blocking}\label{blocking}}

Blocking is the grouping of experimental units that have similar properties. Within each block, treatments are randomly assigned to experimental units. Blocking essentially repeats the same, completely randomized experiment multiple times, once for each block. Differences between treatments are only evaluated within blocks, and in this way the component of variation arising from differences between blocks is discarded. The cartoon below depicts two blocks.

\includegraphics[width=12.28in]{images/images_6a.008}

\hypertarget{blocking-paired-designs}{%
\subsubsection{Blocking \textbar{} Paired designs}\label{blocking-paired-designs}}

As an example of blocks with paired subjects, consider the design choices for a two-treatment experiment to investigate the effect of clear cutting on salamander density. In the completely randomized (``two-sample'') design we take a random sample of forest plots from the population and then randomly assign each plot to either the clear-cut treatment or the no clear-cut treatment. In the paired design we take a random sample of forest plots and clear-cut a randomly chosen half of each plot, leaving the other half untouched. In the paired design, measurements on adjacent plot-halves are not independent. This is because they are likely to be similar in soil, water, sunlight, and other conditions that affect the number of salamanders. As a result, we must analyze paired data differently than when every plot is independent of all the others, as in the case of the two-sample design. Paired design is usually more powerful than completely randomized design because it controls for a lot of the extraneous variation between plots or sampling units that sometimes obscures the effects we are looking for.

\begin{center}\includegraphics[width=0.9\linewidth]{images/images_6a.009} \end{center}

\hypertarget{blocking-randomized-complete-block-design}{%
\subsubsection{Blocking \textbar{} Randomized complete block design}\label{blocking-randomized-complete-block-design}}

Randomized complete block (RCB) design is analogous to the paired design, but may have more than two treatments. Each treatment is applied once to every block. As in the paired design, treatment effects in a randomized block design are measured by differences between treatments exclusively within blocks. By accounting for some sources of sampling variation blocking can make differences between treatments stand out. Blocking is worthwhile if units within blocks are relatively homogeneous, apart from treatment effects, and units belonging to different blocks vary because of environmental or other differences.

\hypertarget{what-if-you-cant-do-experiments}{%
\section{What if you can't do experiments?}\label{what-if-you-cant-do-experiments}}

Experimental studies are not always feasible, in which case we must fall back upon observational studies. The best observational studies incorporate as many of the features of good experimental design as possible to minimize bias (e.g., blinding) and the impact of sampling error (e.g., replication, balance, blocking, and even extreme treatments) except for one: randomization. Randomization is out of the question, because in an observational study the researcher does not assign treatments to subjects. Two strategies are used to limit the effects of confounding variables on a difference between treatments in a controlled observational study: matching, and adjusting for known confounding variables (covariates). Matching is similar to the idea of blocks, but there is no random assignment. To give an example of matching, pairs of indivduals with similar explanatory variable values (e.g.~females in a specific age class) might be defined, such that one member of the pair has one level of the focal explanatory variable (e.g. ``control'') and the other member the other level (e.g. ``treatment''). Study adjustment for known confounding covariates, on the other hand, simply restricts the study to subjects in such a way to exclude variation from confounding variable(s). With this strategy, groups of individuals, for example age rages, may be simply excluded from the study alltogether.

\hypertarget{exercises-associated-with-this-chapter-5}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-5}}

\begin{itemize}
\tightlist
\item
  Problem Set 3
\end{itemize}

\hypertarget{additional-learning-resources-5}{%
\section{Additional learning resources:}\label{additional-learning-resources-5}}

\begin{itemize}
\tightlist
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\end{itemize}

\hypertarget{introduction-to-hypothesis-tests}{%
\chapter{Introduction to Hypothesis Tests}\label{introduction-to-hypothesis-tests}}

\hypertarget{background-5}{%
\section{Background}\label{background-5}}

An extension from the concept of estimating a population parameter is the formal comparison of a such a parameter to a single value, or comparison to a value for another group, understood as another population. After all, if we can use probability and sampling theory to estimate a parameter and its uncertainty, we should also be able to compare parameter estimates, with inclusion of uncertainty about their true difference. The ``frequentist'' approach in statistics traditionally taken to perform these types of comparisons requires the definition of precise statements before we do the numerical comparisons. These statements are known as \textbf{\emph{statistical hypotheses}}, and how we frame them is very important because it dictates both how we calculate the test statistics required to compare populations and how we use probability distributions to determine how extreme the statistics are, relative to random chance. This notion of paying attention to an observed test statistic's value relative to ``how frequently'' we would expect to see a value that extreme or more extreme across multiple theoretical samples (under random expectations) is why we refer to this approach as ``frequentist'' statistical inference. We'll get more into what test statistics are and how they are calculated for various tests later in the book. For now, let's focus on how we frame hypotheses to enable the types of comparisons we wish to do as scientists.

\hypertarget{null-and-alternative-hypotheses}{%
\section{Null and alternative hypotheses}\label{null-and-alternative-hypotheses}}

Often as empiricists we want to know whether some parameter differs between groups (populations). Perhaps the populations are different because we have differentiated them in an experiment by applying a treatment to one and no treatment to another. And, if we expect a focal variable we are measuring (i.e.~a response variable) to be fundamentally tied to the treatment based on our knowlede of the system, we can test the validity of that relationship. We might state very simply, for example, ``I hypothesize that on average the variable \emph{X} differs between the treatment and control groups.'' A hypothesis is really just a statement of belief about the world. One issue with hypotheses is that from a logic of deduction standpoint, we can't universally ``prove'' a hypothesis, only reject (i.e.~falsify) it. For this reason we must frame a \textbf{\emph{null hypothesis}} to complement our originally stated \textbf{\emph{alternative hypothesis}}. The null hypothesis represents all possibilities \emph{except} the expected outcome under the alternative. A statistical test is then conducted with respect to the null hypothesis, and if we reject the null hypothesis we typically infer support for the alternative, provided the assumptions of the statistical test about our data were valid.

It's a good idea to practice the formal framing of null and alternative hypotheses, as this will help with the setting up of statistical tests and the reporting of tests in written reports or publications. Here is one example of a null and alternative hypothesis regarding trees of two different species (our populations) being of different heights.

\(H_0\) : \emph{Null hypothesis} : Ponderosa pine trees are the same height on average as Douglas fir trees

\(H_A\) : \emph{Alternative Hypothesis}: Ponderosa pine trees are not the same height on average as Douglas fir trees

You will often see the shorthand notation above for hypotheses (\(H_0\) for null and \(H_A\) for alternative), especially when hypotheses are expressed nonverbally. An nonverbal (quantitative) expression of the above hypotheses, assuming we choose to compare ``averages'' using the mean, would be:

\[H_0 : \mu_1 = \mu_2\]

\[H_A: \mu_1 \neq \mu_2\]

Where \(\mu_1\) and \(\mu_2\) are the population means for ponderosa pine and Douglas fir tree species, respectively.

One important point to make here is that unlike the example above, hypotheses (and the statistical tests used to evaluate them) can be directional (also called ``one-sided'' or ``one-tailed''). If, for instance, we really wanted to test whether ponderosa pines are shorter, on average, than Douglas firs because we suspect this directionality, we could frame the null and alternative hypotheses as follows:

\[H_0 : \mu_1 \geq \mu_2\]

\[H_A: \mu_1 < \mu_2\]

Remember, the null hypothesis encapsulates all outcomes not specified by the alternative. The implications regarding uncertainty (\emph{p}-values) when defining hypothesis tests as either non-directional (two-sided) or directional (one-sided) are important to understand and will be discussed below.

\hypertarget{hypotheses-tests}{%
\section{Hypotheses tests}\label{hypotheses-tests}}

Statistical tests provide a way to perform formal evaluations we call \emph{critical tests} of null hypotheses such as in the examples above. Statistical tests require the definition of \textbf{\emph{test statistics}} that form the basis for comparison among populations. Just like raw data, test statistics are \textbf{\emph{random variables}} and depend on sampling distributions of the underlying data. In the case of parametric statistical tests (those that make use of a particular probability distribution), test statistics are calculated from the data using a specialized formula. For example we may want to test the null hypothesis that two population means are equal. One option is to calculate what is called the \emph{t}-statistic. (We will get into the details of the t-test shortly.) The \emph{t}-statistic is a standardized difference between two sample means, so a value of \emph{t} equal to zero indicates no difference between population means. We can then evaluate where our sample (data)-based value of \emph{t} falls with respect to a known theoretical distribution for \emph{t}, called the ``\emph{t}-distribution,'' for which the center and peak are at the value zero. If our observed value of the test statistic is sufficiently far from zero (i.e.~in the ``tail'' of the \emph{t}-distribution), we will decide to reject the null hypothesis.

The \emph{t}-distribution is just one example of probability distributions used in statistical hypothesis testing. The figure below shows the \emph{t}-distribution and three others commonly used in statistical inference: the \emph{z}, \(\chi^2\), and \emph{F} distributions, some drawn with multiple shape parameters defined.

\begin{center}\includegraphics[width=0.5\linewidth]{images/week_3.003} \end{center}

The particular distribution and shape (such as those above) chosen for a statistical test depends on the whether the appropriate test statistic (such as the \emph{t}-statistic) can be calculated from your data. That determination is ultimately made by the analyst, based on test assumptions about the sample (we will cover those in turn as we discuss different tests). The shape and associated parameters of a distribution used to evaluate a test statistic also depend on sample properties such as sample size. \textbf{\emph{Degrees of freedom}}, for example are an important parameter for critical tests. The degrees of freedom for a particular test convey how many independent observations are used to estimate a population parameter of interest. Because parameter estimates are used in test statistics, we need to account for degrees of freedom. You may ask, ``shouldn't all observations in a sample be independent with respect to estimation of a parameter?'' The answer is actually, ``no'' as it turns out, because estimates (like a sample mean, for example) are calculated from the individual observations in a sample. In a sample of 10, 9 of the observations can theoretically vary freely when calculating a given sample mean, but the final, 10th observation cannot, simply based on the nature of an arithmetic mean. In this case because only 9 observations can vary independently, we have \emph{n} - 1 = 9 degrees of freedom. As mentioned, degrees of freedom determine the shape of the distributions used to evaluate test statistics. In particular, as the degrees of freedom increase (i.e.~the sample size increases), the shape of the probability distribution gets narrower. This means that a test statistic calculated from large sample will be more extreme (``further in the tail''), relative to if that same test statistic value had been calculated from a smaller sample. Recall that as a test statistic is located further into the tail of its distribution, the more extreme it is relative to our null hypothesis expectation, and therefore the smaller the \emph{p}-value is for our statistical test of the null hypothesis. In summary, we are more likely to reject the null hypothesis with a greater sample size. This gets to the concept of ``statistical power'' which we will return to below.

To think about all of this at a high level, consider the plots below of two different population distributions for the same variable. In \emph{a} the two different population distributions are in very different locations. If we took even a moderately sized sample from both populations, the difference in the sample mean between the blue and red populations would be large relative to their respective variances. This means that if we calculated a \emph{t}-statistic from our samples, it would be quite large. In \emph{b} on the other hand, the population distributions are nearly on top of one another. If we calculated a \emph{t}-statistic from samples in that scenario, it would be near zero. Finally, by comparing our calculated \emph{t}-statistic to a \emph{t}-distribution with the appropriate degrees of freedom, we could deterimine (in both scenarios) how likely it is to have observed that particular value for \emph{t} under the null hypothesis of \emph{t} = 0. In \emph{a} we would observe an extreme value for \emph{t} and reject the null hypothesis, but in \emph{b} we would observe a value for \emph{t} close to the center of the distribution (at 0), and fail to reject the null hypothesis of no difference in means.

\begin{center}\includegraphics[width=0.5\linewidth]{images/week_3.001} \end{center}

\hypertarget{p-values-type-i-and-type-ii-error}{%
\subsection{\texorpdfstring{\emph{p}-values, Type I, and Type II error}{p-values, Type I, and Type II error}}\label{p-values-type-i-and-type-ii-error}}

At this point we should consider the possible outcomes of a hypothesis test. These include situations in which we may either falsely reject or falsely fail to reject the null hypothesis. The table below is a useful summary of the four possible outcomes we face when testing a null hypothesis.

\begin{center}\includegraphics[width=0.5\linewidth]{images/week_3.007} \end{center}

As indicated, the columns correspond to our actual evaluation of the null hypothesis (whether we reject or fail to reject it), and the rows correspond to whether the null hypothesis is actually incorrect or correct (which of course we never know unless data are simulated). In the upper left-hand scenario, we reject the null hypothesis and correctly conclude that there is an effect (e.g.~population means differ, etc.). In the upper right-hand scenario, we fail to reject the null hypothesis and conclude there is no effect, but that conclusion is wrong. In this scenario, what we call a \textbf{\emph{Type II error}} (``false negative''), there is a real effect but we ``miss'' it with our test. In the lower left-hand situation we reject the null hypothesis but do so incorrectly, as there is no real effect. This is called \textbf{\emph{Type I error}} (``false positive''), and is the error reflected in a \emph{p}-value from a statistical test. Finally, in the lower right-hand situation we fail to reject the null hypothesis and have done so correctly, as there really is no effect. You will often see the probability of Type II error represented by \(\beta\) (\emph{beta}) and the probability of Type I error represented by \(\alpha\) (\emph{alpha}). As mentioned, we usually decide to reject a null hypothesis if the \emph{p-value} for our statistical test is smaller than a given Type I error rate we are ``willing'' to tolerate. As you are probably well aware, a routinely used threshold for \(\alpha\) is 0.05. The origin of this convention dates back to a paper published by one of the founders of frequentist statistics, R. A. Fisher in 1926. In the paper, titled, ``The Arrangment of Field Experiments,'' Fisher proposed also considering more conservative \(\alpha\) thresholds of 0.02 or 0.01 if desired, but expressed his ``personal preference'' of setting the \(\alpha\) threshold at 0.05. The same passage in the paper does, however, imply that using an \(\alpha\) threshold of 0.05 to assess significance should be done in the context of \emph{multiple, repeated experiments}, in which the experimenter almost always observes \emph{p}-values less than 0.05. The latter point is certainly worth thinking about carefully, as most experimentalists today stick with the ``0.05'' convention but do not commonly repeat experiments many times.

The \emph{p}-value for a statistical test, as we will re-visit below for \emph{t}-tests in more detail, is simply the area under the probability distribution that lies outside (in the tail or tails) of the test statistic value(s), and is calculated using integral calculus. You can think of a \emph{p}-value, then, as the probability of observing a test statistic at least as surprising as the one you observed based on your data, assuming the null hypothesis is correct. So, if your test statistic is far into the tail(s) of its probability distribution, it is a surprising observation under the null hypothesis. You can think of the null hypothesis as being characterized by the test statistic sampling distribution. If you were to take samples over and over again many, many times, and calculate the test statistic each time, it would follow the shape of the distribution. Again taking the \emph{t} distribution as an example of the null expectation of ``no difference between means,'' a value of zero is the most common outcome, with values in the tails much less likely. So the \emph{p}-value reflects the probability that your null hypothesis is true, and very small values suggest that we reject the null hypothesis. Here is a generic schematic that illustrates the concept of \emph{p}-values:

\begin{center}\includegraphics[width=0.5\linewidth]{images/week_3.009} \end{center}

To summarize, if we reject the null hypothesis, we conclude that there is evidence in favor of the alternative hypothesis (again assuming assumptions of the test are met), but we keep in mind that there is a non-zero chance of Type I error, reflected in our \emph{p}-value. If we fail to reject our null hypothesis, the current evidence suggests that we have little reason to believe our alternative is true, but again there is risk of committing Type II error. How we interpret whether we actually had enough data to confidently rule out our null hypothesis requires an estimate of \textbf{\emph{statistical power}}.

\hypertarget{statistical-power}{%
\subsection{Statistical power}\label{statistical-power}}

Power is the probability of rejecting a false null hypothesis, which is equivalent to 1 - \(\beta\), where \(\beta\) is the Type II error rate. So, the higher the power, the more confident we can be in detecting ``an effect'' with our hypothesis test when that effect truly exists. Power is commonly calculated before an experiment (\emph{a priori}), using either simulated data, a ``pilot'' data set, or data from similar experiments. As you will see from the relationships below, pre-study power analyses can be extremely useful in determining the sample sizes required to detect an effect of a particular size. This is especially important if the resources to conduct a study are limited, and indeed, pre-study power analyses are often required for grant proposals, especially those that involve experiments with animals. As a benchmark, statisticians conventionally ``aim'' for a power of 0.8 or higher, but this is of course subject to the nature of the experiment at hand and how critical detecting true effects is. For example, certain clinical studies may need to be especially high-powered for ethical reasons. It all depends on the ``cost'' of committing Type II error. Power analyses can also be conducted after a study (\emph{post hoc}), especially if experimenters don't want to be left wondering whether they may have detected the effect associated with their alternative hypothesis, had they only a larger sample size.

Just below is a generic expression for power, and how it varies given other variables associated with a particular hypothesis test. Adjustments to this expression may be required, depending on the particular statistical framework used, but it works as a good guide. Note that in this expression power is ``proportional to'' these variables as indicated, and not ``equal to.'' In many relatively simple experimental design scenarios this expression will provide practical estimates. If not, there are more complicated formulae depending on the design of your study, and there is also frequently the prospect of doing simulations to understand the power relationships inherent in your study system.

\[ Power \propto \frac{(ES)(\alpha)(\sqrt n)}{\sigma}\]

In the above expression power is proportional to the combination of these parameters:

\begin{itemize}
\item
  \emph{ES} = Effect size. This is the magnitude of the difference between populations you hope to detect with your test. For example, the difference in population means, etc. It can be expressed in different ways depending on the calculation, so pay attention to the input requirements of functions you are using for power analysis.
\item
  \(\alpha\) = Type I error rate tolerated (usually 0.05).
\item
  \emph{n} = Sample size. The number of observations per sample group.
\item
  \(\sigma\) = Standard deviation among the experimental units within the same group.
\end{itemize}

We often care about the relationships depicted in the example power relationships below. For instance, we may want to know what effect size we can detect at various power levels, assuming sample size and standard deviation are fixed. Likewise, we may want to identify the smallest sample size required to detect a particular effect size, assuming a given power (e.g.~0.8) and standard deviation.

\begin{center}\includegraphics[width=0.9\linewidth]{images/images_6b.002} \end{center}

For a rough calculation under certain experimental design constraints, the following can be used as a ``quick'' sample size estimator when desired power and \(\alpha\) are the conventional 0.8 and 0.05, respectively.

\begin{center}\includegraphics[width=13.11in]{images/images_6b.003} \end{center}

\hypertarget{a-quick-note-on-practical-vs.statistical-significance}{%
\subsection{A quick note on practical vs.~statistical significance}\label{a-quick-note-on-practical-vs.statistical-significance}}

By studying the above power relationships you may have noticed that you might correctly reject your null hypothesis (and with a very low \emph{p}-value at that) for an extremely small effect size if your sample size is very large. This is an important point, indeed. If your study is incredibly ``well powered'' (e.g.~huge sample sizes and small standard deviations), you can detect very small effect sizes. In some cases, though, such small effect sizes may not be relevant to your larger questions at hand. In a clinical study, for instance, a tiny effect size may not have any practical bearing on future patients' outcomes, so this should be understood. Just because you reject your null hypothesis and conclude ``a significant test,'' it does not necessarily mean the effect you've detected is significant in practical sense. Therefore, the results of any hypothesis test need to be interpreted in combination with the observed effect size.

\hypertarget{the-t-test-and-t-sampling-distribution}{%
\section{\texorpdfstring{The \emph{t}-test and \emph{t} sampling distribution}{The t-test and t sampling distribution}}\label{the-t-test-and-t-sampling-distribution}}

Above we discussed that a difference between two population means can be compared using a test based on the \emph{t} distribution. The \emph{t}-test is also often referred to as ``Student's \emph{t}-test,'' because ``Student'' was the pseudonym used by a person who wrote a widely read paper (in 1908) in which the test's practical application was published for one of the first times. That person, whose real name was William Sealy Gosset, was a student of statistician Karl Pearson, but because Gosset's employer (Guinness Brewery) didn't allow their employees to publish work-related material, the pseudonym was used.

As previously mentioned, the \emph{t}-test is based on a test statistic (the \emph{t}-statistic) that usually considers the difference between two sample means, to test the null hypothesis of no population difference in means. This is the so-called ``two-sample \emph{t}-test,'' and the one we will consider in this course. It is also possible to perform a ``one-sample \emph{t}-test,'' in which the sample mean from a single population is tested to differ from a fixed value (such as zero). Below we consider the calculation of the \emph{t}-statistic and two forms of the hypothesis test (one- and two-tailed) for the two-sample comparison case. Note first that the \emph{t}-statistic is simply the difference in sample means divided by the standard error for that difference, to account for variation within the two populations:

\[\large t = \frac{(\bar{y}_1-\bar{y}_2)}{s_{\bar{y}_1-\bar{y}_2}} \]

where

\begin{center}\includegraphics[width=0.6\linewidth]{images/week_3.016} \end{center}

As mentioned, the denominator is the calculation for the standard error of the mean difference, in which \emph{s} denotes the sample standard deviations for populations 1 and 2, and \emph{n} denotes the sample sizes for populations 1 and 2. The degrees of freedom (\emph{df}) for this test are equal to \(n_1+n_2-2\).

For a ``one-tailed'' test, recall that our hypothesis assumes directionality in the difference in means. So, if our alternative hypothesis is that \(\mu_1>\mu_2\), a large value of \emph{t} in the right tail of the distribution - one that is \textbf{greater} than the ``critical value'' - will result in a \emph{p-value} of less than 0.05. The critical value simply marks the point beyond which the area under the probability density sums to 0.05. The generalized figure below illustrates where the critcal value \(t_c\) falls. For a one-tailed test in this case, we would reject the null hypothesis if the observed \emph{t} was greater than \(t_c\).

\begin{center}\includegraphics[width=0.9\linewidth]{images/week_3.005_1_tailed} \end{center}

For a ``two-tailed'' test, our hypothesis allows for the possibility that the difference in population means might be greater or less than zero (i.e.~we don't assume a directionality in the difference \emph{a priori}). In this case, we simply have to consider our critical value at both tails of the \emph{t} distribution, such that the areas under the probability density beyond the location in both tails \textbf{sum} to 0.05. And, if our observed \emph{t} is either less than \(-t_c\) or greater than \(t_c\), we would reject the null hypothesis.

\begin{center}\includegraphics[width=0.6\linewidth]{images/week_3.005_2_tailed} \end{center}

In \texttt{R} we can easily perform a \emph{t}-test using the \texttt{t.test()} function. For two samples we simply supply the function two vectors of values, one from sample 1 (argument \texttt{x}) and one from sample 2 (argument \texttt{y}). The default test is two-tailed, but if we want to run a one-tailed test we supply ``less'' to the \texttt{alternative} argument if we are testing whether \(\mu_1<\mu_2\), or ``greater'' to the \texttt{alternative} argument if our alternative hypothesis is that \(\mu_1>\mu_2\). Note that the \texttt{t.test()} function actually performs a ``Welch's \emph{t}-test,'' which is an adpatation of the Student's \emph{t}-test. It is very similar, with only minor calculation differences, but more reliable for unequal sample sizes and/or slightly different variances.

\hypertarget{assumptions-of-parameteric-t-tests}{%
\subsection{Assumptions of parameteric t-tests}\label{assumptions-of-parameteric-t-tests}}

As with any parametric statistical test, we should only use a \emph{t}-test if our samples adhere to certain assumptions. Otherwise, our actual Type I and/or Type II error will not be accurately reflected by the test, and we will be more likely to draw the wrong conclusions than intended. The theoretical t-distributions for each degree of freedom were calculated based on the following assumptions:

\begin{itemize}
\item
  The response variable in the populations is normally distributed. This assumption is most easily assessed by looking at histograms for your data (samples). Confirm that your variable appears to be approximately normally distributed in your samples.
\item
  The response variable in the populations has equal variances (if comparing two means). Informally this can be evaluated by looking at histograms or boxplots to see if the spread of distributions for both of your samples looks similar. Formally, you can perform something called an F Test for equal variances, using the \texttt{var.test()} function.
\item
  The observations within each sample are independent. This assumption stipulates that you randomly sampled individuals from each of your populations. For example, if your populations represented different species in a specific location, you need to randomly select individuals of each species, as opposed to selecting individuals from one particular family, sub-location, shape, etc.
\end{itemize}

What should you do if the assumption of normality and/or equal variances is not met? There are a few alternatives. As mentioned, we call these alternatives ``non-parametric'' approaches because they do not rely on specific probability distributions, and consequently their assumptions. Nonparametric tests based on the ``rank'' of the values instead of the orignal values themselves are often an option. The Mann-Whitney \emph{U} (also called "Mann-Whitney-Wilcoxon) Test tests for distributional differences bewtween the ranks of two samples. In \texttt{R} the function \texttt{wilcox.test()} can be used to perform it, in much the same way the \texttt{t.test()} function is used.

Another nonparametric option is to generate a null distribution of the appropriate test statistic from your samples, using either randomization, or resampling with replacement (i.e.~a ``bootstrap test''). These are briefly discussed below, with a simple coded example.

\hypertarget{comparing-means-using-resampling-and-randomization-tests}{%
\section{Comparing means using resampling and randomization tests}\label{comparing-means-using-resampling-and-randomization-tests}}

In many cases when our sample data don't meet assumptions of parametric tests we can create a \textbf{\emph{null statistical distribution}} that models the distribution of a test statistic under the null hypothesis. As in the parametric approaches described above, we first calculate an \textbf{observed test statistic value} for our data. In the situation of comparing two population means, for example, we can calculate the \emph{t} statistic from our data, as above. To create the null distribution we can use either randomization or resampling. For randomization, and assuming a one-tailed test of a larger mean for population 1, we could:
1. Combine values from both populations into a single vector,
2. Randomly shuffle the vector using the \texttt{sample()} function,
3. Calculate a \emph{t} statistic based on the first n1 and n2 observations as our ``pseudo samples'' from ``populations'' 1 and 2, respectively, and save the value,
4. Repeat steps 2 and 3 many times (e.g. \$\geq\$1000),
5. Calculate the proportion of pseudo replicates in which \emph{t} is \(\geq\) to our original, observed value of \emph{t}. This proportion is our estimated \emph{p}-value for the test.
An example using simulated data in \texttt{R} is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{56}\NormalTok{)}
\NormalTok{pop_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n=}\DecValTok{50}\NormalTok{, }\DataTypeTok{mean=}\FloatTok{20.1}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}\CommentTok{#simulate population 1 for this example}
\NormalTok{pop_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n=}\DecValTok{50}\NormalTok{, }\DataTypeTok{mean=}\FloatTok{19.3}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}\CommentTok{#simulate population 2 for this example}

\CommentTok{# Store the t statistic calculated from our samples, using t.test()}
\NormalTok{t_obs <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(}\DataTypeTok{x=}\NormalTok{pop_}\DecValTok{1}\NormalTok{, }\DataTypeTok{y=}\NormalTok{pop_}\DecValTok{2}\NormalTok{, }\DataTypeTok{alternative=}\StringTok{"greater"}\NormalTok{)}\OperatorTok{$}\NormalTok{statistic}

\CommentTok{# Combine both population vectors into one}
\NormalTok{pops_comb <-}\StringTok{ }\KeywordTok{c}\NormalTok{(pop_}\DecValTok{1}\NormalTok{, pop_}\DecValTok{2}\NormalTok{)}

\CommentTok{# Randomly shuffle and calculate t statistic 1000 times}
\NormalTok{t_rand <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  pops_shuf <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(pops_comb)}
  \KeywordTok{t.test}\NormalTok{(}\DataTypeTok{x=}\NormalTok{pops_shuf[}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{], }\DataTypeTok{y=}\NormalTok{pops_shuf[}\DecValTok{51}\OperatorTok{:}\DecValTok{100}\NormalTok{], }\DataTypeTok{alternative=}\StringTok{"greater"}\NormalTok{)}\OperatorTok{$}\NormalTok{statistic}
\NormalTok{  \})}

\CommentTok{# Plot the "null distribution" from the randomization-based t-values}
\KeywordTok{hist}\NormalTok{(t_rand)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-78-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate the p-value for the test as the number of randomization t-values greater}
\CommentTok{# than or equal to our actual t-value observed from the data}
\NormalTok{p <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(t_rand}\OperatorTok{>=}\NormalTok{t_obs)}\OperatorTok{/}\DecValTok{1000}

\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.016
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# p = 0.016, so we reject the null hypothesis of a population 1 mean less than or equal \textbackslash{}}
\CommentTok{# to the population 2 mean. The population 1 mean is likely larger than the population 2 mean.}
\end{Highlighting}
\end{Shaded}

A similar approach may be taken by randomly resampling (with replacement) from the combined vector of values for both populations, provided that the sample sizes are equal, in order to generate a null distribution against which the observed \emph{t} statistic may be compared. This approach would technically be considered a ``bootstrap'' \emph{t}-test. Both randomization and resampling approaches should yield similar results for moderate to large sample sizes. For small sample sizes the randomization approach is preferable, as all values from both populations will be included in each pseudo-replicate.

\hypertarget{a-summary-of-key-components-of-hypothesis-testing}{%
\section{A summary of key components of hypothesis testing}\label{a-summary-of-key-components-of-hypothesis-testing}}

\begin{itemize}
\item
  \emph{p}-value = The long run probability of rejecting a true null hypothesis. If our observed test statistic is very extreme in relation to the distribution under the null hypothesis, the \emph{p}-value will be very small.
\item
  \(\alpha\) = The Type I error rate for a hypothesis test. Often stated as a ``critical p-value cutoff'' for experiments, as in the Type I error we are willing to tolerate.
\item
  \(\beta\) = The Type II error rate for a hypothesis test. Often stated as a cutoff for probability of accepting a false null hypothesis.
\item
  Power = The probability that a test will correctly reject the null hypothesis (1 - \(\beta\)). It depends on effect size, sample size, chosen \(\alpha\), and population standard deviation.
\item
  Multiple testing = Performing the same or similar tests multiple times. When we perform multiple hypothesis tests to answer a general study question (like in the case of analyzing many genes in an RNA-seq experiment), we need to adjust the \(\alpha\) threshold to be lower than it would for a single test. There are multiple ways to correct \emph{p}-values if multiple testing is used. One correction uses a ``tax'' (e.g. \textbf{Bonferonni} adjustment) based simply on the number of tests, while another is the direct estimation of a \textbf{False Discovery Rate (FDR)}. We will return to the multiple testing problem when we consider ANOVA.
\end{itemize}

\hypertarget{exercises-associated-with-this-chapter-6}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-6}}

\begin{itemize}
\tightlist
\item
  Problem Set 3
\end{itemize}

\hypertarget{additional-learning-resources-6}{%
\section{Additional learning resources:}\label{additional-learning-resources-6}}

\begin{itemize}
\item
  Irizarry, R. A. Introduction to Data Science. \url{https://rafalab.github.io/dsbook/} - A gitbook written by a statistician, with great introductions to key topics in statistical inference.
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\end{itemize}

\hypertarget{correlation-and-simple-linear-regression}{%
\chapter{Correlation and Simple Linear Regression}\label{correlation-and-simple-linear-regression}}

\hypertarget{background-6}{%
\section{Background}\label{background-6}}

Quite frequently we want to know whether two continuous variables are \emph{related} based on measuring them in the same set of observations, and if so, how and how strongly they are related. When two random variables (say \emph{X} and \emph{Y}) deviate from their respective means in a systematic, predictable way, we say that they \textbf{\emph{covary}}, or that they are \textbf{\emph{correlated}} variables. Levels of expression for pairs of genes, for example, are often correlated, especially if the genes are members of the same regulatory network. Two genes may share the same transcription factor, for instance, and when the abundance of that transcription factor increases in cells, so do transcript levels for the two genes. In this case if you measure abundance of both transcripts in a sample of cells, tissues, individuals, or whatever, you may well find many observations with low expression values for both genes, many with moderate expression values for both, and many with high values for both genes. Cleary in this situation there appears to be a ``positive'' relationship bewteen the two gene expression variables, but as statisticians how do we formally describe the relationship better, and how might we make inferences about the system from a sample? This chapter focuses on the estimation of parameters, and the testing of hypotheses, relevant to relationships between quantitative variables.

\hypertarget{covariance-and-correlation}{%
\section{Covariance and correlation}\label{covariance-and-correlation}}

Before we get into the parameters of interest and how we estimate them from samples, we should first make some practical considerations. Two variables may covary for a number of reasons, which may or may not involve one variable systematically influencing the other. We would call that a ``causal'' relationship, but covariance can arise for non-causal reasons too, such as in the example above. In that example the expression level of ``gene A'' was not influenced by the expression of ``gene B,'' but the two covaried simply because they were affected in similar ways by a third force (the transcription factor). This can be an important distinction (between causal and non-causal relationships) when thinking about how to proceed with analysis because for some statistics (like covariance and correlation) causality is not assumed or interpreted, but for other approaches (like regression) it might be. In the case of regression, which we will return to later in this chapter, there is a clear dependent (response) and independent (explanatory) variable. Regression models, especially in the case of controlled experiments in which the values of the explanatory variable are set and assigned by the experimentors, the goal is often to understand whether, and if so by what magnitude, that variable directly influences the response variable in order to test hypothesis and/or make predictions about the system.

\hypertarget{covariance}{%
\subsection{covariance}\label{covariance}}

We stated above that ``systematic deviation from respective means'' defines a situation in which two variables covary, but how do we actually convey this numerically? One statistic, known as the \textbf{\emph{covariance}}, multiplies each \emph{y} and \emph{x} deviation (for a given observation) from its respective mean, sums that product across all observations, and divides by the total number of observations to yield an average. If individual values of one variable deviate from their mean in one direction, and \emph{corresponding} values of the other variable consistently deviate from their mean in the same (or the opposite) direction, the products in the sum will be either consistently positive or consistently negative, resulting in a substantial positive covariance, or a substantial negative covariance, respectively. If there is no consistent, directional deviation for the two variables, on the other hand, the products will sum to a covariance of zero (no relationship between variables).\\

The \emph{population} covariance can be expressed as:
\[cov(X,Y)=\sigma_{XY}=\frac{\sum_{i=1}^N (x_i-\mu_x)(y_i-\mu_y)}{N}\]

Where \(x_i\) and \(y_i\) correspond to the values of random variables \emph{X} and \emph{Y} for the \emph{i}th observation in a populaiton of size \emph{N}, and \(\mu_x\) and \(\mu_y\) are the respective population means. Again, the important takeaway is that when the product of \((x_i-\mu_x)\) and \((y_i-\mu_y)\) is \emph{consistently} positive or negative across observations, the \emph{x} and \emph{y} variables are consistently deviating from their means in a similar or opposite manner, resulting in a positive or negative covariance.\\

To estimate covariance from a \emph{sample}, we divide by the degrees of freedom (\emph{n} - 1) instead of dividing by \emph{n}:
\[cov(x,y)=s_{xy}=\frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{n-1}\]

\hypertarget{correlation}{%
\subsection{correlation}\label{correlation}}

Remember when we noted (in Chapter 9) that variables with larger values on average tend to have larger variances as well (the ``positive mean-variance relationship'')? This dependence of variance magnitude on variable ``scale'' similarly applies to covariance. That is, if one or more variables that covary have relatively large values, it will be reflected in the magnitude of the covariance. For this reason, and much in the same way we use the coefficient of variation (CV) to adjust for scale when comparing standard deviations, we often use a standardized covariance called the \textbf{\emph{correlation coefficient}} that is obtained by dividing the covariance by the standard deviations of \emph{x} and \emph{y}. The correlation coefficient, therefore, ranges from -1 to 1. Values of -1 and 1 indicate perfect linear relationships, and a value of 0 indicates uncorrelated variables. The correlation coefficient (sometimes called the Pearson correlation coefficient) for a \emph{population} is:
\[\rho_{XY}=\frac{cov(X,Y)}{\sigma_X\sigma_Y} \]

Where \(cov(X,Y)\) is the population covariance between variables \emph{X} and \emph{Y}, and \(\sigma_X\) and \(\sigma_Y\) are the population standard deviations for \emph{X} and \emph{Y}.

For a \emph{sample}, the Pearson correlation coefficient can be calculated as:
\[r_{xy}=\frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}}\]

Where \(\bar{x}\) and \(\bar{y}\) are the sample means for variables \emph{x} and \emph{y}, and \emph{n} is the sample size.

The following scatter plots show a range of scenarios for two variables \emph{x} and \emph{y}, depicting various relationship types and corresponding covariance and correlation values.

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-79-1.pdf}

In plots A and B we see a positive covariance and correlation. In B the covariance is large because the scale of \emph{x} is 10 times larger than in A, but the correlation coefficient is the same. In C we see a negative relationship between \emph{x} and \emph{y} (\emph{y} decreases as \emph{x} increases), and with covariance and correlation of greater magnitude than in A, owing to a ``tighter'' relationship. In plot D values for the variables \emph{x} and \emph{y} were both drawn randomly and independently, so there is no significant correlation or covariance.

\hypertarget{hyptohesis-tests-for-correlation}{%
\subsection{hyptohesis tests for correlation}\label{hyptohesis-tests-for-correlation}}

Formal hypothesis tests about correlation concern whether or not the population correlation coefficient (\(\rho\)) differs from zero. The null and alternative hypothesis statements are as follows

\[H_0 : \rho_1 = 0\]

\[H_A: \rho_1 \neq 0\]

The null hypothesis can be tested by calculating a \emph{t} statistic, which is the sample correlation coefficient (\emph{r}) standardized by its standard error. Below is one way to calculate \emph{t}:
\[t=r\sqrt{\frac{n-2}{1-r^2}}\]

Where \emph{n} is the sample size and \emph{r} is the sample correlation coefficient. This \emph{t} statistic can then be compared to a \emph{t} distribution with \emph{n}-2 degrees of freedom. In \texttt{R} the function \texttt{cor.test()} can be used for this parametric test, but keep in mind that the following assumptions apply:

\begin{itemize}
\item
  The relationship being tested under the alternative hypothesis is assumed to be linear (as opposed to strongly curvilinear), as the Pearson correlation coefficient won't characterize non-linear relationships adequately.
\item
  The ``joint probability distribution'' of the two variables in the population (and therefore the sample) is assumed to be bivariate normal. For this to be true, both \emph{x} and \emph{y} variables should be approximately normally distributed in the sample.
\end{itemize}

There are non-parametric alternatives to test the above null hypothesis that \(\rho\) = 0 when either of these assumptions is not met. Rank-based approaches calculate a test statistc based on the ranks of \emph{x} and \emph{y} values, so they are appropriate as long as the association between the variables is monotonic (consistently increasing or decreasing) in nature. The Spearman's rank correlation test is best suited for small sample sizes (e.g. \emph{n} \textless{} 30), and the Kendall's tau (\(\tau\)) test is more appropriate for larger sample sizes. These tests can also be performed in \texttt{R} using the \texttt{cor.test()} function, by supplying either ``pearson'' or ``kendall'' to the \texttt{method} argument. Yeta another nonparametric option would be to peform a randomization or bootstrap test for \(\rho\) = 0, by either shuffling or resampling \emph{x} and \emph{y} values independently to generate a null distribution for the sample correlation coefficient \emph{r}.

You may have noticed that correlation analysis can tell us whether, in what direction, and how ``tightly'' two variables are correlated, but it is agnostic with respect to other properties of the relationship, namely the \emph{steepness} of the relationship (i.e.~the rate at which \emph{y} decreases (or increases) with an increase in \emph{x}). This parameter, which is extremely important to understand in a variety of practical contexts, is inferred using linear regression.

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

We can also model linear relationships between variables using linear equations, with which you are probably quite familiar. Linear regression, as we refer to this approach in statistics, has been around since the 19th Century, when the biometrician Francis Galton developed it to understand phenotypic similarity between human parents and their offspring. One of the traits Galton studied extensively, for example, was adult height. Linear regression models describe how the magnitude of a response variable \emph{y} changes as a function of a predictor variable \emph{x}, based on the generic equation \(y=bx+a\). In this equation \emph{b} (the slope) gives the amount of change that occurs in \emph{y} per unit of \emph{x}, and \emph{a} is the ``y-intercept'' (the value of \emph{y} when \emph{x} = 0). Not surprisingly, \emph{b} \textgreater{} 0 indicates a positive relationship between \emph{x} and \emph{y}, \emph{b} \textless{} 0 indicates a negative relationship, and when \emph{b} = 0 there is no linear relationship between \emph{x} and \emph{y}

If we consider \emph{X} and \emph{Y} as random variables in a population, from an estimation standpoint we may naturally be interested in estimating the population slope \(\beta_1\). The population y-intercept \(\beta_0\) is also a parameter in the linear regression model, but it is usually of little interest inference-wise. Under our usual sampling-based inference framework we can represent a simple linear regression model as:
\[y_i=\beta_0+\beta_1x_i+\varepsilon_i\]
Where our sample includes \emph{y} and \emph{x} values across \emph{i} observations, and with the aforementioned designations for population slope and intercept. Importantly, because we rarely expect a perfect, straight-line relationship between \emph{X} and \emph{Y}, we include an ``error'' (or ``residual'') term \(\varepsilon_i\) in the model. This term absorbs any ``noise'' (i.e.~random error unexplained by the effect of \emph{X}), and can be quantified by departures of \emph{y} values from the straight line dictated by the model. We will return to these departures, also called ``residuals'' repeatedly in this chapter and the next. You may be asking how we estimate \(\beta_1\) and \(\beta_0\) from a sample. Similar to using a formula to calculate a sample mean, a sample correlation coefficient, etc., we can calculate a sample slope (\(b_1\)) and intercept (\(b_0\)) using one of several ``best fit'' equations. One of these, known as \textbf{\emph{ordinary least squares (OLS)}}, or ``model I regression,'' derives a linear equation for a straight line such that the vertical distances between \emph{y}-values in the sample and points on the line (the ``predicted'' \emph{y}-values) are minimized. Effectively, the goal with this approach is to minimize the variation in \emph{y} unexplained by \emph{x}. The slope and intercept of this ``best fit line'' are (\(b_1\)) and (\(b_0\)), our estimates for \(\beta_1\) and \(\beta_0\).

\hypertarget{hypothesis-tests-in-linear-regression}{%
\subsection{Hypothesis tests in linear regression}\label{hypothesis-tests-in-linear-regression}}

The first hypothesis testing approach for linear regression involves the individual parameters (\(\beta_1\) and \(\beta_0\)) themselves. We can state a null hypothesis for the slope and intercept:
\[H_0:  \beta_1=0\]

\[H_0:  \beta_0=0\]

Alternative hypotheses are, of course, that these parameters are not equal to zero. As discussed, a nonzero population slope indicates a relationship between \emph{X} and \emph{Y}, and the slope's magnitude indicates the rate at which \emph{Y} changes with \emph{X}. A nonzero \emph{y}-intercept indicates the ``background'' level for \emph{Y} in the absence of \emph{X} but, as stated, is usually not of too much interest. Both the sample slope and sample intercept can be used to calculate respective \emph{t} statistics (\(t=\frac{b}{s_b}\)), where the denominator is the usual standard error of the point estimate, and \emph{t} can be compared to a \emph{t} distribution with \emph{n} - 2 degrees of freedom.

A more generalized framework for testing linear regression hypotheses involves considering the amount of variation explained by the \textbf{\emph{full}} linear model (\(y_i=\beta_0+\beta_1x_i+\varepsilon_i\)) relative to the amount of variation it does not explain. If the amount of variation in \emph{y} explained by the full model is significantly greater than the amount unexplained, we should reject our null hypothesis of a zero slope. In practice, though, we can only directly measure \emph{unexplained variation}, so we calculate the difference between the unexplained variation in the \textbf{\emph{reduced model}}, in which the slope is set to zero (\(y_i=\beta_0+\varepsilon_i\)), and the unexplained variation in the full model above. If this difference is large, it means that the full model explains a lot of variation in \emph{y}, and, as said, we should reject our null hypothesis.

You may be wondering how we quantify the variation unexplained in full and reduced linear models. This brings us back to the concept of \textbf{\emph{residuals}}. By calculating the sum of squared deviations of observed \emph{y} values from \emph{y} values predicted under a given model (which we call \(\hat{y}\)s), we can measure unexplained variation. These measures are referred to ``sums of squares'' (SS). Based on what was stated above, the expression \(\frac{SS_{reduced}-SS_{full}}{SS_{full}}\) gives us the ratio we need to test the null hypothesis. The following example illustrates how SSs work for reduced and full models.

\begin{center}\includegraphics[width=14.22in]{images/images_4b.011} \end{center}

In this example, there is appears to be a negative relationship between body mass and captures. The reduced model, with a slope of zero, is not as good a fit, so the SS (reflected in the vertical lines characterizing residuals) is greater than in the full model.

Under the assumptions below, the ratio of explained to unexplained variation (called an \emph{F}-ratio) can be compared to the \emph{F} distribution for the null hypothesis test. Extremely large values of \emph{F} are unlikely to be observed due to random chance under the null hypothesis, so if the \emph{F}-ratio is large enough, we reject the null hypothesis.

\begin{itemize}
\item
  A linear relationship between the variables under the alternative hypothesis is assumed. Non-linear relationships (such as curvilinear ones) are not modeled adequately by this framework and need to be analyzed differently. This assumption can be checked with a scatter plot.
\item
  Both variables are assumed to be normally distributed, so samples should also reflect normality, and can be checked in the usual ways (boxplots, histograms, etc.).
\item
  Variance of the response variable (i.e. \emph{y}) is assumed to be homogeneous across all values of the explantatory variable (i.e. \emph{x}). In regression, this assumption is evaluted in the context of the fitted line. The residuals should form a uniform ``band'' of points when plotted against predicted values of \emph{y}. A ``residual plot'' will address this assumption. Below is an example of what to look for in that plot type.
\end{itemize}

\begin{center}\includegraphics[width=14.22in]{images/images_4b.018} \end{center}

In the plots above, (a) shows the expected residual pattern under our assumptions, while (b), (c), and (d) show patterns of unequal or systematically changing variance, all violations of linear regression assumptions. The section below describes how to view a residual plot in \texttt{R}.

\hypertarget{linear-regression-in-r}{%
\subsection{\texorpdfstring{linear regression in \texttt{R}}{linear regression in R}}\label{linear-regression-in-r}}

Fitting a regression model in \texttt{R} is very simple. We use the function \texttt{lm()} to specify the structure of the model. The \texttt{lm()} function can actually be used to fit an entire class of models we call ``general linear models.'' We will return to this idea in the next chapter, when we discuss categorical predictors and ANOVA. For now, know that you can fit a simple regression model with \texttt{lm()} using the simple \texttt{\textasciitilde{}} syntax. The response (\emph{y}) variable goes to the left of the \texttt{\textasciitilde{}}, and the predictor variable to the right. Below is an example of how to fit a regression model for the toy data set (panel A) used to demonstrate covariance and correlation above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## First, plot the relationship with a scatter plot}
\KeywordTok{plot}\NormalTok{(x_}\DecValTok{1}\NormalTok{, y_}\DecValTok{1}\NormalTok{, }\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"y"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-82-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Define the model using the lm() function and assign it to the object "reg_mod_1"}
\NormalTok{reg_mod_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y_}\DecValTok{1} \OperatorTok{~}\StringTok{ }\NormalTok{x_}\DecValTok{1}\NormalTok{)}

\CommentTok{## We can make a residual plot to help evaluate assumptions}
\KeywordTok{plot}\NormalTok{(reg_mod_}\DecValTok{1}\OperatorTok{$}\NormalTok{fitted.values, reg_mod_}\DecValTok{1}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{xlab=}\StringTok{"predicted y"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-82-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## We can use the summary() function to look at parameter estimates and hypothesis tests}
\KeywordTok{summary}\NormalTok{(reg_mod_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y_1 ~ x_1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0042 -0.8783 -0.2644  1.1652  2.2413 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  1.02754    0.52547   1.955   0.0577 .  
## x_1          0.79920    0.07803  10.243 1.29e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.437 on 39 degrees of freedom
## Multiple R-squared:  0.729,  Adjusted R-squared:  0.7221 
## F-statistic: 104.9 on 1 and 39 DF,  p-value: 1.292e-12
\end{verbatim}

As you can see in the example, our assumptions look to be met, and our hypothesis tests (both for the individual slope via \emph{t}-test and equivalently for the full model via \emph{F}-test), suggest that we should reject the null hypothesis of no linear relationship with high confidence. Also note that the parameter estimates are reported in the output. In this case we care the most about the population slope, which is estimated to be 0.7992.

\hypertarget{a-note-on-the-coefficient-of-determination}{%
\subsection{a note on the coefficient of determination}\label{a-note-on-the-coefficient-of-determination}}

There is a clear connection between regression and correlation if we consider the sources of unexplained variation in a regression model. As it turns out \(1-\frac{SS_{full}}{SS_{reduced}}\) quantifies the proportion of variance in \emph{y} that is explained by \emph{x}. This quantity is also called \(r^2\), the ``coefficient of determination,'' and, for simple linear regression, is the square of the correlation coefficient \(r\). \(r^2\)s (sometimes called ``R-squred values'') are commonly reported in regression analysis results.

\hypertarget{a-note-on-model-ii-regression}{%
\subsection{a note on model II regression}\label{a-note-on-model-ii-regression}}

As stated, OLS regression assumes that we don't have any error associated with our explanatory variable (\emph{x}) values. While this certainly the case for experiments in which we set those values or can establish them with great precision, at least, in many cases (especially in observational or descriptive studies) we have as much measurement error for the \emph{x} variable as we do for the \emph{y} variable. In these cases, that uncertainty of measurement for \emph{x} needs to be accounted for when fitting regression models. We use models classified as ``model II'' for these cases. Going into details about them is beyond the scope of this course, but you should at least know that they exist. The figure below depicts how residuals are calculated for three different versions of model II regression.

\begin{center}\includegraphics[width=13.89in]{images/images_5a.002} \end{center}

\begin{itemize}
\item
  Major Axis (MA) regression should be used when \emph{x} and \emph{y} have the same error, and they have the same units or are dimensionless.
\item
  Ranged Major Axis (ranged MA) regresion should be used when there is error in both \emph{x} and \emph{y}, but if they are on different scales or have different units. This approach should not be used when there are outliers (observations with large residuals).
\item
  Reduced Major Axis (RMA or SMA) regression should be used when there is error in both \emph{x} and \emph{y}, but if they are on different scales or have different units. This method is robust to outliers and used when the two variables are strongly correlated.
\end{itemize}

\hypertarget{exercises-associated-with-this-chapter-7}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-7}}

\begin{itemize}
\tightlist
\item
  Problem Set 3
\end{itemize}

\hypertarget{additional-learning-resources-7}{%
\section{Additional learning resources:}\label{additional-learning-resources-7}}

\begin{itemize}
\item
  Irizarry, R. A. Introduction to Data Science. \url{https://rafalab.github.io/dsbook/} - A gitbook written by a statistician, with great introductions to key topics in statistical inference.
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\end{itemize}

\hypertarget{introduction-to-analysis-of-variance}{%
\chapter{Introduction to Analysis of Variance}\label{introduction-to-analysis-of-variance}}

\hypertarget{background-7}{%
\section{Background}\label{background-7}}

In the last chapter we covered analysis situations in which we want to understand the relationship between two continuous variables. We also learned that in some of those situations there is a clear response (\emph{y}) and a clear predictor (\emph{x}) variable, making possible the application of linear regression analysis. In this chapter we will continue to work within the space of response and predictor variables, but now we consider one or more predictors that are categorical (not quantitative) in nature. This type of study design is especially common when we apply treatments that fall into classes (e.g. ``mutant'' vs ``wild-type'' in genetics) or observe explanatory factors in nature that are qualitative (e.g.~some climatic and geological conditions). One approach to this type of problem is called ``Analysis of Variance'' (ANOVA or AOV), and it, like other frequentist methods we have discussed, was formalized in the early 1900s. The general idea behind ANOVA is that we can test hypotheses about differences in group means for a response variable by comparing average within-group variance to among-group variance. In this case the ``groups'' are different factor levels of our explanatory variable(s). When within-group variances are substantially smaller than the among-group variance it stands to reason, given a few assumptions, that the distributions (and therefore the means) of at least some of the ``groups'' are different. Interestingly, this exercise of variance partitioning is tractable in a regression framework, because we can calculate sums of squares to reflect different variance components, and we can conceptualize the degree of difference between group means much in the same way we think about a slope in a regression model. For this reason, approaches such as regression, ANOVA, and others are all categorized as \textbf{\emph{general linear models}}.

\hypertarget{general-linear-models}{%
\section{General linear models}\label{general-linear-models}}

As mentioned, we can express the effects of categorical predictor variables on a numeric response in models that are very similar to regression models. Recall that for regression, we used the following straight-line model:
\[y_i=\beta_0+\beta_1x_i+\varepsilon_i\]

Where \(y\) is the response variable, \(x\) is the predictor variable, \(\beta_0\) is the \emph{y}-intercept, \(\beta_1\) is the slope, and \(\varepsilon\) is the unexplained variation, or error.

In the case of a single categorical predictor, for example, we can similarly include effects of each factor level relative to the overall mean of the response variable, as follows:
\[y_{ij}=\mu+\beta_1(level_1)_{ij}+\beta_2(level_2)_{ij}+...+\varepsilon_{ij}\]
Where each group (factor level) \(i\) contains a number of observations \(j\), \(\mu\) is the overall mean of \(y\), \(\beta\)s represent the effects of the corresponding factor levels relative to the overall mean, and \(\varepsilon_{ij}\) is the error term. You can think of \(\mu\) as being analogous to the \emph{y}-intercept, and the \(\beta\)s as adding or subtracting effect sizes to or from the grand mean. Because factor levels don't actually take on numeric values, in practice they are encoded using what are called binary ``dummy'' variables. If a particular observation is in group \(i\), it is represented as a ``1'', and otherwise as a ``0''. So, although your data frame (in \texttt{R}) may include a factor with three factor levels (for example ``A'', ``B'', and ``C''), under the hood \texttt{R} functions use three dummy variables to encode that factor and perform the appropriate calculations. A shorthand linear model notation, which collapses all level effects for a given factor into one term (denoted by \(\alpha\)) is often used:
\[y_{ij}=\mu+\alpha_i+\varepsilon_{ij}\]
Where \(\alpha_i\) represents the effect of belonging to group \(i\), expressed as the difference between each group \(i\) mean (\(\mu_i\)) and the overall mean (\(\mu\)). This notation is more convenient, especially when more than one factor is included in the model, a situation we will address later in the chapter.

\hypertarget{single-factor-anova}{%
\section{Single-factor ANOVA}\label{single-factor-anova}}

Single-factor Analysis of Variance describes the case in which we have a single quantitative response variable and a single categorical predictor variable. As discussed, the predictor variable (which we call a \textbf{\emph{factor}}) consists of two or more \textbf{\emph{factor levels}} that make up the possible conditions, or categories, of that variable. The procedure for ANOVA involves calculating sum of squares (SS) describing variation between/among factor levels (groups), and the SS descrbing variation within each group. We divide each of these SS values by the appropriate degrees of freedom (resulting in values we refer to as ``mean squares'' or MS). Finally we divide the group-level MS (\(MS_{groups}\)) by the within-group MS (called \(MS_{residual}\) because it represents the residual variation not explained by the factor). This value is an \emph{F}-ratio, which should sound familiar from the regression section of last chapter. Recall that an \emph{F}-ratio (in this case \(F=\frac{MS_{groups}}{MS_{residual}}\)) quantifies how much variation in the response variable is explained by a model, relative to how much variation is not explained by it. Large \emph{F}-ratios in the case of ANOVA indicate that the explanatory variable (the factor) is explaining a significant amount of variation in \emph{y} relative to the overall variation. We compare \emph{F} to an \emph{F} distribution with the appropriate degrees of freedom in order to calculate our \emph{p}-value for a given hypothesis test.

Let's walk through an example to help visualize what is actually going on when we perform single-factor ANOVA. Say that we are studying the percent time that male mice experiencing discomfort spend ``stretching,'' and that we are intereseted in how social context influences this variable. We have data from an actual experiment (Langford et al.~2006) in which mice experiencing mild discomfort (result of injection of 0.9\% acetic acid into the abdomen) were randomly assigned to one of three social treatments: 1. isolation, 2. housed with a companion mouse not injected, or 3. housed with a companion mouse also injected and exhibiting ``stretching'' behaviors associated with discomfort. The results suggest that mice stretch the most when a companion mouse is also experiencing mild discomfort. Mice experiencing pain appear to ``empathize'' with co-housed mice also in pain. To verbally state a linear model for the analysis of this experiment, we might say : \(stretching=mean_{overall}+treatment\). This model statement includes a response variable, a constant, and an explanatory variable. If we plot the data, we can see the respective distributions for time spent stretching among the three different treatments.

\begin{center}\includegraphics[width=0.8\linewidth]{images/images_5b.014} \end{center}

Note that in this type of plot (sometimes referred to as a ``strip chart''), the points are ``jittered'' with respect to the factor levels along the \emph{x}-axis, to assist with seeing all of the points clearly. In \texttt{R} we can use either the \texttt{plot()} function or the \texttt{stripchart()} function with the \texttt{method="jitter"} argument to do this. By the looks of the plot, we might reasonably suspect that the ``injected companion'' treatment appears to shift the percent time spent stretching up, relative to the other groups. As a consequence, we would expect the among-group SS to be larger than the average within-group SS.

To calculate the appropriate \emph{F}-ratio, for this data set, we would use the MS equations from column 3 of the table below (from Logan 2010):

\begin{center}\includegraphics[width=1\linewidth]{images/images_5b.018} \end{center}

The next figure (also from Logan 2010) shows how SS, and therefore MS, calculations are visualized in a situation with two factor levels. Our mouse example from above includes three factor levels, but the concept is exactly the same. The quantity in the red box divided by the quantity in the blue box forms our \emph{F}-ratio, which we can then use to test our null hypothesis of no effect of treatment.

\begin{center}\includegraphics[width=0.9\linewidth]{images/images_5b.019} \end{center}

\hypertarget{single-factor-anova-hypothesis-tests}{%
\subsection{Single-factor ANOVA hypothesis tests}\label{single-factor-anova-hypothesis-tests}}

For the type of single-factor ANOVA described above, we can state our null and alternative hypotheses in terms of the group means, or in terms of the ``effect'' of the explanatory variable. These are of course equivalent, but let's state them below (in terms of the mouse stretching experiment) just to be complete. For population means, we could state the following:
\[H_0:\mu_{isolated}=\mu_{companion}=\mu_{inj-companion}\]

\[H_A:\mu_{isolated}\neq\mu_{companion}=\mu_{inj-companion}\]
OR
\[\mu_{isolated}=\mu_{companion}\neq\mu_{inj-companion}\]
OR
\[\mu_{isolated}\neq\mu_{companion}\neq\mu_{inj-companion}\]

Recall that ANOVA tests for \textbf{\emph{any difference}} among groups, so there are multiple possible scenarios of group difference when we have more than two factor levels.

Stated in terms of ``effect,'' we can state the hypotheses as follows:
\[H_0:\alpha_i=0\]
\[H_A:\alpha_i\neq0\]
Where \(i\) represents any of our three factor levels (treatments)

At this point we should also introduce (briefly) the idea of fixed versus random effects (factors) in linear models and ANOVA. So far when discussing explanatory variables we have understood them as factors with levels we want to explicity compare. In the mouse experiment above, comparing the effects of the three social treatments on time spent stretching was a clear focus of our inference. We wanted to test specifically whether those three factor levels could be influencing the response variable. This is an example of a \textbf{\emph{fixed factor or effect}}. If groups are predetermined, of direct interest, and/or repeatable, they should most likely be treated as a fixed effect. Examples include experimental treatments, doses, age groups, habitat type, season, etc. Any conclusions reached from the analysis are specific to those factor levels and should not be generalized to other possible factor levels.

In some cases, though, we may be interested in whether a response variable is affected generically by a factor with a large range of possible levels. Examples may include plots, animal cages, kinship units, batches, buildings, etc. In these cases, assuming we don't care about the individual factor levels \emph{per se}, we can instead think of the factor levels in an experiment as a random sample of many possible levels. We call factors in this situation \textbf{\emph{random factors or effects}}. Many of the factors in your own studies will be fixed, but you should also consider the possibility of random factors in your study design. As stated below, how we frame the hypothesis for ANOVA depends on whether a factor is fixed versus random. Complex ANOVA models that include both fixed and random factors are called ``mixed models,'' and they are beyond the scope of this course. It is good practice, however, to learn the basic distinction between fixed and random effects. The null and alternative hypotheses for a random effect consider whether the variance associated with differing levels is zero:
\[H_0:\sigma_{\alpha}^2=0\]
\[H_A:\sigma_{\alpha}^2\neq0\]
The consideration is whether including the factor in the model explains any variance in the response variable.

\hypertarget{anova-assumptions}{%
\subsection{ANOVA assumptions}\label{anova-assumptions}}

The following assumptions should be met if the \emph{F}-ratio and \emph{F} distribution are used to evaluate an ANOVA hypothesis:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The response variable is approximately normally distributed in all groups (factor levels). Some departures from normality are tolerable if sample sizes and variances across the groups are equal. The normality assumption can be evaluated by histograms, and boxplots, for example.
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Variances are equal across groups. As long as there is no clear relationship between variance and mean or variance and sample size across groups, some departures from this assumption are tolerable if sample sizes are equal. Boxplots, mean vs.~variance plots, and equal variances tests can be used to address this assumption.
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Observations in a group are independent. As discussed previously, observations in a group should be randomly sampled and not structured in any way. If there is structure within groups (e.g.~via kinship, etc.), that structure should be dealt with by adding the appropriate nested terms to the model (see below).
  \end{enumerate}
\end{itemize}

\hypertarget{post-hoc-comparisons}{%
\subsection{Post-hoc comparisons}\label{post-hoc-comparisons}}

We stated above that a fixed, single-factor ANOVA tests whether any of the group means are different from one another. This hypothesis test may be sufficient to address the questions of the analyst, but in many cases we want to know which factor levels are significantly different. This can be achieved via several different approaches. Planned comparisons or ``contrasts'' are defined in advance of the ANOVA, and performing them is part of the model evaluation. Setting up planned contrasts requires defining comparisons based on rules, which include the avoidance of comparisons among groups that overlap, for one. Learning to set them up properly is a more advanced topic beyond what we can reasonably cover in this course. Post-hoc, or ``unplanned'' comparisons, however, make all pairwise comparisons among factor levels and are relatively straightforward to perform. Their implementation is similar to the performance of individual \emph{t}-tests, adjusted to account for the inflated Type I error associated with multiple testing. The \texttt{R} package \texttt{multcomp} can be used to perform a variety of post-hoc comparisons.

\hypertarget{single-factor-anova-in-r}{%
\subsection{\texorpdfstring{Single-factor ANOVA in \texttt{R}}{Single-factor ANOVA in R}}\label{single-factor-anova-in-r}}

In \texttt{R} we can use either the \texttt{lm()} or \texttt{aov()} functions to define fixed-effects ANOVA models, and then evaluate the models (e.g.~run \emph{F} tests, print ANOVA tables, etc.) using functions like \texttt{summary()} or \texttt{anova()} on the fitted model objects. The \texttt{aov()} function will format calculations in such a way as to present a traditional ANOVA table, whereas the \texttt{lm()} function will enable presentation of the parameter estimates. In the example below we will perform ANOVA using the \texttt{iris} data set to test whether there is a difference in mean sepal length among three flower species.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{stripchart}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length }\OperatorTok{~}\StringTok{ }\NormalTok{iris}\OperatorTok{$}\NormalTok{Species, }\DataTypeTok{vertical=}\NormalTok{T, }\DataTypeTok{method=}\StringTok{"jitter"}\NormalTok{,}
           \DataTypeTok{ylab=}\StringTok{"sepal length"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"species"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{, }\DataTypeTok{cex=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-87-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set up and evaluate linear model using lm() and summary()}
\NormalTok{iris_lm <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Sepal.Length }\OperatorTok{~}\StringTok{ }\NormalTok{Species, iris)}
\KeywordTok{summary}\NormalTok{(iris_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Length ~ Species, data = iris)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.6880 -0.3285 -0.0060  0.3120  1.3120 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(>|t|)    
## (Intercept)         5.0060     0.0728  68.762  < 2e-16 ***
## Speciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***
## Speciesvirginica    1.5820     0.1030  15.366  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.5148 on 147 degrees of freedom
## Multiple R-squared:  0.6187, Adjusted R-squared:  0.6135 
## F-statistic: 119.3 on 2 and 147 DF,  p-value: < 2.2e-16
\end{verbatim}

In this output there are a few items that are especially useful. The most useful information is in the first 2 columns of the ``Coefficients'' table, which contain the group mean estimates and their standard errors. In default linear model output in \texttt{R}, the intercept is actually set to the mean of the first (alphabetically) factor level. In this case, ``setosa'' is the first factor level, so its mean (5.006) is the intercept. The means for ``versicolor'' and ``virginica'' are the \textbf{\emph{intercept plus the value in the column}}. So the versicolor mean is 5.936 and the virginica mean is 6.588. The last line in the output shows the results for the ANOVA null hypothesis test of equal means across all three species, including the \emph{F}-ratio, the groups and residuals degrees of freedom, respectively, and the \emph{p}-value.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set up and evaluate ANOVA model using aov() and anova()}
\NormalTok{iris_aov <-}\StringTok{ }\KeywordTok{aov}\NormalTok{(Sepal.Length }\OperatorTok{~}\StringTok{ }\NormalTok{Species, iris)}
\KeywordTok{anova}\NormalTok{(iris_aov)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: Sepal.Length
##            Df Sum Sq Mean Sq F value    Pr(>F)    
## Species     2 63.212  31.606  119.26 < 2.2e-16 ***
## Residuals 147 38.956   0.265                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Here the output is the ANOVA table, complete with the degrees of freedom, SS, and MS for groups and residuals, our \emph{F}-ratio, and our \emph{p}-value. Clearly, we reject the null hypothesis. There is at least one difference among the three group means.

Finally, if we wanted to know whether all three species are different from each other, we could apply a ``Tukey's'' (post-hoc) test of all three mean pairs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(multcomp)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{glht}\NormalTok{(iris_aov, }\DataTypeTok{linfct =} \KeywordTok{mcp}\NormalTok{(}\DataTypeTok{Species =} \StringTok{"Tukey"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = Sepal.Length ~ Species, data = iris)
## 
## Linear Hypotheses:
##                             Estimate Std. Error t value Pr(>|t|)    
## versicolor - setosa == 0       0.930      0.103   9.033   <1e-08 ***
## virginica - setosa == 0        1.582      0.103  15.366   <1e-08 ***
## virginica - versicolor == 0    0.652      0.103   6.333   <1e-08 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## (Adjusted p values reported -- single-step method)
\end{verbatim}

The \emph{p}-values adjusted for multiple comparisons are reported, and they are all very low. We therefore conclude that all three species means are significantly different from one another.

\hypertarget{a-note-on-nonparametric-tests-similar-to-single-factor-anova}{%
\subsection{A note on nonparametric tests similar to single-factor ANOVA}\label{a-note-on-nonparametric-tests-similar-to-single-factor-anova}}

As with most categories of hypothesis test, there are nonparametric alternatives to ANOVA when the assumptions are not met. Randomization tests similar to the ones we have discussed previously can be used to generate a null distribution of \emph{F}-ratios on which to base a hypothesis test. There is also a rank-based ANOVA alternative that is similar to the Mann-Whitney U test we cited as a \emph{t}-test alternative. It is called the Kruskal-Wallis test, and is robust to non-normality and group variance differences. The Kruskal-Wallis test can be performed in \texttt{R} using the base function \texttt{kruskal.test()}.

\hypertarget{multi-factor-anova}{%
\section{Multi-factor ANOVA}\label{multi-factor-anova}}

When more than one categorical predictor variable is included in a study, we can apply our general linear model and ANOVA framework to accommodate this complexity. Indeed, it is often very important to include multiple predictor variables in a model, even if they are not of primary interest, and especially if they are expected to explain a significant amount of variation in the response variable. If these addtional factors do explain variation in \emph{y}, it is crucial to include them in the model so that we can account for that variation and in the process ``isolate'' variation explained by the focal predictor(s) central to our questions. For multiple fixed-effect factors, the guidelines described for single-factor ANOVA in the previous section (including hypotheses, assumptions, and model structure in \texttt{R}) hold. The only difference is that more than one term is added to the model on the ``right-hand side'' of the equation (or the \texttt{\textasciitilde{}} in \texttt{R} model notation), with some added syntax depending on the nature of the model. In the sections below we will briefly cover two different forms of multi-factor ANOVA: ``nested'' and ``factorial'' models. Nested models allow us to deal with observations that are, in some structured way, not independent, and factorial models allow us to test for statistical interactions among factors.

\hypertarget{nested-anova}{%
\subsection{Nested ANOVA}\label{nested-anova}}

In many observational studies and experiments the sampling units we measurements are heterogeneous in some way, be it spatially, temporally, or structurally. Measurements on individual organisms may vary substantially over time. Measurements in a plot may vary greatly depending on the region of the plot measured. The phenotypic effects of a mutation may vary across similar genetic backgrounds (e.g.~a family). In all of these cases, there could be a clear advantage to taking multiple, related measurments to capture some of this variation, especially when we have an experimental factor (e.g.~a treatment) that we are trying to test and understand. This is where \textbf{\emph{nested models}} come into play. We group observations (e.g.~sub-replicates) or measurements (e.g.~repeated measurements from the same individual) that are not independent of one another, according to a ``nested'' term in our ANOVA model. This allows us to account for that heterogeneity mentioned above, and is necessary to avoid violating the assumption of independence. When the appropriate nestedness is not included in a model, observations or measurements that are not independent are used erroneously to calculate mean squares, resulting in an artificially low \emph{p}-value. This is called \textbf{\emph{pseudoreplication}}. The schematic below shows the structure of a model with a main factor of interest (``A'') and a nested factor (``B'') which groups the non-independent subreplicates.

\begin{center}\includegraphics[width=0.9\linewidth]{images/images_7a.005} \end{center}

In this hypothetical example we have \emph{k} subreplicates for each site. Assuming we had 3 subreplicates for each site and did not include ``B'' (site) as a nested term in our model, we would erroneously include a sample size of 9 for each treatment group. By including ``B'' we properly account for the non-independence of observations within each site.

\hypertarget{nested-anova-hypothesis-tests}{%
\subsubsection{Nested ANOVA hypothesis tests}\label{nested-anova-hypothesis-tests}}

Hypothesis tests for nested ANOVAs take on the same basic structure as single-factor models, except that we can define a null and alternative hypothesis for each factor in our model. Typically nested terms are included to account for variation unexplained by the main factor(s) of interest and properly structure non-independence, so they are almost always considered as random effects. So hypotheses for main factors are stated as in the single-factor ANOVA section above, and hypotheses for nested factors are usually stated in random effects format:
\[H_0(B):\sigma_{\beta}^2=0\]
\[H_A(B):\sigma_{\beta}^2\neq0\]
Where the null hypothesis states that all possible levels of B within each level of the main factor (A) contribute no added variance to the response variable.

The same assumptions we addressed for single-factor ANOVA hold for nested ANOVA. In the case of the independence assumption, we still have to ensure that levels of the nested term are independent. In the example above, the three ``sites'' within each treatment should be independent of one another.

\hypertarget{nested-anova-in-r}{%
\subsubsection{\texorpdfstring{Nested ANOVA in \texttt{R}}{Nested ANOVA in R}}\label{nested-anova-in-r}}

As mentioned, mixed model ANOVA is not always straightforward to set up in \texttt{R}, but simple cases (e.g.~one main and one nested factor) can be analyzed easily. The function \texttt{lm()} function will not accommodate mixed models, so we instead rely on \texttt{aov()} if the design is balanced (equal sample sizes) and \texttt{lme()} (package \texttt{nlme}) or \texttt{lmer()} (package \texttt{lme4}) for unbalanced designs. Below is some hyptothetical \texttt{R} code that might be applied to the one main, one nested, two-factor example design above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## With aov() use "Error" with parentheses to specify the nested term.}
\CommentTok{## This will report the hypothesis test for the main effect.}
\NormalTok{mod_nested <-}\StringTok{ }\KeywordTok{aov}\NormalTok{(Response }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment }\OperatorTok{+}\StringTok{ }\KeywordTok{Error}\NormalTok{(Site), df_name)}
\KeywordTok{summary}\NormalTok{(mod_nested)}

\CommentTok{## To report the relative % variation explained by the main vs. nested factor,}
\CommentTok{## we use the lme() function to fit the model and then VarCorr() to report variance components.}
\CommentTok{## Note that the nested term notation is different for lme()}
\KeywordTok{library}\NormalTok{(nlme)}
\KeywordTok{library}\NormalTok{(lme4)}
\NormalTok{mod_nested_lme <-}\StringTok{ }\KeywordTok{lme}\NormalTok{(Response }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{random=}\OperatorTok{~}\DecValTok{1}\OperatorTok{|}\NormalTok{Site, df_name)}
\KeywordTok{VarCorr}\NormalTok{(mod_nested_lme)}
\end{Highlighting}
\end{Shaded}

\hypertarget{factorial-anova}{%
\subsection{Factorial ANOVA}\label{factorial-anova}}

Another example of multi-factor ANOVA is the factorial model. In a strictly factorial ANOVA we don't have nestedness, but the design is set up so that each level from one fixed-effects (usually) factor is observed together with the levels of another fixed-effects (usually) factor, and those ``factor level combinations'' are replicated. Factorial ANOVA allows one to test hypotheses for the main effects (the individual factors) and ``interactions'' between the factors. A statistical interaction is when the effect of a level from one factor depends on the level from another factor. For example, we may have a two-level factor like genotype (mutant vs.~wild-type) and a two-level treatment factor (control vs.~toxin). If all 4 combinations of the levels from those two factors are replicated across individuals in our experiment, we would have what is called a ``two-by-two'' factorial design. If we found that both mutant and wild-type groups respond similarly to the treatment (have the same control-toxin difference in means), there is no interaction. If, on the other hand, there is a difference in response (e.g.~there is a toxin-control difference for the wild-type group but not for the mutant group), we would say that there is evidence for an interaction, and in this example a ``genotype-by-environment'' interaction. The phenomenon of epistais in genetics is also an example of an interaction, between two or more loci, in which the phenotypic effect of a genotype at one locus depends on the genotype at another locus.

Because an interaction can take different forms, understanding the nature of an interaction is often made easier by plotting group means (and standard errors) in what is called an ``interaction plot.'' The \texttt{R} function \texttt{interaction.plot()} can be used to produce two-by-two interactiosn plots, or you can use \texttt{plot()}, \texttt{points()}, and \texttt{segments()} functions to make a custom interaction plot. In the figure below (from Logan 2010), we see several possible scenarios for a hypothetical two-by-two factorial design. On the \emph{x}-axis are the two levels for the Temperature factor (high and low), and the two lines (dashed and solid) represent low and high fertilizer, respectively.

\begin{center}\includegraphics[width=0.75\linewidth]{images/images_7a.022} \end{center}

The upper-left plot shows a likely interaction between temperature and fertilizer, in which there is an effect of fertilizer on seedling growth rate, but only at the high temperature. In the upper-right plot we see what look like overall main effects of temperature and fertilizer, but not interaction (the lines are parallel). In the lower-left plot we see an effect of fertilizer, but no effect of temperature in both fertilizer treatments, so no interaction. In the lower-right plot we see a ``crossing'' interaction, in which fertilizer has an opposite effect at the two temperatures. In cases like the last one, it is possible to detect no significant main effects (because they ``average out''), but an obviously strong interaction.

\hypertarget{factorial-anova-hypothesis-tests}{%
\subsubsection{Factorial ANOVA hypothesis tests}\label{factorial-anova-hypothesis-tests}}

Fixed- and random-effects hypotheses for the individual factors in a factorial ANOVA are subject to the same hypothesis statements mentioned above. However, a separate null hypothesis for each interaction term is testable in factorial models. As mentioned, for two factors (A and B) there is a single interaction term (A:B), with the following null and alternative hypotheses, assuming fixed main effects:
\[H_0(AB):\mu_{ij}=\mu_i+\mu_j-\mu\]
\[H_A(AB):\mu_{ij}\neq\mu_i+\mu_j-\mu\]

The interaction null hypothesis may look a bit strange, but it's really just saying that if you compare a difference bewteen levels of factor A within one level of factor B, to the same difference within another level of factor B, that ``difference of a difference'' should be zero if there is no interaction.

If at least one of the factors is a random-effects factor, then the interaction is understood as a random effect, with the following hypotheses:
\[H_0(AB):\sigma_{\alpha\beta}^2=0\]
\[H_A(AB):\sigma_{\alpha\beta}^2\neq0\]

The null hypothesis states that there is no additional variation in \emph{y} contributed by all possible interactions among all possible factor levels of A and B.

The same assumptions we addressed for single-factor ANOVA hold for factorial ANOVA, except that in this case groups are defined by factor level combinations, so the assumptions have to be met for each of those groups.

\hypertarget{factorial-anova-in-r}{%
\subsubsection{\texorpdfstring{Factorial ANOVA in \texttt{R}}{Factorial ANOVA in R}}\label{factorial-anova-in-r}}

In \texttt{R} we can use either the \texttt{aov()} function to define fixed-effects factorial ANOVA models, and then evaluate the models (e.g.~run \emph{F} tests, print ANOVA tables, etc.) using functions like \texttt{summary()} or \texttt{anova()} on the fitted model objects. The \texttt{aov()} function will format calculations in such a way as to present a traditional ANOVA table, for example when running \texttt{anova()} on an object from \texttt{aov()}, as we did in the case of single-factor ANOVA above.

Let's run a quick example of two-by-two factorial ANOVA below using the \texttt{mtcars} data frame. In this case we are interested in whether miles per gallon (\texttt{mpg}) is affected by the engine cylinder configuration (``V'' or ``straight''), the transmission type (``automatic'' or ``manual''), and their interaction. Before we specify the model and run the ANOVA, let's look at the group means in an interaction plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## First, modify the vs and am factor levels to be more descriptive}
\NormalTok{mtcars_mod <-}\StringTok{ }\NormalTok{mtcars}
\NormalTok{mtcars_mod}\OperatorTok{$}\NormalTok{vs <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(mtcars}\OperatorTok{$}\NormalTok{vs}\OperatorTok{==}\DecValTok{0}\NormalTok{, }\StringTok{"V"}\NormalTok{,}\StringTok{"straight"}\NormalTok{))}
\NormalTok{mtcars_mod}\OperatorTok{$}\NormalTok{am <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(mtcars}\OperatorTok{$}\NormalTok{am}\OperatorTok{==}\DecValTok{0}\NormalTok{, }\StringTok{"auto"}\NormalTok{,}\StringTok{"manual"}\NormalTok{))}

\CommentTok{## Make an interaction plot}
\KeywordTok{with}\NormalTok{(mtcars_mod, \{}
  \KeywordTok{interaction.plot}\NormalTok{(}\DataTypeTok{x.factor=}\NormalTok{am, }\DataTypeTok{trace.factor=}\NormalTok{vs, }\DataTypeTok{response=}\NormalTok{mpg, }\DataTypeTok{xlab=}\StringTok{"transmission"}\NormalTok{,}
                   \DataTypeTok{trace.label=}\StringTok{"cyl config"}\NormalTok{) }
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\includegraphics{foundational_statistics_files/figure-latex/unnamed-chunk-94-1.pdf}

The lines look fairly parallel, but let's run a factorial ANOVA to test for the two main effects, and their interaction, with respect to miles per gallon.

When specifying interactions in \texttt{R} models, there are options. In the two-by-two factorial case with fixed effects, we can set up the model in two, equivalent ways.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## To specify the main and interaction effects individually}
\NormalTok{mpg_fac1 <-}\StringTok{ }\KeywordTok{aov}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{vs }\OperatorTok{+}\StringTok{ }\NormalTok{am }\OperatorTok{+}\StringTok{ }\NormalTok{vs}\OperatorTok{:}\NormalTok{am, mtcars_mod)}

\CommentTok{## Shorthand notation to include all main effects and interactions}
\NormalTok{mpg_fac2 <-}\StringTok{ }\KeywordTok{aov}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{vs}\OperatorTok{*}\NormalTok{am, mtcars_mod)}
\end{Highlighting}
\end{Shaded}

And we can obtain the ANOVA table using the \texttt{anova()} function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anova}\NormalTok{(mpg_fac1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: mpg
##           Df Sum Sq Mean Sq F value    Pr(>F)    
## vs         1 496.53  496.53 41.1963 5.981e-07 ***
## am         1 276.03  276.03 22.9021 4.984e-05 ***
## vs:am      1  16.01   16.01  1.3283    0.2589    
## Residuals 28 337.48   12.05                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Consistent with our suspicion from the plot, we do not reject the null hypothesis of no interaction. There is evidence for the main effects of cylinder configuration (\texttt{vs}) and transmission (\texttt{am}), but not for an interaction (\texttt{vs:am}) between them.

In other more complex scenarios with multiple terms and interactions, interpretation can often be complicated. It definitely helps to use interaction plots to make those interpretations more clearly, but one can also perform either contrasts or post-hoc comparisons among groups to better understand significant differences among factor level combinations. It is also possible to have both nested and factorial terms in the same, ``partly nested'' analysis model. More advanced courses and reading on mixed general linear models go into these complex situations in depth, and evaluating these models requires careful consideration in many cases.

\hypertarget{exercises-associated-with-this-chapter-8}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-8}}

\begin{itemize}
\tightlist
\item
  Problem Set 4
\end{itemize}

\hypertarget{additional-learning-resources-8}{%
\section{Additional learning resources:}\label{additional-learning-resources-8}}

\begin{itemize}
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\item
  Langford, D. J.,et al.~2006. Science 312: 1967-1970. - example used for single-factor ANOVA
\end{itemize}

\hypertarget{introduction-to-frequency-analysis}{%
\chapter{Introduction to Frequency Analysis}\label{introduction-to-frequency-analysis}}

\hypertarget{background-8}{%
\section{Background}\label{background-8}}

Up to this point we have dealt exclusively with response variables that are continuous. It is possible, and relatively common, to encounter response variables that are not continuously distributed. For example, we may have a binary response variable or a categorical response with more than two levels. One way to approach the analysis of these discrete variables is to tally the number of observations in each category and compare these ``frequencies'' (proportions) to a null (i.e.~random) expectation for the frequencies, possibly with respect to another factor. In this multi-factor situation we often want to know whether the two categorical variables are independent of one another, that is, whether an observation is more or less likely than expected by random chance to take on a certain level of factor A, given it is characterized by a certain level of factor B. Hypothesis tests of this nature are often called ``tests of independence.'' Another common goal is to compare the frequencies of observations across factor levels to an expectation based on certain ``rules'' of a natural system. This type of test is called a ``goodness of fit'' test, and one example of its application is to test expected genetic patterns of Mendelian segregation using offspring phenotypes that result from a particular cross of plant or animal parents. In this chapter we will focus on fundamental application of goodness of fit tests and tests of independence. Although we will not cover it in this course, you should also be aware of another widely used analytical framework for discrete response variables: generalized linear models. These models take a non-continuous response variable and mathematically relate it to a linear combination of predictor variables, via something called a ``link'' function." For a binary response variable, for example, observations are modeled probabilistically as if they vary between 0 and 1 (even though they don't in reality), using an approach called logistic regression. More advanced statistical inference courses cover generalized linear models, as these approaches are frequently used to analyze counts, binary variables, frequencies, and ordinal categorical variables.

\hypertarget{goodness-of-fit-tests}{%
\section{Goodness of fit tests}\label{goodness-of-fit-tests}}

The null hypothesis for a goodness of fit test is that the relative frequencies of observations across categories in a population occur at a specific ratio. In practice this means that we need to compare observed frequency ratios with expected ones. If the deviation of observed from expected ratios is high, our test statistic should reflect that how extreme it is and be useful in a null hypothesis test. One such test statistic is the chi-square (\(\chi^2\)) statistic. It is calculated based on a sum of values (across categories) that reflects how much the observed frequencies differ from the expected:
\[\chi^2=\sum\frac{(o-e)^2}{e}\]
Where \(o\) and \(e\) are observed and expected counts, respectively in each category. If the observed and expected frequencies are the same (our null hypothesis), \(\chi^2\) sums to zero, or approximately zero considering sampling noise. We compare this test statistic calculated from a sample to a \(\chi^2\) distribution with degrees of freedom equal to the number of categories minus 1. Especially large values of \(\chi^2\) fall in the tail of the distribution and are extremely likely under the null hypothesis. The probablility of observing a value at least this extreme is our \emph{p}-value for the hypothesis test. In nearly all cases it makes sense to perform a one-sided test in this direction, but in principle a left-sided test could be performed to test for ``artificially high'' congruence of observed and expected frequencies.

\hypertarget{assumptions-of-the-chi-square-test}{%
\subsection{Assumptions of the chi-square test}\label{assumptions-of-the-chi-square-test}}

There are two assumptions for a valid null hypothesis test using the \(\chi^2\) statistic and its theoretical distribution:

\begin{itemize}
\item
  The observations are classified independently of one another, which should be satisfied through a random sample.
\item
  Only a small proportion of the categories (20\% or less) should have expected frequencies of less than five. Increasing the sample size sufficiently will ensure that this assumption is met. Other tests are more appropriate if this assumption cannot be met.
\end{itemize}

\hypertarget{goodness-of-fit-tests-in-r}{%
\subsection{\texorpdfstring{goodness of fit tests in \texttt{R}}{goodness of fit tests in R}}\label{goodness-of-fit-tests-in-r}}

As an example, let's say that we repeated one of Gregor Mendel's pea crossing experiments in which we were tracking the inheritance pattern for two traits: pea color (yellow vs.~green) and pea shape (smooth vs.~dented). Assuming independent assortment and unbiased segregation, we would expect a dihybrid cross (between two heterozygous parents) for these tratis to yield an expected 9:3:3:1 ratio of phenotype combinations in the progeny. We can perform a quick chi-square test in \texttt{R} to test the null hypothesis that our observed progeny from the cross adhere to this expected ratio.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## First, we create a vector for our observed counts}
\NormalTok{pea_count <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{160}\NormalTok{, }\DecValTok{39}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{11}\NormalTok{)}

\CommentTok{## Next, we create a vector of factor levels that name the 4 different categories, using the `gl()` function, and combine into a data frame}
\NormalTok{pea_type <-}\StringTok{ }\KeywordTok{gl}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"yellow_smooth"}\NormalTok{,}\StringTok{"yellow_dent"}\NormalTok{,}\StringTok{"green_smooth"}\NormalTok{,}\StringTok{"green_dent"}\NormalTok{))}
\NormalTok{pea_data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(pea_type, pea_count)}

\CommentTok{## Many frequency test functions need the data formatted as a "table," so we need to reformat}
\NormalTok{pea_table <-}\StringTok{ }\KeywordTok{xtabs}\NormalTok{(pea_count }\OperatorTok{~}\StringTok{ }\NormalTok{pea_type, }\DataTypeTok{data=}\NormalTok{pea_data)}

\CommentTok{## Before the test, let's evaluate our 20% of expected frequencies < 5 assumption.}
\CommentTok{## We can do this by running the chi-square test and pulling out just the expected counts vector}
\KeywordTok{chisq.test}\NormalTok{(pea_table, }\DataTypeTok{p=}\KeywordTok{c}\NormalTok{(}\DecValTok{9}\OperatorTok{/}\DecValTok{16}\NormalTok{, }\DecValTok{3}\OperatorTok{/}\DecValTok{16}\NormalTok{, }\DecValTok{3}\OperatorTok{/}\DecValTok{16}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{16}\NormalTok{), }\DataTypeTok{correct=}\NormalTok{F)}\OperatorTok{$}\NormalTok{exp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## yellow_smooth   yellow_dent  green_smooth    green_dent 
##       145.125        48.375        48.375        16.125
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## It looks like all 4 expected counts are greater than 5, so we will proceed.}
\KeywordTok{chisq.test}\NormalTok{(pea_table, }\DataTypeTok{p=}\KeywordTok{c}\NormalTok{(}\DecValTok{9}\OperatorTok{/}\DecValTok{16}\NormalTok{, }\DecValTok{3}\OperatorTok{/}\DecValTok{16}\NormalTok{, }\DecValTok{3}\OperatorTok{/}\DecValTok{16}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{16}\NormalTok{), }\DataTypeTok{correct=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Chi-squared test for given probabilities
## 
## data:  pea_table
## X-squared = 4.9733, df = 3, p-value = 0.1738
\end{verbatim}

Here we fail to reject the null hypothesis that our data adhere to our 9:3:3:1 ratio expectation, given that our \emph{p}-value is quite high. If the 20\% or fewer of expected frequencies \textless{} 5 assumption had not been met, several test statistic corrections (e.g.~Williams'), or a randomization test could have been applied. Also, if differences between observed and expected values are especially small (i.e.~much smaller than the expected values) the G-test (see below) can be a more powerful alternative.

\hypertarget{tests-of-independence-for-frequencies}{%
\section{Tests of independence for frequencies}\label{tests-of-independence-for-frequencies}}

The null hypothesis for frequency-based tests of independence is that the two or more categorical variables are independent of one another. Tests of independence do not assume causal relationships between variables, but causality may be argued depending on context. To test the independence of two categorical variables we organize the sample data into what is known as a ``contingency table,'' in which the rows represent the conditional category counts of variable 1 and the columns represent the conditional category counts of variable 2. Expected frequencies (under the null hypothesis of independence) for each cell in the table can be calculated by the product of the row and column total divided by the overall total. One possible test is to use these expected frequencies to calculate a (\(\chi^2\)) statistic with degrees of freedom equal to the number of rows minus 1 multiplied by the number of columns minus 1.

Another way to test the null hypothesis of independence between categorical variables is with something called a G-test. This test is a form of the ``likelihood ratio test,'' which is a test that evaluates whether the likelihoods (see Chapter 8) of two models or hypotheses are equal by evaluating how extreme their ratio (in practice the natural log of their ratio) is. If the log-likelihood ratio is extreme, the two models are likely different in how well they fit the data, and we reject the null hypothesis. For G-tests of independence, we sum up log-likelihood ratios (based on observed divided by expected counts) across all categories (cells in our contingency table), and then compare twice that sum to a (\(\chi^2\)) distribution with degrees of freedom again equal to \((n_{rows}-1)*(n_{columns}-1)\):
\[G^2=2\sum{o*ln(\frac{o}{e})}\]
Where \(o\) and \(e\) are observed and expected cell counts, respectively from the contingency table. Again, especially large values of \(G^2\) will result in rejection of the null hypothesis. G-tests of independence have the same aforementioned assumptions as the chi-square test.

\hypertarget{g-test-of-independence-in-r}{%
\subsection{\texorpdfstring{G-test of independence in \texttt{R}}{G-test of independence in R}}\label{g-test-of-independence-in-r}}

As an example of a G-test of independence, we can use the base \texttt{R} data set \texttt{HairEyeColor} to test whether hair and eye color are independent in a sample of female students from the University of Delaware in the 70s. We will use the \texttt{GTest()} function from the package \texttt{DescTools}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(DescTools)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## First, use indexing to retrieve just the female data from the 3-D table}
\NormalTok{hair_eye_females <-}\StringTok{ }\NormalTok{HairEyeColor[,,}\DecValTok{2}\NormalTok{]}

\CommentTok{## Test the assumption that 20% or fewer of expected frequencies are < 5}
\KeywordTok{chisq.test}\NormalTok{(hair_eye_females, }\DataTypeTok{correct=}\NormalTok{F)}\OperatorTok{$}\NormalTok{exp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Eye
## Hair       Brown     Blue     Hazel     Green
##   Black 20.26837 18.93930  7.642173  5.150160
##   Brown 55.73802 52.08307 21.015974 14.162939
##   Red   14.42173 13.47604  5.437700  3.664537
##   Blond 31.57188 29.50160 11.904153  8.022364
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Only one cell of 16 has a count < 5, so proceed with the G-test}
\KeywordTok{GTest}\NormalTok{(hair_eye_females)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Log likelihood ratio (G-test) test of independence without correction
## 
## data:  hair_eye_females
## G = 112.23, X-squared df = 9, p-value < 2.2e-16
\end{verbatim}

We reject our null hypothesis of independence between hair and eye color for this data set. It is extremely likely that certain hair colors are more or less likely to co-occur with certain eye colors than what is expected through random chance. There is very strong evidence of a statistical association between hair and eye color in this sample.

\hypertarget{odds-ratios}{%
\subsection{odds ratios}\label{odds-ratios}}

Rejecting a null hypothesis in a test of independence is one thing, but it doesn't tell us much about how or to what extent the variables are independent. For one, recall our past discussion of statistical versus practical significance. A \emph{p}-value from a statistical test does not, by itself, tell us how big a difference between or effect on populations actually is. For this, we rely on the calculation and reporting of \textbf{\emph{effect sizes}}. An effect size, such as a slope in a regression analysis, a difference in means, a ``fold-change'' in a gene expression analysis, etc., qunatifies the effect on or association between variables. Furthermore, in tests of independence with multiple factors or many more than two categories per factor, the strurctural details of how categories depend on one another may be complex and are not apparent in a \emph{p}-value.

Fortunately, we can appraise what are equivalent to effect sizes for frequency data by looking at the cells and marginal totals in a contingency table, and by calcuating what are called \textbf{\emph{odds ratios}} to measure the magnitude of indpendence among categories. The term ``odds'' in statistics simply refers to how likely a particular outcome is relative to how likely all other possible outcomes are. The formal expression of this is \(\pi_j=(1-\pi_j)\), with \(\pi_j\) representing the probability of that particular event occurring. If we flip a fair coin once, for example, the probability of getting ``tails'' is 0.5, and the probability of all other alternatives is 0.5. So, our odds of getting tails is \(0.5/(1-0.5) = 1\). An odds of one, also referred to as ``even odds'' simply means that getting tails is equally likely relative to all other possibilities (in this case getting ``heads''). But we can also compare odds for two different conditions that are distinct in some way, which is where odds ratios come into play. Say that we have a coin that we know is not fair, and specifically we know (because we flipped it a million times) that the probability of getting tails is actually 0.6. If we calculate an odds ratio for this ``tails-biased'' coin relative to the ``fair'' coin, we get
\(\frac{0.6/(1-0.4)}{0.5/(1-0.5)}=1.5\). The odds ratio tells us how many times more (or less) likely a particular event is in one scenario versus another. In this example, we are 1.5 times more likely to get a tails with the tails-biased coin than with the fair coin. This same logic applies to contingency tables. We might, for instance, compare the odds of selecting (from the \texttt{HairEyeColor} sample) a person with brown eyes who also has black hair, to the odds of selecting a person with brown eyes who also has blonde hair. In a situation where one has rejected the null hypothesis of independence, the odds ratios that deviate the most from one will give us some signal of where the association between variables is most likely coming from. For contingency table odds ratio calculations, the following simplified form of equation can be used for a 2x2 table to calculate the odds ratio \(\theta\):
\[\theta=\frac{(cell_{1,1}+0.5)(cell_{2,2}+0.5)}{(cell_{1,2}+0.5)(cell_{2,1}+0.5)}\]

Where the counts in the cells of a \(r\)x\(c\) table are denoted as \(cell_{r,c}\). A value of 0.5 is usually added to each cell count to prevent division by zero. If we revisit the coin flip example, but with (unrealistically perfect) coin flip data, we can work through how the odds ratio is calculated using this equation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Set up our imaginary coin flip data}
\CommentTok{## Rows will represent the tail-heavy and fair coin, respectively}
\CommentTok{## Columns will represent number of tails, and heads, respectively in 100 flips}
\NormalTok{flips <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{60}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{), }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(flips) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"tails"}\NormalTok{,}\StringTok{"heads"}\NormalTok{)}
\KeywordTok{rownames}\NormalTok{(flips) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"tail_biased"}\NormalTok{,}\StringTok{"fair"}\NormalTok{)}
\NormalTok{flips <-}\StringTok{ }\KeywordTok{as.table}\NormalTok{(flips)}
\NormalTok{flips}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             tails heads
## tail_biased    60    40
## fair           50    50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Perform the odds ratio calculation for odds of tails with the biased coin over odds of tails for the fair coin}

\NormalTok{odds_ratio_flips <-}\StringTok{ }\NormalTok{((flips[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\NormalTok{)}\OperatorTok{*}\NormalTok{(flips[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\NormalTok{))}\OperatorTok{/}\NormalTok{((flips[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\NormalTok{)}\OperatorTok{*}\NormalTok{(flips[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\NormalTok{))}
\NormalTok{odds_ratio_flips}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.493827
\end{verbatim}

We see that this odds ratio is \textasciitilde{}1.5, the same as calculated based on probabilies above. Here the value is a bit smaller than 1.5 because of the ``convenience'' 0.5 addtions. In practice, calculating odds ratios from tables that are larger than 2x2 requires splitting up those tables into ``partial tables,'' because odds ratios must always be calculated in a pairwise manner. Also, it is common to interpret odds ratios after taking their natural logarithm. These ``log odds'' or ``LOD'' values are necessary when calculating confidence intervals, for example. In \texttt{R}, odds ratios can be calculated manually from a contingency table, or using a function such as \texttt{oddsratio()} from the \texttt{epitools} package.

\hypertarget{a-final-note-on-presenting-statistical-test-results-in-writing}{%
\section{A final note on presenting statistical test results in writing}\label{a-final-note-on-presenting-statistical-test-results-in-writing}}

One final topic we should cover briefly in this course is the written presentation of statistical results. It is important to present your statistical analysis results in a clearly stated, consistent, and efficient manner, especially in reports and scientific articles or books. The general guidelines below apply to at least all frequentist statistical analyses (definitely the ones in this book!), and most apply to non-frequentist results as well.

As an example, suppose you asked the question, ``Is the average height of male students the same as female students in a pool of randomly selected Biology majors?'' During your study you collected height data from random samples (100 each) of male and female students. You then visualized the data using the appropriate plots, calculated descriptive statistics, and performed your hypothesis test. In your results section you would would include the figures, perhaps a table of your descriptive statistics for those samples (e.g.~mean, standard error of the mean, n, range, etc), and a declaration of your hypothesis test result with the formal details (effect size, test statistic, degrees of freedom, and \emph{p}-value) to support it. Don't forget to refer to your tables or figures as you state the main results in the text. Suppose you found that male Biology majors are, on average, 12.5 cm taller than female majors, and you rejected your null hypothesis of no difference in means. Declaring that males were taller by an average of 12.5 cm is the most important message, and the statistical details (which give support and clarification of your conclusion) come after that statement. When stating a main result, make sure that you actively state it as your (or your and your co-authors') finding. A statistical test doesn't ``do'' anything iteself, so it weakens the strength and confidence of your statement if you say, ``An ANOVA showed that males were significantly taller than females\ldots{}'' Instead write something like, ``We found that male Biology students were significantly taller than female Biology students, by an average of 12.5 cm (single-factor ANOVA, \(F=59.9\), \(d.f.=1;198\), \(p=1.23*10^{-8}\)).'' If the means and standard errors were not reported in a table or elsewhere, you could also have included them parenthetically in that sentence. Also, degrees of freedom can alternatively be reported as subscripts to the test statistic symbol (i.e. \(F_{1,198}=59.9\)). Below are some bulleted guidelines with good additional details about presentation of statistical analysis results.

\hypertarget{differences-directionality-and-magnitude}{%
\subsection{Differences, directionality, and magnitude}\label{differences-directionality-and-magnitude}}

\begin{itemize}
\item
  Emphasize clearly the nature of differences or relationships.
\item
  If you are testing for differences among groups, and you find a significant difference, it is not sufficient to simply report that ``groups A and B were significantly different''. How are they different and by how much?
\item
  It is much more informative to say ``Group A individuals were 23\% larger than those in Group B'', or, ``Group B pups gained weight at twice the rate of Group A pups.''
\item
  Report the direction of differences (greater, larger, smaller, etc) and the magnitude of differences (\% difference, how many times, etc.) whenever possible.
\end{itemize}

\hypertarget{other-statistical-results-reporting-formalities}{%
\subsection{Other statistical results reporting formalities}\label{other-statistical-results-reporting-formalities}}

\begin{itemize}
\item
  Always enter the appropriate units when reporting data or summary statistics. For an individual value you would write, ``\ldots{}the mean length was 10 cm'', or, ``\ldots{}the maximum time was 140 min.''
\item
  When including a measure of variability, place the unit after the error value, e.g., ``\ldots{}was 10  2.3 m''.
\item
  Likewise place the unit after the last in a series of numbers all having the same unit. For example: ``\ldots{}lengths of 5, 10, 15, and 20 m'', or ``\ldots{}no differences were observed after 2, 4, 6, or 8 min. of incubation''.
\end{itemize}

\hypertarget{exercises-associated-with-this-chapter-9}{%
\section{Exercises associated with this chapter:}\label{exercises-associated-with-this-chapter-9}}

\begin{itemize}
\tightlist
\item
  Problem Set 4
\end{itemize}

\hypertarget{additional-learning-resources-9}{%
\section{Additional learning resources:}\label{additional-learning-resources-9}}

\begin{itemize}
\tightlist
\item
  Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis
\end{itemize}

\bibliography{book.bib,packages.bib}

\end{document}
